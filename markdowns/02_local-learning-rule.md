# ç¬¬2ç« ï¼šç™ºç«ç‡ãƒ¢ãƒ‡ãƒ«ã¨å±€æ‰€å­¦ç¿’å‰‡
## ç¥çµŒç´°èƒã®ç”Ÿç†
## ç™ºç«ç‡ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³
### ç™ºç«ç‡ãƒ¢ãƒ‡ãƒ«

### ç·šå½¢å›å¸°â€»
ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ« (linear regression) ã§ã¯èª¬æ˜å¤‰æ•° (explanatory variable) $\mathbf{x}$ ã‚’ç·šå½¢å¤‰æ›ã—ï¼Œç›®çš„å¤‰æ•° (objective variable) $y$ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹ï¼èª¬æ˜å¤‰æ•°$p$å€‹ã®ç·šå½¢ãƒ¢ãƒ‡ãƒ« 

$$
\begin{equation}
y=w_0+w_1x_1+\cdots+w_px_p+\varepsilon=w_0+\sum_{j=1}^p w_jx_j+\varepsilon
\end{equation}
$$

ã§èª¬æ˜ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ï¼èª¬æ˜å¤‰æ•°ãŒå˜ä¸€ $(p=1)$ ã®å ´åˆã‚’å˜å›å¸°ï¼Œè¤‡æ•° $(p>1)$ ã®å ´åˆã‚’é‡å›å¸°ã¨å‘¼ã¶ã“ã¨ãŒã‚ã‚‹ï¼

æ¬¡ã«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D}=\left\{\mathbf{x}^{(i)}, y^{(i)}\right\}_{i=1}^n$ ã‚’è€ƒãˆã‚‹ï¼ãŸã ã—ï¼Œ$\mathbf{x}^{(i)}=\left[x_1^{(i)}, x_2^{(i)}, \ldots, x_p^{(i)}\right]^\top\in \mathbb{R}^p,\ y^{(i)}\in \mathbb{R}$ã¨ã™ã‚‹ï¼ã“ã“ã§æ·»ãˆå­— $(i)$ ãŒä»˜ã„ã¦ã„ã‚‹å ´åˆã¯è¦³æ¸¬å€¤ã‚’ï¼Œç„¡ã„å ´åˆã¯ãƒ¢ãƒ‡ãƒ«å†…å¤‰æ•°ã‚’è¡¨ã™ã“ã¨ã«æ³¨æ„ã—ã‚ˆã†ï¼
ã“ã“ã§ï¼Œ
$$
\mathbf{y}= \left[ \begin{array}{c} y^{(1)}\\ y^{(2)}\\ \vdots \\ y^{(n)} \end{array} \right] \in \mathbb{R}^n,\quad 
\mathbf{X}=\left[ \begin{array}{ccccc} 1 & x_{1}^{(1)}& x_{2}^{(1)} &\cdots & x_{p}^{(1)} \\ 1& x_{1}^{(2)}& x_{2}^{(2)}&\cdots & x_{p}^{(2)}\\ \vdots & \vdots& \vdots& \ddots & \vdots \\1 &x_{1}^{(n)} & x_{2}^{(n)} &\cdots & x_{p}^{(n)} \end{array} \right] \in \mathbb{R}^{n\times (p+1)}, \quad \mathbf{w}= \left[ \begin{array}{c} w_0\\ w_1\\ \vdots \\ w_p \end{array} \right] \in \mathbb{R}^{p+1}
$$

ã“ã®å ´åˆï¼Œå›å¸°ãƒ¢ãƒ‡ãƒ«ã¯ $\mathbf{y}=\mathbf{X}\mathbf{w}+\mathbf{\varepsilon}$ã¨æ›¸ã‘ã‚‹ï¼ãŸã ã—ï¼Œ$\mathbf{X}$ã¯è¨ˆç”»è¡Œåˆ— (design matrix)ï¼Œ$\boldsymbol{\varepsilon}$ã¯èª¤å·®é …ã§ã‚ã‚‹ï¼ç‰¹ã«ï¼Œ$\mathbf{\varepsilon}$ãŒå¹³å‡0, åˆ†æ•£$\sigma^2$ã®ç‹¬ç«‹ãªæ­£è¦åˆ†å¸ƒã«å¾“ã†å ´åˆï¼Œ$\mathbf{y}\sim \mathcal{N}(\mathbf{X}\mathbf{w}, \sigma^2\mathbf{I})$ã¨è¡¨ã›ã‚‹ï¼

#### æœ€å°äºŒä¹—æ³•ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®š
æœ€å°äºŒä¹—æ³• (ordinary least squares)ã«ã‚ˆã‚Šç·šå½¢å›å¸°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã™ã‚‹ï¼$y$ã®äºˆæ¸¬å€¤ã¯$\mathbf{X} \mathbf{w}$ãªã®ã§ï¼Œèª¤å·® $\mathbf{\delta} \in \mathbb{R}^n$ã¯
$\mathbf{\delta} = \mathbf{y}-\mathbf{X} \mathbf{w}$ã¨è¡¨ã›ã‚‹ï¼ã‚†ãˆã«ç›®çš„é–¢æ•°$L(\mathbf{w})$ã¯ 

$$
\begin{equation}
L(w)=\sum_{i=1}^n \delta_i^2 = \|\mathbf{\delta}\|^2=\mathbf{\delta}^\top \mathbf{\delta}
\end{equation}
$$

ã¨ãªã‚Šï¼Œ $L(\mathbf{w})$ã‚’æœ€å°åŒ–ã™ã‚‹$\mathbf{w}$, ã¤ã¾ã‚Š $\hat {\mathbf {w }}={\underset {\mathbf {w}}{\operatorname {arg min} }}\,L({\mathbf{w}})$
ã‚’æ±‚ã‚ã‚‹ï¼

##### æ­£è¦æ–¹ç¨‹å¼ã‚’ç”¨ã„ãŸæ¨å®š
æ¡ä»¶ã«åŸºã¥ã„ã¦ç›®çš„é–¢æ•°$L(\mathbf{w})$ã‚’å¾®åˆ†ã™ã‚‹ã¨æ¬¡ã®ã‚ˆã†ãªæ–¹ç¨‹å¼ãŒå¾—ã‚‰ã‚Œã‚‹ï¼

$$
\begin{equation}
\mathbf{X}^\top\mathbf{X}\mathbf{\hat w}=\mathbf{X}^\top\mathbf{y}
\end{equation}
$$

ã“ã‚Œã‚’**æ­£è¦æ–¹ç¨‹å¼** (normal equation)ã¨å‘¼ã¶ï¼ã“ã®æ­£è¦æ–¹ç¨‹å¼ã‚ˆã‚Šã€ä¿‚æ•°ã®æ¨å®šå€¤ã¯$\mathbf{\hat w}={(\mathbf{X}^\top\mathbf{X})}^{-1}X^\top\mathbf{y}$ã¨ã„ã†å¼ã§å¾—ã‚‰ã‚Œã‚‹ï¼ãªãŠï¼Œæ­£è¦æ–¹ç¨‹å¼è‡ªä½“ã¯$\mathbf{y}=\mathbf{X}\mathbf{w}$ã®å·¦ã‹ã‚‰$\mathbf{X}^\top$ã‚’ã‹ã‘ã‚‹ï¼Œã¨è¦šãˆã‚‹ã¨è‰¯ã„ï¼

##### å‹¾é…æ³•ã‚’ç”¨ã„ãŸæ¨å®š
æœ€å°äºŒä¹—æ³•ã«ã‚ˆã‚‹å›å¸°ç›´ç·šã‚’å‹¾é…æ³•ã§æ±‚ã‚ã¦ã¿ã‚ˆã†ï¼$w$ã®æ›´æ–°å¼ã¯$w \leftarrow w + \alpha\cdot \dfrac{1}{n} \delta \mathbf{X}$ã¨æ›¸ã‘ã‚‹ï¼ãŸã ã—ï¼Œ$\alpha$ã¯å­¦ç¿’ç‡ã§ã‚ã‚‹ï¼

### ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¨ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³â€»
æœ¬ç¯€ã§ã¯éç·šå½¢å›å¸°ã®ä¸€ç¨®ã§ã‚ã‚‹ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° (logistic regression) ãŠã‚ˆã³ 1å±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ (perceptron) ã‚’å–ã‚Šæ‰±ã†ï¼

åˆ†é¡å•é¡Œ
, perceptron
<https://www.cs.utexas.edu/~gdurrett/courses/fa2022/perc-lr-connections.pdf>

<https://en.wikipedia.org/wiki/Perceptron>

<https://arxiv.org/abs/2012.03642>


perceptronã¯0/1 or -1/1ã®ã©ã¡ã‚‰ã‹

UNDERSTANDING STRAIGHT-THROUGH ESTIMATOR IN TRAINING ACTIVATION QUANTIZED NEURAL NETS

Yoshua Bengio, Nicholas LÂ´eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

Hinton (2012) in his lecture 15b

G. Hinton. Neural networks for machine learning, 2012.
<https://www.cs.toronto.edu/~hinton/coursera_lectures.html>

delta rule


Here Ïƒ denotes the (point-wise) activation function, $W \in R^{m\times n}$
is the weight-matrix and $b \in R^n$
is
the bias-vector. The vector $x \in R^m$ and the vector $y \in R^n$ denote the input, respectively the output

$$
\begin{equation}
y=\sigma(W^\top x + b)
\end{equation}
$$

$$
\begin{align}
& \text { Initialize } W^0, b^0 \text {; } \\
& \text { for } k=1,2, \ldots \text { do } \\
& \qquad \begin{array}{|l}
\text { for } i=1, \ldots, s \text { do } \\
e_i=y_i-\sigma\left(\left(W^k\right)^{\top} x_i+b^k\right) \\
W^{k+1}=W^k+e_i x_i^{\top} \\
b^{k+1}=b^k+e_i
\end{array} \\
& \text { end }
\end{align}
$$

## Hebbå‰‡ã¨ä¸»æˆåˆ†åˆ†æ
### Hebbå‰‡
ç¥çµŒå›è·¯ã¯ã©ã®ã‚ˆã†ã«ã—ã¦è‡ªå·±çµ„ç¹”åŒ–ã™ã‚‹ã®ã ã‚ã†ã‹ï¼1940å¹´ä»£ã«ã‚«ãƒŠãƒ€ã®å¿ƒç†å­¦è€…Donald O. Hebbã«ã‚ˆã‚Šè‘—æ›¸"The Organization of Behavior"{cite:p}`Hebb1949-iv` ã§ææ¡ˆã•ã‚ŒãŸå­¦ç¿’å‰‡ã¯ã€Œç´°èƒAãŒåå¾©çš„ã¾ãŸã¯æŒç¶šçš„ã«ç´°èƒBã®ç™ºç«ã«é–¢ä¸ã™ã‚‹ã¨ï¼Œç´°èƒAãŒç´°èƒBã‚’ç™ºç«ã•ã›ã‚‹åŠ¹ç‡ãŒå‘ä¸Šã™ã‚‹ã‚ˆã†ãªæˆé•·éç¨‹ã¾ãŸã¯ä»£è¬å¤‰åŒ–ãŒä¸€æ–¹ã¾ãŸã¯ä¸¡æ–¹ã®ç´°èƒã«èµ·ã“ã‚‹ã€ã¨ã„ã†ã‚‚ã®ã§ã‚ã£ãŸï¼ã™ãªã‚ã¡ï¼Œç™ºç«ã«æ™‚é–“çš„ç›¸é–¢ã®ã‚ã‚‹ç´°èƒé–“ã®ã‚·ãƒŠãƒ—ã‚¹çµåˆã‚’å¼·åŒ–ã™ã‚‹ã¨ã„ã†å­¦ç¿’å‰‡ã§ã‚ã‚‹ï¼ã“ã‚Œã‚’**Hebbã®å­¦ç¿’å‰‡ (Hebbian learning rule)** ã‚ã‚‹ã„ã¯**Hebbå‰‡(Hebb's rule)** ã¨ã„ã†ï¼Hebbå‰‡ã¯ (Hebbè‡ªèº«ã§ã¯ãªã) Shatzã«ã‚ˆã‚Š"cells that fire together wire together" (å…±ã«æ´»å‹•ã™ã‚‹ç´°èƒã¯å…±ã«çµåˆã™ã‚‹)ã¨éŸ»ã‚’è¸ã¿ãªãŒã‚‰çŸ­ãè¨€ã„æ›ãˆã‚‰ã‚Œã¦ã„ã‚‹ {cite:p}`Shatz1992-he`ï¼

#### Hebbå‰‡ã®å°å‡º
æ•°å¼ã§Hebbå‰‡ã‚’è¡¨ã—ã¦ã¿ã‚ˆã†ï¼$n$å€‹ã®ã‚·ãƒŠãƒ—ã‚¹å‰ç´°èƒã¨$m$å€‹ã®å¾Œç´°èƒã®ç™ºç«ç‡ã‚’ãã‚Œãã‚Œ$\mathbf{x}\in \mathbb{R}^n, \mathbf{y}\in \mathbb{R}^m$ ã¨ã™ã‚‹ï¼å‰ç´°èƒã¨å¾Œç´°èƒé–“ã®ã‚·ãƒŠãƒ—ã‚¹çµåˆå¼·åº¦ã‚’è¡¨ã™è¡Œåˆ—ã‚’$\mathbf{W}\in \mathbb{R}^{m\times n}$ã¨ã—ï¼Œ$\mathbf{y}=\mathbf{W}\mathbf{x}$ãŒæˆã‚Šç«‹ã¤ã¨ã™ã‚‹ï¼ã“ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚’ç·šå½¢ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ« (Linear neuron model) ã¨ã„ã†ï¼ã“ã®ã¨ãï¼ŒHebbå‰‡ã¯

$$
\begin{equation}
\tau\frac{d\mathbf{W}}{dt}=\phi(\mathbf{y})\varphi(\mathbf{x})^\top
\end{equation}
$$

ã¨ã—ã¦è¡¨ã•ã‚Œã‚‹ï¼ãŸã ã—ï¼Œ$\tau$ã¯æ™‚å®šæ•°ã§ã‚ã‚Šï¼Œ$\eta:=1/\tau$ ã¯**å­¦ç¿’ç‡ (learning rate)** ã¨å‘¼ã°ã‚Œã‚‹å­¦ç¿’ã®é€Ÿã•ã‚’æ±ºå®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãªã‚‹ï¼$\varphi(\cdot)$ãŠã‚ˆã³$\phi(\cdot)$ã¯ï¼Œãã‚Œãã‚Œã‚·ãƒŠãƒ—ã‚¹å‰ç´°èƒãŠã‚ˆã³å¾Œç´°èƒã®æ´»å‹•é‡ã«å¿œã˜ã¦é‡ã¿ã®å¤‰åŒ–é‡ã‚’æ±ºå®šã™ã‚‹é–¢æ•°ã§ã‚ã‚‹ï¼ãŸã ã—ï¼Œ$\varphi(\cdot), \phi(\cdot)$ã¯åŸºæœ¬çš„ã«æ’ç­‰é–¢æ•°ã«è¨­å®šã•ã‚Œã‚‹å ´åˆãŒå¤šã„ï¼ã“ã®å ´åˆï¼ŒHebbå‰‡ã¯$
\tau\dfrac{d\mathbf{W}}{dt}=\mathbf{y}\mathbf{x}^\top=(\text{post})\cdot (\text{pre})^\top
$ã¨ç°¡æ½”ã«è¡¨ç¾ã•ã‚Œã‚‹ï¼

ã“ã®Hebbå‰‡ã¯æ•°å­¦çš„ã«å°å‡ºã•ã‚ŒãŸã‚‚ã®ã§ã¯ãªã„ãŒï¼Œç‰¹å®šã®ç›®çš„é–¢æ•°ã‚’ç¥çµŒæ´»å‹•åŠã³é‡ã¿ã‚’å¤‰åŒ–ã•ã›ã¦æœ€é©åŒ–ã™ã‚‹ã‚ˆã†ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã™ã‚Œã°è‡ªç„¶ã«å‡ºç¾ã™ã‚‹ï¼ã“ã®ã‚ˆã†ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’**ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« (energy-based models)** ã¨ã„ã„ï¼Œæ¬¡ç« ã§æ‰±ã†ï¼ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã¯ï¼Œå…ˆã«ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° (ã‚ã‚‹ã„ã¯ã‚³ã‚¹ãƒˆé–¢æ•°) $\mathcal{E}$ ã‚’å®šç¾©ã—ï¼Œãã®ç›®çš„é–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ãªç¥çµŒæ´»å‹• $\mathbf{z}$ ãŠã‚ˆã³é‡ã¿è¡Œåˆ— $\mathbf{W}$ ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’ãã‚Œãã‚Œ,

$$
\begin{equation}
\frac{d \mathbf{z}}{dt}\propto-\frac{\partial \mathcal{E}}{\partial \mathbf{z}},\ \frac{d \mathbf{W}}{dt}\propto-\frac{\partial \mathcal{E}}{\partial \mathbf{W}}
\end{equation}
$$

ã¨ã—ã¦å°å‡ºã™ã‚‹ï¼ã“ã®æ‰‹é †ã®é€†ã‚’è¡Œã†ï¼Œã™ãªã‚ã¡å…ˆã«ç¥çµŒç´°èƒã®æ´»å‹•ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’å®šç¾©ã—ï¼Œç¥çµŒæ´»å‹•ã§ç©åˆ†ã™ã‚‹ã“ã¨ã§ç¥çµŒå›è·¯ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°$\mathcal{E}$ã‚’å°å‡ºã—ï¼Œã•ã‚‰ã« $\mathcal{E}$ ã‚’é‡ã¿è¡Œåˆ—ã§å¾®åˆ†ã™ã‚‹ã“ã¨ã§Hebbå‰‡ãŒå°å‡ºã§ãã‚‹ {cite:p}`Isomura2020-sn`ï¼Hebbå‰‡ã®å°å‡ºã‚’é€£ç¶šæ™‚é–“ç·šå½¢ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ« $\dfrac{d\mathbf{y}}{dt}=\mathbf{W}\mathbf{x}$ ã‚’ä¾‹ã«ã—ã¦è€ƒãˆã‚ˆã†ï¼ã“ã“ã§$\dfrac{\partial\mathcal{E}}{\partial\mathbf{y}}:=-\dfrac{d\mathbf{y}}{dt}$ã¨ãªã‚‹ã‚ˆã†ãªã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° $\mathcal{E}(\mathbf{x}, \mathbf{y}, \mathbf{W})$ã‚’ä»®å®šã™ã‚‹ã¨ï¼Œ

$$
\begin{equation}
\mathcal{E}(\mathbf{x}, \mathbf{y}, \mathbf{W})=-\int \mathbf{W}\mathbf{x}\ d\mathbf{y}=-\mathbf{y}^\top \mathbf{W}\mathbf{x} \in \mathbb{R}
\end{equation}
$$

ã¨ãªã‚‹ï¼ã“ã‚Œã‚’ã•ã‚‰ã«$\mathbf{W}$ã§å¾®åˆ†ã™ã‚‹ã¨ï¼Œ

$$
\begin{equation}
\dfrac{\partial\mathcal{E}}{\partial\mathbf{W}}=-\mathbf{y}\mathbf{x}^\top\Rightarrow
\frac{d\mathbf{W}}{dt}=-\dfrac{\partial\mathcal{E}}{\partial\mathbf{W}}=\mathbf{y}\mathbf{x}^\top
\end{equation}
$$

ã¨ãªã‚Šï¼ŒHebbå‰‡ãŒå°å‡ºã§ãã‚‹ (ç°¡å˜ã®ãŸã‚æ™‚å®šæ•°ã¯1ã¨ã—ãŸ)ï¼

### Hebbå‰‡ã®å®‰å®šåŒ–ã¨LTP/LTD
#### BCMå‰‡
Hebbå‰‡ã«ã¯å•é¡Œç‚¹ãŒã‚ã‚Šï¼Œã‚·ãƒŠãƒ—ã‚¹çµåˆå¼·åº¦ãŒéš›é™ãªãå¢—å¤§ã™ã‚‹ã‹ï¼Œ0ã«è¿‘ã¥ãã“ã¨ã¨ãªã£ã¦ã—ã¾ã†ï¼ã“ã‚Œã‚’æ•°å¼ã§ç¢ºèªã—ã¦ãŠã“ã†ï¼å‰ç´°èƒã¨å¾Œç´°èƒãŒãã‚Œãã‚Œ1ã¤ã®å ´åˆã‚’è€ƒãˆã‚‹ï¼2ç´°èƒé–“ã®çµåˆå¼·åº¦ã‚’$w\ (>0)$ ã¨ã—ï¼Œ$y=wx$ãŒæˆã‚Šç«‹ã¤ã¨ã™ã‚‹ã¨ï¼ŒHebbå‰‡ã¯$\dfrac{dw}{dt}=\eta yx=\eta x^2w$ã¨ãªã‚‹ï¼ã“ã®å ´åˆï¼Œ$\eta x^2>1$ ãªã‚‰ $\lim_{t\to\infty} w= \infty$, $\eta x^2<1$ ãªã‚‰ $\lim_{t\to\infty} w= 0$ ã¨ãªã‚‹ï¼å½“ç„¶ï¼Œç”Ÿç†çš„ã«ã‚·ãƒŠãƒ—ã‚¹çµåˆå¼·åº¦ãŒç„¡é™å¤§ã¨ãªã‚‹ã“ã¨ã¯ã‚ã‚Šå¾—ãªã„ãŒï¼Œä¸å®‰å®šãªã»ã©å¤§ãããªã£ã¦ã—ã¾ã†å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã«é•ã„ã¯ãªã„ï¼ã“ã®ãŸã‚ï¼ŒHebbå‰‡ã‚’å®‰å®šåŒ–ã•ã›ã‚‹ãŸã‚ã®ä¿®æ­£ãŒå¿…è¦ã¨ã•ã‚ŒãŸï¼

Cooper, Liberman, Ojaã‚‰ã«ã‚ˆã‚Šé ­æ–‡å­—ã‚’ã¨ã£ã¦**CLOå‰‡** (CLO rule) ãŒææ¡ˆã•ã‚ŒãŸ {cite:p}`Cooper1979-wz`ï¼ãã®å¾Œï¼ŒBienenstock, Cooper, Munroã‚‰ã«ã‚ˆã‚Šææ¡ˆã•ã‚ŒãŸå­¦ç¿’å‰‡ã¯åŒæ§˜ã«é ­æ–‡å­—ã‚’ã¨ã£ã¦**BCMå‰‡** (BCM rule) ã¨å‘¼ã°ã‚Œã¦ã„ã‚‹{cite:p}`Bienenstock1982-km` {cite:p}`Cooper2012-ec`ï¼

$\mathbf{x}\in \mathbb{R}^d, \mathbf{w}\in \mathbb{R}^d, y\in \mathbb{R}$ã¨ã—ï¼Œå˜ä¸€ã®å‡ºåŠ›$y = \mathbf{w}^\top \mathbf{x}=\mathbf{x}^\top \mathbf{w}$ã‚’æŒã¤ç·šå½¢ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ä»®å®šã™ã‚‹ï¼é‡ã¿ã®æ›´æ–°å‰‡ã¯æ¬¡ã®ã‚ˆã†ã«ã™ã‚‹ï¼

$$
\begin{equation}
\frac{d\mathbf{w}}{dt} = \eta_w \mathbf{x} \phi(y, \theta_m)
\end{equation}
$$

ã“ã“ã§é–¢æ•°$\phi$ã¯$\phi(y, \theta_m)=y(y-\theta_m)$ãªã©ã¨ã™ã‚‹ï¼ã¾ãŸ$\theta_m:=\mathbb{E}[y^2]$ã¯é–¾å€¤ã‚’æ±ºå®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼Œ**ä¿®æ­£é–¾å€¤(modification threshold)** ã§ã‚ã‚Šï¼Œ

$$
\begin{equation}
\frac{d\theta_m}{dt} = \eta_{\theta} \left(y^2-\theta_m\right)
\end{equation}
$$

ã¨ã—ã¦æ›´æ–°ã•ã‚Œã‚‹ï¼

#### Hebbå‰‡ã®ç”Ÿç†çš„æ©Ÿåº
LTPã®å®Ÿé¨“çš„ç™ºè¦‹ {cite:p}`Bliss1973-vj` {cite:p}`Dudek1992-nz`

### Ojaå‰‡
Hebbå‰‡ã‚’å®‰å®šåŒ–ã•ã›ã‚‹åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦ï¼Œçµåˆå¼·åº¦ã‚’æ­£è¦åŒ–ã™ã‚‹ã¨ã„ã†æ‰‹æ³•ãŒè€ƒãˆã‚‰ã‚Œã‚‹ï¼BCMå‰‡ã¨åŒæ§˜ã«$\mathbf{x}\in \mathbb{R}^d, \mathbf{w}\in \mathbb{R}^d, y\in \mathbb{R}$ã¨ã—ï¼Œå˜ä¸€ã®å‡ºåŠ›$y = \mathbf{w}^\top \mathbf{x}=\mathbf{x}^\top \mathbf{w}$ã‚’æŒã¤ç·šå½¢ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ä»®å®šã™ã‚‹ï¼$\eta$ã‚’å­¦ç¿’ç‡ã¨ã™ã‚‹ã¨ï¼Œ$\mathbf{w}\leftarrow\dfrac{\mathbf{w}+\eta \mathbf{x}y}{\|\mathbf{w}+\eta \mathbf{x}y\|}$ã¨ã™ã‚Œã°æ­£è¦åŒ–ã§ãã‚‹ï¼ã“ã“ã§ï¼Œ$f(\eta):=\dfrac{\mathbf{w}+\eta \mathbf{x}y}{\|\mathbf{w}+\eta \mathbf{x}y\|}$ã¨ã—ï¼Œ$\eta=0$ã«ãŠã„ã¦Taylorå±•é–‹ã‚’è¡Œã†ã¨ï¼Œ

$$
\begin{align}
f(\eta)&\approx f(0) + \eta \left.\frac{df(\eta^*)}{d\eta^*}\right|_{\eta^*=0} + \mathcal{O}(\eta^2)\\
&=\frac{\mathbf{w}}{\|\mathbf{w}\|} + \eta \left(\frac{\mathbf{x}y}{\|\mathbf{w}\|}-\frac{y^2\mathbf{w}}{\|\mathbf{w}\|^3}\right)+ \mathcal{O}(\eta^2)
\end{align}
$$

ã“ã“ã§$\|\mathbf{w}\|=1$ã¨ã—ã¦ï¼Œ1æ¬¡è¿‘ä¼¼ã™ã‚Œã°$f(\eta)\approx \mathbf{w} + \eta \left(\mathbf{x}y-y^2 \mathbf{w}\right)$ã¨ãªã‚‹ï¼é‡ã¿ã®å¤‰åŒ–ãŒé€£ç¶šçš„ã§ã‚ã‚‹ã¨ã™ã‚‹ã¨ï¼Œ

$$
\begin{equation}
\frac{d\mathbf{w}}{dt} = \eta \left(\mathbf{x}y-y^2 \mathbf{w}\right)
\end{equation}
$$

ã¨ã—ã¦é‡ã¿ã®æ›´æ–°å‰‡ãŒå¾—ã‚‰ã‚Œã‚‹ï¼ã“ã‚Œã‚’**Ojaå‰‡ (Oja's rule)** ã¨å‘¼ã¶ {cite:p}`Oja1982-yd`ï¼ã“ã†ã—ã¦å¾—ã‚‰ã‚ŒãŸå­¦ç¿’å‰‡ã«ãŠã„ã¦$\|\mathbf{w}\|\to 1$ã¨ãªã‚‹ã“ã¨ã‚’ç¢ºèªã—ã‚ˆã†ï¼

$$
\begin{equation}
\frac{d\|\mathbf{w}\|^2}{dt}=2\mathbf{w}^\top\frac{d\mathbf{w}}{dt}= 2\eta y^2\left(1-\|\mathbf{w}\|^2\right)
\end{equation}
$$

ã‚ˆã‚Šï¼Œ$\dfrac{d\|\mathbf{w}\|^2}{dt}=0$ã®ã¨ãï¼Œ$\|\mathbf{w}\|= 1$ã¨ãªã‚‹ï¼

#### æ’å¸¸çš„å¯å¡‘æ€§
Ojaå‰‡ã¯æ›´æ–°æ™‚ã®å³æ™‚çš„ãªæ­£è¦åŒ–ã‹ã‚‰å°å‡ºã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚‹ãŒï¼Œæ’å¸¸çš„å¯å¡‘æ€§ (synaptic scaling)ã«ã‚ˆã‚Šå®‰å®šåŒ–ã—ã¦ã„ã‚‹ã¨ã„ã†èª¬ãŒã‚ã‚‹{cite:p}`Turrigiano2008-lm`{cite:p}`Yee2017-fb`ï¼ã—ã‹ã—ï¼Œã“ã®éç¨‹ã¯é…ã™ãã‚‹ãŸã‚ï¼ŒHebbå‰‡ã®ä¸å®‰å®šåŒ–ã‚’å®‰å®šåŒ–ã™ã‚‹ã«è‡³ã‚‰ãªã„{cite:p}`Zenke2017-el`

ToDo:æ’å¸¸çš„å¯å¡‘æ€§ã®è©³ç´°

Johansen, Joshua P., Lorenzo Diaz-Mataix, Hiroki Hamanaka, Takaaki Ozawa, Edgar Ycu, Jenny Koivumaa, Ashwani Kumar, et al. 2014. â€œHebbian and Neuromodulatory Mechanisms Interact to Trigger Associative Memory Formation.â€ Proceedings of the National Academy of Sciences 111 (51): E5584â€“92.

#### Hebbå‰‡ã¨ä¸»æˆåˆ†åˆ†æ
Ojaå‰‡ã‚’ç”¨ã„ã‚‹ã“ã¨ã§**ä¸»æˆåˆ†åˆ†æ(Principal component analysis; PCA)** ã¨ã„ã†å‡¦ç†ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã„ã¦å®Ÿç¾ã§ãã‚‹ï¼ä¸»æˆåˆ†åˆ†æã¨ã¯-

ToDo:ä¸»æˆåˆ†åˆ†æã®èª¬æ˜

### Ojaå‰‡ã«ã‚ˆã‚‹PCAã®å®Ÿè¡Œ
ã“ã“ã§Ojaå‰‡ãŒä¸»æˆåˆ†åˆ†æã‚’å®Ÿè¡Œã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ï¼é‡ã¿ã®å¤‰åŒ–é‡ã®æœŸå¾…å€¤ã‚’å–ã‚‹ï¼

$$
\begin{align}
\frac{d\mathbf{w}}{dt} &= \eta \left(\mathbf{x}y - y^2 \mathbf{w}\right)=\eta \left(\mathbf{x}\mathbf{x}^\top \mathbf{w} - \left[\mathbf{w}^\top \mathbf{x}\mathbf{x}^\top \mathbf{w}\right] \mathbf{w}\right)\\
\mathbb{E}\left[\frac{d\mathbf{w}}{dt}\right] &= \eta \left(\mathbf{C} \mathbf{w} - \left[\mathbf{w}^\top \mathbf{C} \mathbf{w}\right] \mathbf{w}\right)
\end{align}
$$

$\mathbf{C}:=\mathbb{E}[\mathbf{x}\mathbf{x}^\top]\in \mathbb{R}^{d\times d}$ã¨ã™ã‚‹ï¼$\mathbf{x}$ã®å¹³å‡ãŒ0ã®å ´åˆï¼Œ$\mathbf{C}$ã¯åˆ†æ•£å…±åˆ†æ•£è¡Œåˆ—ã§ã‚ã‚‹ï¼$\mathbb{E}\left[\dfrac{d\mathbf{w}}{dt}\right]=0$ã¨ãªã‚‹$\mathbf{w}$ãŒåæŸã™ã‚‹å›ºå®šç‚¹(fixed point)ã§ã¯æ¬¡ã®å¼ãŒæˆã‚Šç«‹ã¤ï¼

$$
\begin{equation}
\mathbf{C}\mathbf{w} = \lambda \mathbf{w}
\end{equation}
$$

ã“ã‚Œã¯å›ºæœ‰å€¤å•é¡Œã§ã‚ã‚Šï¼Œ$\lambda:=\mathbf{w}^\top \mathbf{C} \mathbf{w}$ã¯å›ºæœ‰å€¤ï¼Œ$\mathbf{w}$ã¯å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«(eigen vector)ã«ãªã‚‹ï¼

ã“ã“ã§ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’$n$ã¨ã—ï¼Œ$\mathbf{X} \in \mathbb{R}^{d\times n}, \mathbf{y}=\mathbf{X}^\top\mathbf{w} \in \mathbb{R}^n$ã¨ã™ã‚‹ï¼æ¨™æœ¬å¹³å‡ã§è¿‘ä¼¼ã—ã¦$\mathbf{C}\simeq \mathbf{X}\mathbf{X}^\top$ã¨ã™ã‚‹ï¼ã“ã®å ´åˆï¼Œ

$$
\begin{align}
\mathbb{E}\left[\frac{d\mathbf{w}}{dt}\right] &\simeq \eta \left(\mathbf{X}\mathbf{X}^\top \mathbf{w} - \left[\mathbf{w}^\top \mathbf{X}\mathbf{X}^\top \mathbf{w}\right] \mathbf{w}\right)\\
&=\eta \left(\mathbf{X}\mathbf{y} - \left[\mathbf{y}^\top\mathbf{y}\right] \mathbf{w}\right)
\end{align}
$$

ã¨ãªã‚‹ï¼

å¾Œã®ãŸã‚ã«Ojaå‰‡ã«ãŠã„ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ$q$å€‹ã®è¤‡æ•°å‡ºåŠ›ã‚’æŒã¤å ´åˆã‚’è€ƒãˆã‚ˆã†ï¼é‡ã¿è¡Œåˆ—ã‚’$\mathbf{W} \in \mathbb{R}^{q\times d}$, å‡ºåŠ›ã‚’$\mathbf{y}=\mathbf{W}\mathbf{x} \in \mathbb{R}^{q}, \mathbf{Y}=\mathbf{W}\mathbf{X} \in \mathbb{R}^{q\times n}$ã¨ã™ã‚‹ï¼ã“ã®å ´åˆã®æ›´æ–°å‰‡ã¯

$$
\begin{equation}
\frac{d\mathbf{W}}{dt} = \eta \left(\mathbf{y}\mathbf{x}^\top - \mathrm{Diag}\left[\mathbf{y}\mathbf{y}^\top\right] \mathbf{W}\right)
\end{equation}
$$

ã¨ãªã‚‹ï¼ãŸã ã—ï¼Œ$\mathrm{Diag}(\cdot)$ã¯è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã‹ã‚‰ãªã‚‹å¯¾è§’è¡Œåˆ—ã‚’ç”Ÿã¿å‡ºã™ä½œç”¨ç´ ã§ã‚ã‚‹ï¼

### Sangerå‰‡
Ojaå‰‡ã«è¤‡æ•°ã®å‡ºåŠ›ã‚’æŒãŸã›ãŸå ´åˆã§ã‚ã£ã¦ã‚‚ï¼Œå‡ºåŠ›ãŒç›´äº¤ã—ãªã„ãŸã‚ï¼ŒPCAã®ç¬¬1ä¸»æˆåˆ†ã—ã‹æ±‚ã‚ã‚‹ã“ã¨ãŒã§ããªã„ï¼**Sangerå‰‡ (Sanger's rule)**ï¼Œã‚ã‚‹ã„ã¯**ä¸€èˆ¬åŒ–Hebbå‰‡ (generalized Hebbian algorithm; GHA)** ã¯ï¼ŒOjaå‰‡ã«**Gramâ€“Schmidtã®æ­£è¦ç›´äº¤åŒ–æ³•(Gramâ€“Schmidt orthonormalization)** ã‚’çµ„ã¿åˆã‚ã›ãŸå­¦ç¿’å‰‡ã§ã‚ã‚Šï¼Œæ¬¡å¼ã§è¡¨ã•ã‚Œã‚‹ï¼

$$
\begin{equation}
\frac{d\mathbf{W}}{dt} = \eta \left(\mathbf{y}\mathbf{x}^\top - \mathrm{LT}\left[\mathbf{y}\mathbf{y}^\top\right] \mathbf{W}\right)
\end{equation}
$$

$\mathrm{LT}(\cdot)$ã¯è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã‚ˆã‚Šä¸Šå´ã®è¦ç´ ã‚’0ã«ã—ãŸä¸‹ä¸‰è§’è¡Œåˆ—(lower triangular matrix)ã‚’ä½œã‚Šå‡ºã™ä½œç”¨ç´ ã§ã‚ã‚‹ï¼Sangerå‰‡ã‚’ç”¨ã„ã‚Œã°PCAã®ç¬¬2ä¸»æˆåˆ†ä»¥é™ã‚‚æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼

### éç·šå½¢Hebbå­¦ç¿’
å‡ºåŠ›$\mathbf{y}$ã«éç·šå½¢é–¢æ•°$g(\cdot)$ã‚’é©ç”¨ã—ï¼Œ$\mathbf{y}\to g(\mathbf{y})$ã¨ã—ã¦ç½®ãæ›ãˆã‚‹ã“ã¨ã§éç·šå½¢Hebbå­¦ç¿’ã¨ãªã‚‹{cite:p}`Oja1997-hr`{cite:p}`Brito2016-mx`. é–¢æ•°`HebbianPCA`ã®`func`å¼•æ•°ã«éç·šå½¢é–¢æ•°ã‚’æ¸¡ã™ã“ã¨ã§å®Ÿç¾ã§ãã‚‹ï¼

ToDo: è©³ç´°

#### éè² ä¸»æˆåˆ†åˆ†æã«ã‚ˆã‚‹ã‚°ãƒªãƒƒãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‰µç™º
å†…å´å—…å†…çš®è³ª(MEC)ã«ã‚ã‚‹**ã‚°ãƒªãƒƒãƒ‰ç´°èƒ (grid cells)** ã¯å…­è§’å½¢æ ¼å­çŠ¶ã®ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šè‡ªå·±ä½ç½®ç­‰ã‚’ç¬¦å·åŒ–ã™ã‚‹ã®ã«è²¢çŒ®ã—ã¦ã„ã‚‹ï¼ã“ã®ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿã¿å‡ºã™ãƒ¢ãƒ‡ãƒ«ã¯å¤šæ•°ã‚ã‚‹ãŒï¼Œ**å ´æ‰€ç´°èƒ(place cells)** ã®ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’**éè² ä¸»æˆåˆ†åˆ†æ(nonnegative principal component analysis)** ã§æ¬¡å…ƒå‰Šæ¸›ã™ã‚‹ã¨ã‚°ãƒªãƒƒãƒ‰ç´°èƒã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç”Ÿã¾ã‚Œã‚‹ã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ {cite:p}`Dordek2016-ff`ï¼éç·šå½¢Hebbå­¦ç¿’ã‚’ç”¨ã„ã¦ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã‚ˆã†ï¼ãªãŠï¼ŒåŒæ§˜ã®ã“ã¨ã¯**éè² å€¤è¡Œåˆ—å› å­åˆ†è§£ (NMF: nonnegative matrix factorization)** ã§ã‚‚å¯èƒ½ã§ã‚ã‚‹ï¼

##### å ´æ‰€ç´°èƒã®ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³
ã¾ãšï¼Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãªã‚‹å ´æ‰€ç´°èƒã®ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’äººå·¥çš„ã«ä½œæˆã™ã‚‹ï¼å ´æ‰€ç´°èƒã®ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯**Difference of Gaussians (DoG)** ã§è¿‘ä¼¼ã™ã‚‹ï¼DoGã¯å¤§ãã•ã®ç•°ãªã‚‹2ã¤ã®ã‚¬ã‚¦ã‚¹é–¢æ•°ã®å·®åˆ†ã‚’å–ã£ãŸé–¢æ•°ã§ã‚ã‚Šï¼Œç”»åƒã«é©å¿œã™ã‚Œã°band-passãƒ•ã‚£ãƒ«ã‚¿ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ï¼ã¾ãŸï¼ŒDoGã¯ç¶²è†œç¥çµŒç¯€ç´°èƒç­‰ã®å—å®¹é‡ã®ONä¸­å¿ƒOFFå‘¨è¾ºå‹å—å®¹é‡ã®ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã‚‚ç”¨ã„ã‚‰ã‚Œã‚‹ï¼å—å®¹é‡ä¸­å¤®ã§ã¯æ´»å‹•ãŒå¤§ããï¼Œãã®å‘¨è¾ºã§ã¯æ´»å‹•ãŒæŠ‘åˆ¶ã•ã‚Œã‚‹ï¼Œã¨ã„ã†ç‰¹æ€§ã‚’æŒã¤ï¼2æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹é–¢æ•°ã¨DoGé–¢æ•°ã‚’å®Ÿè£…ã™ã‚‹ï¼

Place cellã®å—å®¹é‡ã‚’DoGã«è¨­å®šã—ãŸãŒï¼Œã“ã‚ŒãŒç„¡ã„ã¨æ ¼å­çŠ¶ã®å—å®¹é‡ã¯å‡ºç¾ã—ãªã„ï¼path integrationã‚’RNNã§å®Ÿè¡Œã™ã‚‹å ´åˆã‚‚åŒæ§˜ï¼ä¸€æ–¹ã§ï¼ŒDoGã¯å ´æ‰€ç´°èƒã®å—å®¹é‡ã¨ã—ã¦ã¯ä¸é©åˆ‡ã§ã‚ã‚‹ï¼

No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit 
<https://openreview.net/forum?id=mxi1xKzNFrb>

ToDo: ä»–ã®grid cellsã®ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦

## ç‹¬ç«‹æˆåˆ†åˆ†æ

The **learning rule** for the **InfoMax ICA** algorithm is derived by maximizing the **mutual information** (or equivalently, maximizing the entropy of the output). Below is a step-by-step derivation.

---

## **Step 1: Define the ICA Model**
We assume a **linear mixing model** where the observed signals \(X\) are mixtures of independent sources \( S \):

\[
X = A S
\]

where:
- \( S \) is an unknown vector of independent sources.
- \( A \) is the unknown **mixing matrix**.
- Our goal is to **recover \( S \) from \( X \)** by finding a **demixing matrix** \( W \):

\[
S' = W X
\]

where \( S' \) is an estimate of the true sources \( S \).

---

## **Step 2: InfoMax Principle**
The **InfoMax principle** suggests that maximizing the entropy of a **nonlinear** function of the sources leads to independent components.

Define a **nonlinear activation function** (sigmoid function):

\[
y_i = g(s_i') = g(w_i^T x)
\]

where \( g \) is the **sigmoid (logistic) function**:

\[
g(u) = \frac{1}{1 + e^{-u}}
\]

Since we want to maximize **mutual information**, this is equivalent to **maximizing the likelihood** of the sources.

---

## **Step 3: Log-likelihood Function**
The likelihood of the data given \( W \) is:

\[
p(X | W) = p(S') \left| \det W \right|
\]

Taking the log:

\[
\log p(X | W) = \sum_i \log p(y_i) + \log |\det W|
\]

For **super-Gaussian sources**, we assume \( p(y) \) follows a **sigmoid-like function**, so we approximate:

\[
p(y_i) \propto e^{-H(y_i)}
\]

where \( H(y_i) \) is the entropy of \( y_i \).

Thus, the **log-likelihood function** becomes:

\[
L(W) = \sum_i \sum_n \log g(w_i^T x_n) + \log |\det W|
\]

where the first term comes from maximizing entropy and the second term ensures invertibility.

---

## **Step 4: Gradient Ascent on Log-Likelihood**
To maximize \( L(W) \), we take its derivative with respect to \( W \):

\[
\frac{\partial L}{\partial W} = \sum_n \left[ (1 - 2 Y) X^T \right] + W^{-T}
\]

where:
- \( Y = g(WX) \) is the output after the nonlinearity.
- The term \( (1 - 2 Y) \) comes from the derivative of the sigmoid.

---

## **Step 5: Learning Rule**
Applying a **stochastic gradient ascent update**, we get the **InfoMax learning rule**:

\[
\Delta W \propto (\text{I} + (1 - 2 Y) X^T) W
\]

where \( I \) is the identity matrix.

---

## **Intuition Behind the Learning Rule**
1. **The term \( (1 - 2Y)X^T \) forces the network to decorrelate the sources** by reducing statistical dependencies.
2. **The term \( W \) ensures proper scaling and invertibility.**
3. **Gradient ascent ensures the network adapts \( W \) to maximize the entropy of the output.**

---

### **Final Thoughts**
This derivation follows from **maximum likelihood estimation (MLE)** under **non-Gaussian assumptions**. It is used in EEG, fMRI, and audio signal processing for **blind source separation (BSS)**.

Would you like to see an implementation of this in **Julia**? ğŸš€

## ä½é€Ÿç‰¹å¾´åˆ†æ
**Slow Feature Analysis (SFA)** ã¨ã¯, è¤‡æ•°ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã‹ã‚‰ä½é€Ÿã«å¤‰åŒ–ã™ã‚‹æˆåˆ† (slow feature) ã‚’æŠ½å‡ºã™ã‚‹æ•™å¸«ãªã—å­¦ç¿’ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ \citep{Wiskott2002-vb,Wiskott2011-uz}ï¼æ½œåœ¨å¤‰æ•° $y$ ã®æ™‚é–“å¤‰åŒ–ã®2ä¹—ã§ã‚ã‚‹ $\left(\frac{dy}{dt}\right)^2$ã‚’æœ€å°ã«ã™ã‚‹ã‚ˆã†ã«æ•™å¸«ãªã—å­¦ç¿’ã‚’è¡Œã†ï¼åˆæœŸè¦–è¦šé‡ã®å—å®¹é‡ \citep{Berkes2005-i} ã‚„æ ¼å­ç´°èƒãƒ»å ´æ‰€ç´°èƒãªã©ã®ãƒ¢ãƒ‡ãƒ«ã«å¿œç”¨ãŒã•ã‚Œã¦ã„ã‚‹ \citep{Franzius2007-sf}ï¼

ç”Ÿç†å­¦çš„å¦¥å½“æ€§ã«ã¤ã„ã¦ã¯ã„ãã¤ã‹ã®æ¤œè¨ãŒã•ã‚Œã¦ã„ã‚‹ï¼\citep{Sprekeler2007-qm} ã§ã¯STDPå‰‡ã«ã‚ˆã‚ŠSFAãŒå®Ÿç¾ã§ãã‚‹ã“ã¨ã‚’å ±å‘Šã—ã¦ã„ã‚‹ï¼å¤å…¸çš„ãªç·šå½¢Recurrent neural networkã§ã®å®Ÿè£…ã‚‚ææ¡ˆã•ã‚Œã¦ã„ã‚‹ \citep{Lipshutz2020-uj}ï¼

ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆã‚’è¡Œã†ï¼\citep{Wiskott2002-vb}ã§ç”¨ã„ã‚‰ã‚Œã¦ã„ã‚‹ãƒˆã‚¤ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã‚‹ï¼

## è‡ªå·±çµ„ç¹”åŒ–ãƒãƒƒãƒ—

### ç«¶åˆå­¦ç¿’

### è‡ªå·±çµ„ç¹”åŒ–ãƒãƒƒãƒ—ã¨è¦–è¦šé‡ã®æ§‹é€ 
è¦–è¦šé‡ã«ã¯ã‚³ãƒ©ãƒ æ§‹é€ ãŒå­˜åœ¨ã™ã‚‹ï¼ã“ã†ã—ãŸæ§‹é€ ã¯ç¥çµŒæ´»å‹•ä¾å­˜çš„ãªç™ºç”Ÿ  (activity dependent development) ã«ã‚ˆã‚Šç²å¾—ã•ã‚Œã‚‹ï¼æœ¬ç¯€ã§ã¯è¦–è¦šé‡ã®ã‚³ãƒ©ãƒ æ§‹é€ ã‚’ç”Ÿã¿å‡ºã™æ•°ç†ãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§ï¼Œ**è‡ªå·±çµ„ç¹”åŒ–ãƒãƒƒãƒ— (self-organizing map)** {cite:p}`Kohonen1982-mn`, {cite:p}`Kohonen2013-yt`ã‚’å–ã‚Šä¸Šã’ã‚‹ï¼

è‡ªå·±çµ„ç¹”åŒ–ãƒãƒƒãƒ—ã‚’è¦–è¦šé‡ã®æ§‹é€ ã«é©å¿œã—ãŸã®ã¯{cite:p}`Obermayer1990-gq` {cite:p}`N_V_Swindale1998-ri`ãªã©ã®ç ”ç©¶ã§ã‚ã‚‹ï¼è¦–è¦šé‡ãƒãƒƒãƒ—ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦è‡ªå·±çµ„ç¹”åŒ–ãƒãƒƒãƒ—ã¯å—å®¹é‡ã‚’è€ƒæ…®ã—ãªã„ãªã©ã®ç°¡ç•¥åŒ–ãŒãªã•ã‚Œã¦ã„ã‚‹ãŒï¼Œå˜ç´”ãªæ‰‹æ³•ã«ã—ã¦è¦–è¦šé‡ã®æ§‹é€ ã«é–¢ã™ã‚‹è‰¯ã„äºˆæ¸¬ã‚’ä¸ãˆã‚‹ï¼ä»–ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯è‡ªå·±çµ„ç¹”åŒ–ãƒãƒƒãƒ—ã¨ç™ºæƒ³ãŒé¡ä¼¼ã—ã¦ã„ã‚‹ **Elastic net**  {cite:p}`Durbin1987-bp` {cite:p}`Durbin1990-xx` {cite:p}`Carreira-Perpinan2005-gy`ã€€(ã“ã“ã§ã®Elastic netã¯æ­£å‰‡åŒ–æ‰‹æ³•ã¨ã—ã¦ã®Elastic net regularizationã¨ã¯ç•°ãªã‚‹)ã‚„å—å®¹é‡ã‚’æ˜ç¤ºçš„ã«è¨­å®šã—ãŸ {cite:p}`Tanaka2004-vz`ï¼Œ {cite:p}`Ringach2007-oe`ãªã©ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ï¼ç·èª¬ã¨ã—ã¦ã¯{cite:p}`Das2005-mq`ï¼Œ{cite:p}`Goodhill2007-va` ï¼Œæ•°ç†ãƒ¢ãƒ‡ãƒ«åŒå£«ã®é–¢ä¿‚ã«ã¤ã„ã¦ã¯{cite:p}`2002-nm`ãŒè©³ã—ã„ï¼

è‡ªå·±çµ„ç¹”åŒ–ãƒãƒƒãƒ—ã§ã¯ã€ŒæŠ¹æ¶ˆã‹ã‚‰ä¸­æ¢ã¸ã®ä¼é”éç¨‹ã§æå¤±ã•ã‚Œã‚‹æƒ…å ±é‡ã€ï¼ŒãŠã‚ˆã³ã€Œè¿‘ã„æ€§è³ªã‚’æŒã£ãŸãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³åŒå£«ãŒçµåˆã™ã‚‹ã‚ˆã†ãªé…ç·šé•·ã€ã®ä¸¡è€…ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ãªå­¦ç¿’ãŒè¡Œã‚ã‚Œã‚‹ï¼åŒ…æ‹¬æ€§ (coverage) ã¨é€£ç¶šæ€§ (continuity) ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ {cite:p}`Carreira-Perpinan2005-gy`ã€€ (Elastic netã¯ä¸¡è€…ã‚’æ˜ç¤ºçš„ã«è¨ˆç®—ã—ï¼Œç·šå½¢çµåˆã§è¡¨ã•ã‚Œã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ï¼Elastic netã¯æœ¬æ›¸ã§ã¯å–ã‚Šæ‰±ã‚ãªã„ãŒï¼ŒMATLABå®Ÿè£…ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹
<https://faculty.ucmerced.edu/mcarreira-perpinan/research/EN.html>) ï¼ é€£ç¶šæ€§ã¨é–¢é€£ã™ã‚‹äº‹é …ã¨ã—ã¦ï¼Œè¿‘ã„æ€§è³ªã‚’æŒã¤ç´°èƒãŒè„³å†…ã§è¿‘å‚ã«å­˜åœ¨ã™ã‚‹ã‚ˆã†ãªç™ºç”Ÿ/ç™ºé”éç¨‹ã‚’**ãƒˆãƒã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒãƒƒãƒ”ãƒ³ã‚° (topographic mapping)** ã¨å‘¼ã¶ï¼ãƒˆãƒã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒãƒƒãƒ”ãƒ³ã‚°ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸã®ç ”ç©¶ã¨ã—ã¦ã¯{cite:p}`Von_der_Malsburg1973-bz` {cite:p}`Willshaw1976-zo` {cite:p}`Takeuchi1979-mi`ãªã©ãŒã‚ã‚‹ï¼

ç™ºç”Ÿã®æ•°ç†ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹ç·èª¬ {cite:p}`Van_Ooyen2011-fz`, {cite:p}`Goodhill2018-ho`

### å˜ç´”ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
SOMã«ãŠã‘ã‚‹$n$ç•ªç›®ã®å…¥åŠ›ã‚’ $\mathbf{v}(t)=\mathbf{v}_n\in \mathbb{R}^{D} (n=1, \ldots, N)$ï¼Œ$m$ç•ªç›®ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³$ (m=1, \ldots, M) $ã®é‡ã¿ãƒ™ã‚¯ãƒˆãƒ« (ã¾ãŸã¯æ´»å‹•ãƒ™ã‚¯ãƒˆãƒ«, å‚ç…§ãƒ™ã‚¯ãƒˆãƒ«) ã‚’$\mathbf{w}_m(t)\in \mathbb{R}^{D}$ã¨ã™ã‚‹ {cite:p}`Kohonen2013-yt`ï¼ã¾ãŸï¼Œå„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ç‰©ç†çš„ãªä½ç½®ã‚’$\mathbf{x}_m$ã¨ã™ã‚‹ï¼ã“ã®ã¨ãï¼Œ$\mathbf{v}(t)$ã«å¯¾ã—ã¦$\mathbf{w}_m(t)$ã‚’æ¬¡ã®ã‚ˆã†ã«æ›´æ–°ã™ã‚‹ï¼

ã¾ãšï¼Œ$\mathbf{v}(t)$ã¨$\mathbf{w}_m(t)$ã®é–“ã®è·é›¢ãŒæœ€ã‚‚å°ã•ã„ (é¡ä¼¼åº¦ãŒæœ€ã‚‚å¤§ãã„) ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹ï¼è·é›¢ã‚„é¡ä¼¼åº¦ã¨ã—ã¦ã¯ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚„ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãªã©ãŒè€ƒãˆã‚‰ã‚Œã‚‹ï¼

$$
\begin{align}
&[\text{ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢}]: c = \underset{m}{\operatorname{argmin}}\left[\|\mathbf{v}(t)-\mathbf{w}_m(t)\|^2\right]\\
&[\text{ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦}]: c  = \underset{m}{\operatorname{argmax}}\left[\frac{\mathbf{w}_m(t)^\top\mathbf{v}(t)}{\|\mathbf{w}_m(t)\|\|\mathbf{v}(t)\|}\right]
\end{align}
$$

ã“ã®ï¼Œ$c$ç•ªç›®ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’**å‹è€…ãƒ¦ãƒ‹ãƒƒãƒˆ(best matching unit; BMU)** ã¨å‘¼ã¶ï¼ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«ãŠã„ã¦ï¼Œ$\mathbf{w}_m(t)^\top\mathbf{v}(t)$ã¯ç·šå½¢ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¨ãªã‚‹ï¼ã“ã®ãŸã‚ï¼Œã‚³ã‚µã‚¤ãƒ³è·é›¢ã‚’æ¡ç”¨ã™ã‚‹æ–¹ãŒç”Ÿç†å­¦çš„ã«å¦¥å½“ã§ã‚ã‚ŠSOMã®åˆæœŸã®ç ”ç©¶ã§ã¯ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãŒç”¨ã„ã‚‰ã‚Œã¦ã„ã‚‹ {cite:p}`Kohonen1982-mn`ï¼ã—ã‹ã—ï¼Œã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ç”¨ã„ã‚‹å ´åˆã¯$\mathbf{w}_m$ãŠã‚ˆã³$\mathbf{v}$ã‚’æ­£è¦åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’ç”¨ã„ã‚‹ã¨æ­£è¦åŒ–ãªã—ã§ã‚‚å­¦ç¿’ã§ãã‚‹ãŸã‚ï¼ŒSOMã‚’å¿œç”¨ã™ã‚‹ä¸Šã§ã¯ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ãŒæ¡ç”¨ã•ã‚Œã‚‹äº‹ãŒå¤šã„ï¼ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’ç”¨ã„ã‚‹å ´åˆï¼Œ$\mathbf{w}_m$ã¯é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã§ã¯ãªããªã‚‹ãŸã‚ï¼Œæ´»å‹•ãƒ™ã‚¯ãƒˆãƒ«ã‚„å‚ç…§ãƒ™ã‚¯ãƒˆãƒ«ã¨å‘¼ã°ã‚Œã‚‹ï¼ã“ã“ã§ã¯çµæœã®å®‰å®šæ€§ã‚’å„ªå…ˆã—ã¦ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’ç”¨ã„ã‚‹ã“ã¨ã¨ã™ã‚‹ï¼

ã“ã†ã—ã¦å¾—ã‚‰ã‚ŒãŸ$c$ã‚’ç”¨ã„ã¦$\mathbf{w}_m$ã‚’æ¬¡ã®ã‚ˆã†ã«æ›´æ–°ã™ã‚‹ï¼

$$
\begin{equation}
\mathbf{w}_m(t+1)=\mathbf{w}_m(t)+h_{cm}(t)[\mathbf{v}(t)-\mathbf{w}_m(t)]
\end{equation}
$$

ã“ã“ã§$h_{cm}(t)$ã¯è¿‘å‚é–¢æ•° (neighborhood function) ã¨å‘¼ã°ã‚Œï¼Œ$c$ç•ªç›®ã¨$m$ç•ªç›®ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®è·é›¢ãŒè¿‘ã„ã»ã©å¤§ããªå€¤ã‚’å–ã‚‹ï¼ã‚¬ã‚¦ã‚¹é–¢æ•°ã‚’ç”¨ã„ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã‚ã‚‹ï¼

$$
\begin{equation}
h_{cm}(t)=\alpha(t)\exp\left(-\frac{\|\mathbf{x}_c-\mathbf{x}_m\|^2}{2\sigma^2(t)}\right)
\end{equation}
$$

ã“ã“ã§$\mathbf{x}$ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ä½ç½®ã‚’è¡¨ã™ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹ï¼ã¾ãŸï¼Œ$\alpha(t), \sigma(t)$ã¯å˜èª¿ã«æ¸›å°‘ã™ã‚‹ã‚ˆã†ã«è¨­å®šã™ã‚‹ï¼\footnote{Generative topographic map (GTM)ã‚’ç”¨ã„ã‚Œã°$\alpha(t), \sigma(t)$ã®ç¸®å°ã¯å¿…è¦ãªã„ï¼ã¾ãŸï¼ŒSOMã¨GTMã®é–“ã‚’å–ã£ãŸãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦S-mapãŒã‚ã‚‹ï¼}