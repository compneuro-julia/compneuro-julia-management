{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896a4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from base64 import b64decode\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pykakasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97ad58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIndexing():\n",
    "    def __init__(self,):\n",
    "        self.kks = pykakasi.kakasi()\n",
    "    \n",
    "    def japanese_check(self, s):\n",
    "        # s: string\n",
    "        if re.search(r'[ぁ-ん]+|[ァ-ヴー]+|[一-龠]+', s):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def converter(self, s):\n",
    "        s = s.replace(r\"**\", \"\")\n",
    "        if self.japanese_check(s):\n",
    "            s_hira = \"\".join([word[\"hira\"] for word in self.kks.convert(s)])\n",
    "            return r'\\textbf{\\index{'+s_hira + \"@\" + s + '}}'\n",
    "        else:\n",
    "            return r'\\textbf{\\index{' + s + '}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e953df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_indexing = AutoIndexing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbc147",
   "metadata": {},
   "source": [
    "同じ文字が連続する場合は長い方から処理する．\n",
    "- ToDo: citationをどうにかする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c0f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '# aa\\n ## aaa\\n `this` is ``` **sample string** **漢字です** for *extracting substring*. {cite:p}`Echeveste2020-sh` <a>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95727cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\textbf{\\\\index{かんじです@漢字です}}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_indexing.converter(\"漢字です\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aed8dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown2latex(s):\n",
    "    # s: string\n",
    "    s = re.sub(r'\\####\\ (.+?)\\n', r'\\\\paragraph{\\1}\\n', s)  # subsubsection\n",
    "    s = re.sub(r'\\###\\ (.+?)\\n', r'\\\\subsubsection{\\1}\\n', s)  # subsubsection\n",
    "    s = re.sub(r'\\##\\ (.+?)\\n', r'\\\\subsection{\\1}\\n', s)  # subsection\n",
    "    s = re.sub(r'\\#\\ (.+?)\\n', r'\\\\section{\\1}\\n', s)      # section\n",
    "    \n",
    "    #s = re.sub(r'\\*\\*(.+?)\\*\\*', r'\\\\textbf{\\\\index{\\1}}', s)   # bold\n",
    "    s = re.sub(r'\\*\\*(.+?)\\*\\*', lambda m: auto_indexing.converter(m.group()), s)   # bold\n",
    "    #s = re.sub(r'\\*(.+?)\\*', r'\\\\textit{\\1}', s)       # italic\n",
    "    \n",
    "    s = s.replace(r\"```{note}\", \"\\\\footnote{\") # note to footnote\n",
    "    s = s.replace(r\"```\", \"}\")\n",
    "    s = re.sub(r'<(.+?)>', r'\\\\url{\\1}', s) # url\n",
    "\n",
    "    s = re.sub(r'{cite:p}`(.+?)`', r'\\\\cite{\\1}', s)     \n",
    "    s = re.sub(r'`(.+?)`', r'\\\\jl{\\1}', s) # inline code with \\newcommand{\\jl}{\\lstinline[language=julia]}\n",
    "\n",
    "    s = re.sub(r'<(.+?)>', r'\\\\url{\\1}', s) # url\n",
    "    s = s.replace(r\":=\", \"\\\\triangleq\")\n",
    "    s = s.replace(r\"（\", \" (\") \n",
    "    s = s.replace(r\"）\", \") \") \n",
    "    s = s.replace(r\"$$\", \"\") \n",
    "    #s = s.replace(r\"．\", \"．\\n\") \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281faa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_itemized(text):\n",
    "    #splited_text = text.split('\\n')\n",
    "    #splited_text = all_remove(splited_text, \"\\n\")\n",
    "    splited_text = list(filter(None, text))\n",
    "    # itemize\n",
    "    item_idx = [line[:2] == \"- \" for line in splited_text]\n",
    "    if np.sum(item_idx) > 0:\n",
    "        item_idx += [False]\n",
    "        item_startend = np.where(np.diff(np.array(item_idx)) == True)[0]\n",
    "        item_startend += np.arange(len(item_startend)) + 1\n",
    "\n",
    "        # replace - to \\item\n",
    "        for i in range(len(splited_text)):\n",
    "            if item_idx[i]:\n",
    "                splited_text[i] = splited_text[i].replace('- ', '\\item ', 1) \n",
    "\n",
    "        # add begin and end\n",
    "        for j in range(len(item_startend)):\n",
    "            if j % 2 == 0:\n",
    "                splited_text.insert(item_startend[j], \"\\\\begin{itemize}\")\n",
    "            else:\n",
    "                splited_text.insert(item_startend[j], \"\\\\end{itemize}\")\n",
    "    \n",
    "    # enumerate\n",
    "    enum_idx = [line[:3] == \"1. \" for line in splited_text]\n",
    "    if np.sum(enum_idx) > 0:\n",
    "        enum_idx += [False]\n",
    "        enum_startend = np.where(np.diff(np.array(enum_idx)) == True)[0]\n",
    "        enum_startend += np.arange(len(enum_startend)) + 1\n",
    "\n",
    "        # replace 1. to \\item\n",
    "        for i in range(len(splited_text)):\n",
    "            if enum_idx[i]:\n",
    "                splited_text[i] = splited_text[i].replace('1. ', '\\item ', 1) \n",
    "\n",
    "        # add begin and end\n",
    "        for j in range(len(enum_startend)):\n",
    "            if j % 2 == 0:\n",
    "                splited_text.insert(enum_startend[j], \"\\\\begin{enumerate}\")\n",
    "            else:\n",
    "                splited_text.insert(enum_startend[j], \"\\\\end{enumerate}\")\n",
    "\n",
    "    for i in range(len(splited_text)):\n",
    "        if splited_text[i][-1:] != \"\\n\":\n",
    "            splited_text[i] += \"\\n\"\n",
    "    return splited_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39487f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\section{aa}\\n \\\\subsection{aaa}\\n \\\\jl{this} is } \\\\textbf{\\\\index{sample string}} \\\\textbf{\\\\index{かんじです@漢字です}} for *extracting substring*. \\\\cite{Echeveste2020-sh} \\\\url{a}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown2latex(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f606d4",
   "metadata": {},
   "source": [
    "変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ca9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../contents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c6b1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_path + \"_toc.yml\") as file:\n",
    "    toc_yaml = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a79db7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'preface'},\n",
       " {'file': 'introduction/intro',\n",
       "  'sections': [{'file': 'introduction/computational-neuroscience'},\n",
       "   {'file': 'introduction/notation'},\n",
       "   {'file': 'introduction/usage-julia-lang'},\n",
       "   {'file': 'introduction/linear-algebra'},\n",
       "   {'file': 'introduction/differential-equation'},\n",
       "   {'file': 'introduction/linear-regression'},\n",
       "   {'file': 'introduction/probability-information-theory'},\n",
       "   {'file': 'introduction/stochastic-process-differential-equation'}]},\n",
       " {'file': 'neuron-model/intro',\n",
       "  'sections': [{'file': 'neuron-model/neuron-physiol'},\n",
       "   {'file': 'neuron-model/hodgkin-huxley'},\n",
       "   {'file': 'neuron-model/fhn'},\n",
       "   {'file': 'neuron-model/lif'},\n",
       "   {'file': 'neuron-model/izhikevich'},\n",
       "   {'file': 'neuron-model/isi'},\n",
       "   {'file': 'neuron-model/neurite-growth-model'}]},\n",
       " {'file': 'synapse-model/intro',\n",
       "  'sections': [{'file': 'synapse-model/synapse-physiol'},\n",
       "   {'file': 'synapse-model/current-conductance-synapse'},\n",
       "   {'file': 'synapse-model/expo-synapse'},\n",
       "   {'file': 'synapse-model/kinetic-synapse'},\n",
       "   {'file': 'synapse-model/synaptic-weighted'},\n",
       "   {'file': 'synapse-model/dynamical-synapses'}]},\n",
       " {'file': 'neuronal-computation/intro',\n",
       "  'sections': [{'file': 'neuronal-computation/neuronal-arithmetic'}]},\n",
       " {'file': 'local-learning-rule/intro',\n",
       "  'sections': [{'file': 'local-learning-rule/pca-hebbian-learning'},\n",
       "   {'file': 'local-learning-rule/mds-anti-hebbian-learning'},\n",
       "   {'file': 'local-learning-rule/slow-feature-analysis'},\n",
       "   {'file': 'local-learning-rule/stdp-learning'},\n",
       "   {'file': 'local-learning-rule/logistic-regression-perceptron'},\n",
       "   {'file': 'local-learning-rule/self-organizing-map'}]},\n",
       " {'file': 'energy-based-model/intro',\n",
       "  'sections': [{'file': 'energy-based-model/energy-based-model'},\n",
       "   {'file': 'energy-based-model/hopfield-model'},\n",
       "   {'file': 'energy-based-model/boltzmann-machine'},\n",
       "   {'file': 'energy-based-model/sparse-coding'},\n",
       "   {'file': 'energy-based-model/predictive-coding'}]},\n",
       " {'file': 'solve-credit-assignment-problem/intro',\n",
       "  'sections': [{'file': 'solve-credit-assignment-problem/backpropagation'},\n",
       "   {'file': 'solve-credit-assignment-problem/linear-network-learning-dynamics'},\n",
       "   {'file': 'solve-credit-assignment-problem/bptt'},\n",
       "   {'file': 'solve-credit-assignment-problem/surrogate-gradient-snn'},\n",
       "   {'file': 'solve-credit-assignment-problem/reservoir-computing'}]},\n",
       " {'file': 'motor-learning/intro',\n",
       "  'sections': [{'file': 'motor-learning/minimum-jerk'},\n",
       "   {'file': 'motor-learning/minimum-variance'},\n",
       "   {'file': 'motor-learning/optimal-feedback-control'},\n",
       "   {'file': 'motor-learning/infinite-horizon-ofc'},\n",
       "   {'file': 'motor-learning/local-learning-ofc'},\n",
       "   {'file': 'motor-learning/rat-trajectory'}]},\n",
       " {'file': 'reinforcement-learning/intro',\n",
       "  'sections': [{'file': 'reinforcement-learning/td-learning'}]},\n",
       " {'file': 'bayesian-brain/intro',\n",
       "  'sections': [{'file': 'bayesian-brain/neural-uncertainty-representation'},\n",
       "   {'file': 'bayesian-brain/bayesian-linear-regression'},\n",
       "   {'file': 'bayesian-brain/mcmc'},\n",
       "   {'file': 'bayesian-brain/neural-sampling'},\n",
       "   {'file': 'bayesian-brain/probabilistic-population-coding'},\n",
       "   {'file': 'bayesian-brain/quantile-expectile-regression'}]},\n",
       " {'file': 'appendix/intro',\n",
       "  'sections': [{'file': 'appendix/grid-cells-decoding'},\n",
       "   {'file': 'appendix/graph-theory-network-model'},\n",
       "   {'file': 'appendix/useful-links'},\n",
       "   {'file': 'appendix/usage-jupyter-book'}]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc_yaml['sections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c73ef2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_remove(xlist, remove):\n",
    "    return [value for value in xlist if value != remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28bc5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_ipynb2latex(dir_path, filename):\n",
    "    save_dir = \"./text/\" + filename\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_path = dir_path + filename\n",
    "    master_list = []\n",
    "    if os.path.isfile(file_path + \".md\"):\n",
    "        f = codecs.open(file_path + \".md\", 'r', encoding=\"utf8\")\n",
    "        md = f.read()\n",
    "        # convert\n",
    "        text = markdown2latex(md)\n",
    "        text = text.split('\\n')\n",
    "        text = latex_itemized(text) #all_remove(text, \"\\n\")\n",
    "        if not \":filter: docname in docnames\" in \"\".join(text):\n",
    "            # save\n",
    "            master_list += text\n",
    "    elif os.path.isfile(file_path + \".ipynb\"):\n",
    "        f = codecs.open(file_path + \".ipynb\", 'r', encoding=\"utf8\")\n",
    "        source = f.read()\n",
    "        y = json.loads(source)\n",
    "        num_cells = len(y['cells'])\n",
    "        for cell_idx in range(num_cells):\n",
    "            cell = y['cells'][cell_idx]\n",
    "            if cell['cell_type'] == 'markdown':\n",
    "                # convert\n",
    "                text = [markdown2latex(s) for s in cell['source']]\n",
    "                text = latex_itemized(text)\n",
    "                if not \":filter: docname in docnames\" in \"\".join(text):\n",
    "                    # save\n",
    "                    master_list += text\n",
    "                    #parted_file_path = save_dir + \"/{:03d}.tex\".format(cell_idx)\n",
    "                    #with open(parted_file_path, 'w', encoding='UTF-8') as f:\n",
    "                    #    f.writelines(text)\n",
    "                    #master_list.append(r\"\\input{\"+parted_file_path+\"}\\n\")\n",
    "            elif cell['cell_type'] == 'code':\n",
    "                # ToDo:'outputs'\n",
    "                code = cell['source']\n",
    "                parted_file_path = save_dir + \"/{:03d}.jl\".format(cell_idx)\n",
    "                with open(parted_file_path, 'w', encoding='UTF-8') as f:\n",
    "                    f.writelines(code)\n",
    "                master_list.append(r\"\\lstinputlisting[language=julia]{\"+parted_file_path+\"}\\n\")\n",
    "\n",
    "                if cell['outputs']:\n",
    "                    if 'data' in cell['outputs'][0]:\n",
    "                        output = cell['outputs'][0]['data']\n",
    "                        if \"image/png\" in output.keys():\n",
    "                            png_bytes = output['image/png']\n",
    "                            png_bytes = b64decode(png_bytes)\n",
    "                            bytes_io = BytesIO(png_bytes)\n",
    "                            image = Image.open(bytes_io)\n",
    "\n",
    "                            figname = \"cell{:03d}.png\".format(cell_idx)\n",
    "                            figsavepath = \"./fig/\" + filename + \"/\" + figname\n",
    "                            os.makedirs(\"./fig/\" + filename, exist_ok=True)\n",
    "                            image.save(figsavepath, 'png')\n",
    "\n",
    "                            caption = figname\n",
    "                            figlabel = figname #\"ccc\"\n",
    "                            figcode = \"\\\\begin{figure}[ht]\\n\\t\\centering\\n\"\n",
    "                            figcode += \"\\t\\includegraphics[scale=0.8, max width=\\linewidth]{\"+figsavepath+\"}\\n\"\n",
    "                            figcode += \"\\t\\caption{\" + caption + \"}\\n\"\n",
    "                            figcode += \"\\t\\label{\"+figlabel+\"}\\n\"\n",
    "                            figcode += \"\\end{figure}\"\n",
    "\n",
    "                            parted_output_path = save_dir + \"/output_{:03d}.tex\".format(cell_idx)\n",
    "                            with open(parted_output_path, 'w', encoding='UTF-8') as f:\n",
    "                                f.writelines(figcode)\n",
    "                            master_list.append(r\"\\input{\"+parted_output_path+\"}\\n\")\n",
    "                        \n",
    "                        elif \"text/plain\" in output.keys():\n",
    "                            print(output[\"text/plain\"])\n",
    "\n",
    "    with open(save_dir + '.tex', 'w', encoding='UTF-8') as f:\n",
    "        f.writelines(master_list)\n",
    "    return master_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf2708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"motor-learning/infinite-horizon-ofc\"\n",
    "#master_list = md_ipynb2latex(dir_path, filename)\n",
    "#master_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abf941ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecea1e5da324eedab0da68019d96499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preface\n",
      "introduction/intro\n",
      "introduction/computational-neuroscience\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "markdown2latex() missing 1 required positional argument: 'auto_indexing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m filename \u001b[38;5;241m=\u001b[39m subsection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(filename)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmd_ipynb2latex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m main_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m./text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfilename\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tex}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mmd_ipynb2latex\u001b[1;34m(dir_path, filename)\u001b[0m\n\u001b[0;32m      8\u001b[0m md \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# convert\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mmarkdown2latex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m latex_itemized(text) \u001b[38;5;66;03m#all_remove(text, \"\\n\")\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: markdown2latex() missing 1 required positional argument: 'auto_indexing'"
     ]
    }
   ],
   "source": [
    "main_list = []\n",
    "for i, section in tqdm(enumerate(toc_yaml['sections'])):\n",
    "    print(section['file']) # intro\n",
    "    if i > 0:\n",
    "        for subsection in section['sections']:\n",
    "            filename = subsection['file']\n",
    "            print(filename)\n",
    "            md_ipynb2latex(dir_path, filename)\n",
    "            main_list.append(r\"\\input{./text/\"+filename+\".tex}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"contents_list.tex\", 'w', encoding='UTF-8') as f:\n",
    "    f.writelines(main_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
