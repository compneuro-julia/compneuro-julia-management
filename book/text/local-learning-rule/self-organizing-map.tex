\section{自己組織化マップと視覚野の構造}
視覚野にはコラム構造が存在する．こうした構造は神経活動依存的な発生  (activity dependent development) により獲得される．本節では視覚野のコラム構造を生み出す数理モデルの中で，\textbf{自己組織化マップ (self-organizing map)}\index{じこそしきかまっぷ (self-organizing map)@自己組織化マップ (self-organizing map)} \citep{Kohonen1982-mn}, \citep{Kohonen2013-yt}を取り上げる．
自己組織化マップを視覚野の構造に適応したのは\citep{Obermayer1990-gq} \citep{N_V_Swindale1998-ri}などの研究である．視覚野マップの数理モデルとして自己組織化マップは受容野を考慮しないなどの簡略化がなされているが，単純な手法にして視覚野の構造に関する良い予測を与える．他の数理モデルとしては自己組織化マップと発想が類似している \textbf{Elastic net}\index{Elastic net}  \citep{Durbin1987-bp} \citep{Durbin1990-xx} \citep{Carreira-Perpinan2005-gy}　(ここでのElastic netは正則化手法としてのElastic net regularizationとは異なる)や受容野を明示的に設定した \citep{Tanaka2004-vz}， \citep{Ringach2007-oe}などのモデルがある．総説としては\citep{Das2005-mq}，\citep{Goodhill2007-va} ，数理モデル同士の関係については\citep{2002-nm}が詳しい．
自己組織化マップでは「抹消から中枢への伝達過程で損失される情報量」，および「近い性質を持ったニューロン同士が結合するような配線長」の両者を最小化するような学習が行われる．包括性 (coverage) と連続性 (continuity) のトレードオフとも呼ばれる \citep{Carreira-Perpinan2005-gy}　 (Elastic netは両者を明示的に計算し，線形結合で表されるエネルギー関数を最小化する．Elastic netは本書では取り扱わないが，MATLAB実装が公開されている
\url{https://faculty.ucmerced.edu/mcarreira-perpinan/research/EN.html}) ． 連続性と関連する事項として，近い性質を持つ細胞が脳内で近傍に存在するような発生/発達過程を\textbf{トポグラフィックマッピング (topographic mapping)}\index{とぽぐらふぃっくまっぴんぐ (topographic mapping)@トポグラフィックマッピング (topographic mapping)} と呼ぶ．トポグラフィックマッピングの数理モデルの初期の研究としては\citep{Von_der_Malsburg1973-bz} \citep{Willshaw1976-zo} \citep{Takeuchi1979-mi}などがある．
発生の数理モデルに関する総説 \citep{Van_Ooyen2011-fz}, \citep{Goodhill2018-ho}
\subsection{単純なデータセット}
SOMにおける$n$番目の入力を $\mathbf{v}(t)=\mathbf{v}_n\in \mathbb{R}^{D} (n=1, \ldots, N)$，$m$番目のニューロン$ (m=1, \ldots, M) $の重みベクトル (または活動ベクトル, 参照ベクトル) を$\mathbf{w}_m(t)\in \mathbb{R}^{D}$とする \citep{Kohonen2013-yt}．また，各ニューロンの物理的な位置を$\mathbf{x}_m$とする．このとき，$\mathbf{v}(t)$に対して$\mathbf{w}_m(t)$を次のように更新する．
まず，$\mathbf{v}(t)$と$\mathbf{w}_m(t)$の間の距離が最も小さい (類似度が最も大きい) ニューロンを見つける．距離や類似度としてはユークリッド距離やコサイン類似度などが考えられる．
\begin{align}
&[\text{ユークリッド距離}]: c = \underset{m}{\operatorname{argmin}}\left[\|\mathbf{v}(t)-\mathbf{w}_m(t)\|^2\right]\\
&[\text{コサイン類似度}]: c  = \underset{m}{\operatorname{argmax}}\left[\frac{\mathbf{w}_m(t)^\top\mathbf{v}(t)}{\|\mathbf{w}_m(t)\|\|\mathbf{v}(t)\|}\right]
\end{align}
この，$c$番目のニューロンを\textbf{勝者ユニット(best matching unit; BMU)}\index{しょうしゃゆにっと(best matching unit; BMU)@勝者ユニット(best matching unit; BMU)} と呼ぶ．コサイン類似度において，$\mathbf{w}_m(t)^\top\mathbf{v}(t)$は線形ニューロンモデルの出力となる．このため，コサイン距離を採用する方が生理学的に妥当でありSOMの初期の研究ではコサイン類似度が用いられている \citep{Kohonen1982-mn}．しかし，コサイン類似度を用いる場合は$\mathbf{w}_m$および$\mathbf{v}$を正規化する必要がある．ユークリッド距離を用いると正規化なしでも学習できるため，SOMを応用する上ではユークリッド距離が採用される事が多い．ユークリッド距離を用いる場合，$\mathbf{w}_m$は重みベクトルではなくなるため，活動ベクトルや参照ベクトルと呼ばれる．ここでは結果の安定性を優先してユークリッド距離を用いることとする．
こうして得られた$c$を用いて$\mathbf{w}_m$を次のように更新する．
\begin{equation}
\mathbf{w}_m(t+1)=\mathbf{w}_m(t)+h_{cm}(t)[\mathbf{v}(t)-\mathbf{w}_m(t)]
\end{equation}
ここで$h_{cm}(t)$は近傍関数 (neighborhood function) と呼ばれ，$c$番目と$m$番目のニューロンの距離が近いほど大きな値を取る．ガウス関数を用いるのが一般的である．
\begin{equation}
h_{cm}(t)=\alpha(t)\exp\left(-\frac{\|\mathbf{x}_c-\mathbf{x}_m\|^2}{2\sigma^2(t)}\right)
\end{equation}
ここで$\mathbf{x}$はニューロンの位置を表すベクトルである．また，$\alpha(t), \sigma(t)$は単調に減少するように設定する．\footnote{Generative topographic map (GTM)を用いれば$\alpha(t), \sigma(t)$の縮小は必要ない．また，SOMとGTMの間を取ったモデルとしてS-mapがある．}
\begin{lstlisting}[language=julia]
using Random, PyPlot, ProgressMeter
using PyPlot: matplotlib
rc("font", family="Arial")
\end{lstlisting}
\begin{lstlisting}[language=julia]
using PyCall
@pyimport numpy as np
\end{lstlisting}
ToDo: dimsをv, wで修正
\begin{lstlisting}[language=julia]
# inputs 
Random.seed!(1234);
σv, σw = 0.1, 0.05
dims = 2  # dims of inputs and neurons
num_v = 300 # num of inputs
num_blobs = 5 # num. cluster of dataset 
num_w_sqrt = 15 # must be int
num_w = num_w_sqrt^2
init_w = σw*randn(num_w, dims);
\end{lstlisting}
\begin{lstlisting}[language=julia]
# 単位円上に等間隔にならんだクラスターによるtoy datasetを作成する
function make_blobs(num_samples, num_blobs, dims, σ)
    n = Int(num_samples/num_blobs) # number of samples in each 
    data = vcat([σ*randn(n, dims) .+ [cos(i/num_blobs*2π), sin(i/num_blobs*2π)]' for i in 0:num_blobs-1]...)
    label = repeat(1:num_blobs, inner=n)
    return data, label
end
\end{lstlisting}
\begin{lstlisting}[language=julia]
v, v_labels = make_blobs(num_v, num_blobs, dims, σv);
\end{lstlisting}
\begin{lstlisting}[language=julia]
function plot_som(v, w, ax; vcolor="tab:blue")
    num_w, dims = size(w)
    num_w_sqrt = Int(sqrt(num_w))
    rw = reshape(w, (num_w_sqrt, num_w_sqrt, dims))
    ax.scatter(v[:, 1], v[:, 2], s=10, color=vcolor)
    ax.plot(rw[:, :, 1], rw[:, :, 2], "k", alpha=0.5); 
    ax.plot(rw[:, :, 1]', rw[:, :, 2]', "k", alpha=0.5)
    ax.scatter(w[:, 1], w[:, 2], s=5, fc="white", ec="k", zorder=99) # w[i, j, 1]とw[i, j, 2]の点をプロット
end;
\end{lstlisting}
近傍関数 (neighborhood function)のための二次元ガウス関数を実装する．Winnerニューロンからの距離に応じて値が減弱する関数である．ここでは一つの入力に対して全てのニューロンの活動ベクトルを更新するということはせず，winner neuronの近傍のニューロンのみ更新を行う．つまり，更新においてはglobalではなくlocalな処理のみを行うということである  (Winner neuronの決定にはWTAによるglobalな評価が必要ではあるが) ．
自己組織化マップのメインとなる関数を書く．ナイーブに実装する．この方法だと空間が円，球体やトーラスのように周期性を持つ場合にも適応できる．
\begin{lstlisting}[language=julia]
function som(v, init_w; α0=1.0, σ0=6, T=500, dist_mat=nothing, return_history=true)
    # α0: update rate, σ0 : width, T : training steps
    w = copy(init_w)
    num_w = size(init_w)[1]
    num_w_sqrt = Int(sqrt(num_w))
    num_v = size(v)[1]
    
    if return_history
        w_history = [copy(init_w)] # history of w
    end
    
    if dist_mat == nothing
        pos = hcat([[i, j] for i in 1:num_w_sqrt for j in 1:num_w_sqrt]...)
        dist_mat = hcat([sum((pos .- pos[:, i]) .^2, dims=1)' for i in 1:num_w]...); #'
    end
    
    @showprogress for t in 1:T
        α = α0 * (1 - t/T); # update rate
        σ = max(σ0 * (1 - t/T), 1); # decay from large to small (linearly decreased, avoid zero)
        exp_dist_mat = exp.(-dist_mat / (2.0(σ^2)))
        exp_dist_mat ./= maximum(sum(exp_dist_mat, dims=1))
        # loop for the num_v inputs
        for i in 1:num_v
            dist = sum((v[i, :]' .- w).^2, dims=2) # distance between input and neurons
            win_idx = argmin(dist)[1] # winner index
            # update the winner & neighbor neuron
            η = α * exp_dist_mat[win_idx, :]
            w[:, :] += η .* (v[i, :]' .- w)
        end
        if return_history
            append!(w_history, [copy(w)]) # save w
        end
    end
    if return_history
        return w_history
    else
        return w
    end
end;
\end{lstlisting}
今回のように2次元のみを扱う場合はwinner neuronの周辺だけをsliceで抜き出して重み更新する方が高速である．
\begin{lstlisting}[language=julia]
# Gaussian mask for inputs
function gaussian_mask(sizex=9, sizey=9; σ=5)
    x, y = 0:sizex-1, 0:sizey-1
    X, Y = ones(sizey) * x', y * ones(sizex)' 
    x0, y0 = (sizex-1) / 2, (sizey-1) / 2
    mask = exp.(-((X .- x0) .^2 + (Y .- y0) .^2) / (2.0(σ^2)))
    return mask ./ sum(mask)
end;
\end{lstlisting}
\begin{lstlisting}[language=julia]
function som_2d(v, init_w; α0=1.0, σ0=6, T=500, return_history=true)
    # α0: update rate, σ0 : width, T : training steps
    w = copy(init_w)
    num_w, dims = size(init_w)
    num_w_sqrt = Int(sqrt(num_w))
    num_v = size(v)[1]
    
    w_history = [copy(w)] # history of w
    
    w_2d = reshape(w, (num_w_sqrt, num_w_sqrt, dims))
    
    if return_history
        w_history = [copy(init_w)] # history of w
    end
    
    @showprogress for t in 1:T
        α = α0 * (1 - t/T); # update rate
        σ = max(σ0 * (1 - t/T), 1); # decay from large to small (linearly decreased, avoid zero)
        wm = ceil(Int, σ)
        h = gaussian_mask(2wm+1, 2wm+1, σ=σ);
        # loop for the num_v inputs
        for i in 1:num_v
            dist = sum([(v[i, j] .- w_2d[:, :, j]).^2 for j in 1:dims]) # distance between input and neurons
            win_idx = argmin(dist) # winner index
            idx = [max(1,win_idx[j] - wm):min(num_w_sqrt, win_idx[j] + wm) for j in 1:2] # neighbor indices
            # update the winner & neighbor neuron
            η = α * h[1:length(idx[1]), 1:length(idx[2])]
            for j in 1:dims
                w_2d[idx..., j] += η .* (v[i, j] .- w_2d[idx..., j])
            end
        end
        if return_history
            w = reshape(w_2d, (num_w, dims))
            append!(w_history, [copy(w)]) # save w
        end
    end
    if return_history
        return w_history
    else
        w = reshape(w_2d, (num_w, dims))
        return w
    end
end;
\end{lstlisting}
\begin{lstlisting}[language=julia]
w_history = som(v, init_w, α0=2, σ0=10, T=100);
#w_history = som_2d(v, init_w, α0=2, σ0=10, T=100);
\end{lstlisting}
赤色の点がデータ位置$\mathbf{v}$，白色の点が重み位置$\mathbf{w}$である．黒線はニューロン間の位置関係を表す (これはWeight unfolding diagramsと呼ばれる)．下段のヒートマップは$\mathbf{w}$の一番目の次元を表す．学習が進むとともに近傍のニューロンが近い活動ベクトルを持つことがわかる．
\begin{lstlisting}[language=julia]
cm = get_cmap(:Reds) 
vcolors = cm.(v_labels / num_blobs);
\end{lstlisting}
\begin{lstlisting}[language=julia]
indices = [1, 50, 100]
fig, axes = subplots(2, length(idx), figsize=(6, 4), sharey="row", subplot_kw=Dict("box_aspect"=>1))
for (i, idx) = enumerate(indices)
    wh = w_history[idx]
    axes[1, i].set_title("Epoch : "*string(idx))
    axes[1, i].spines["right"].set_visible(false)
    axes[1, i].spines["top"].set_visible(false)
    plot_som(v, wh, axes[1, i], vcolor=vcolors);
    axes[2, i].imshow(reshape(wh[:, 1], (num_w_sqrt, num_w_sqrt)));
end
axes[1, 1].set_ylabel("Weight unfolding\n in data space")
axes[2, 1].set_ylabel("1st dim. weight")
tight_layout()
\end{lstlisting}
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.8, max width=\linewidth]{./fig/local-learning-rule/self-organizing-map/cell018.png}
	\caption{cell018.png}
	\label{cell018.png}
\end{figure}
次に$U$-matrix (unified distance matrix) という隣接ユニットの重み間の距離を定義する．$U$-matrixの$i$番目の要素 $U_i\ (i=1, \ldots, M)$ は次式で定義される：
U_{i}\coloneqq \sqrt{\frac{1}{|\mathcal{A}_i|} \sum_{j \in \mathcal{A}_i} \left\|\mathbf{w}_{i} - \mathbf{w}_{j}\right\|^2}
ここで $\mathcal{A}_i=\left\{m \in \{1, \ldots, M\} \mid d(\textbf{x}_i, \textbf{x}_m) = 1\right\}$ であり， $d(\mathbf{x}, \mathbf{y})=\|\mathbf{x} - \mathbf{y}\|_2$ とする．隣接する要素とは位置の差の絶対値が1であることを利用する．$U$-matrixを計算することでクラスター境界を可視化できる．
\begin{lstlisting}[language=julia]
function u_matrix2d(w)
    num_w = size(w)[1]
    num_w_sqrt = Int(sqrt(num_w))
    pos = hcat([[i, j] for i in 1:num_w_sqrt for j in 1:num_w_sqrt]...)
    abs_dist_mat = hcat([sum(abs.(pos .- pos[:, i]), dims=1)' for i in 1:num_w]...)
    adj_indices = [findall(x -> x == 1, abs_dist_mat[i, :]) for i in 1:num_w] # adjacent indices
    U = [sqrt(sum((w[adj_indices[i], :] .- w[i, :]') .^2) / size(adj_indices[i])[1]) for i in 1:num_w]
    U = reshape(U, (num_w_sqrt, num_w_sqrt));
    return U
end
\end{lstlisting}
\begin{lstlisting}[language=julia]
# find best matching unit
function find_bmu(v, w)
    num_v, dims = size(v)
    num_w = size(init_w)[1]
    num_w_sqrt = Int(sqrt(num_w))

    pos = hcat([[i, j] for i in 1:num_w_sqrt for j in 1:num_w_sqrt]...)
    mapped_vpos = zeros(num_v, dims);
    for i in 1:num_v
        dist = sum((v[i, :]' .- w).^2, dims=2) # distance between input and neurons
        win_idx = argmin(dist)[1] # winner index
        mapped_vpos[i, :] = pos[:, win_idx]' .- 1
    end
    return mapped_vpos
end
\end{lstlisting}
\begin{lstlisting}[language=julia]
U = u_matrix2d(w_history[end]);
\end{lstlisting}
\begin{lstlisting}[language=julia]
mapped_vpos = find_bmu(v, w_history[end]);
unique_mapped_vpos, indices, counts = np.unique(mapped_vpos, axis=0, return_index=true, return_counts=true);
\end{lstlisting}
\begin{lstlisting}[language=julia]
fig, ax = subplots(figsize=(3, 3), subplot_kw=Dict("box_aspect"=>1))
ax.set_title(L"$U$-matrix")
ax.pcolormesh(U, cmap="Blues", edgecolors="w", linewidth=0.5)
ax.scatter(unique_mapped_vpos[:, 1].+0.5, unique_mapped_vpos[:, 2].+0.5, s=counts*10, color=vcolors[indices.+1])
tight_layout()
\end{lstlisting}
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.8, max width=\linewidth]{./fig/local-learning-rule/self-organizing-map/cell024.png}
	\caption{cell024.png}
	\label{cell024.png}
\end{figure}
\subsection{視覚野マップ}
集合の直積を配列として返す関数 \jl{product}と極座標を直交座標に変換する関数 \jl{pol2cart}を用意する．
\begin{lstlisting}[language=julia]
product(sets...) = hcat([collect(x) for x in Iterators.product(sets...)]...)' # Array of Cartesian product of sets 
pol2cart(θ, r) = r*[cos(θ), sin(θ)];
\end{lstlisting}
刺激と初期の活動ベクトルは\citep{Carreira-Perpinan2005-gy}　を参考に作成．
SOMの学習に用いる刺激は画像ではなく，刺激間の距離が保たれるようなパラメータのみを使用する．刺激空間は5つの次元を持つ．
視野における受容野の中心座標 ($x, y \in [0, 1]$), 眼優位性 (ocular dominance; OD) $\in [-\ell_\textrm{OD}, \ell_\textrm{OD}]$, 方位 (orientation preference)から成る． 方位選択性における方位$\theta$は$[-\pi/2, \pi/2]$の範囲の値を取る．ここで，$f(-\pi/2)=f(\pi/2)$かつ，$\Delta f(\theta) = f(\theta+\Delta \theta)-f(\theta)$が任意の$\theta$で等しくなるという2条件を満たすには，$f(\theta; r)=[r\cos(2\theta), r\sin(2\theta)]\ (r>0)$とすればよい．実質的に単位円上から均等に点をサンプリングすることとなる．これらの刺激から直積\jl{product}で全ての組の入力を作成する．
\begin{lstlisting}[language=julia]
# generate stimulus
Random.seed!(1234);
Nx, Ny, NOD, NOR = 10, 10, 2, 12
dims = 5  # dims of inputs 
l, r = 0.14, 0.2

rx, ry = range(0, 1, length=Nx), range(0, 1, length=Ny)
rOD = range(-l, l, length=NOD)
rORθ = range(-π/2, π/2, length=NOR+1)[1:end-1]

# stimuli
rORxy = hcat(pol2cart.(2rORθ, r)...)
v = product(rx, ry, rOD, rORxy[1, :], rORxy[2, :]);
\end{lstlisting}
\begin{lstlisting}[language=julia]
# initial neurons
num_w_sqrt = 64
num_w = num_w_sqrt^2
init_w = product(range(0, 1, length=num_w_sqrt), range(0, 1, length=num_w_sqrt))
init_w += (rand(size(init_w)...) .- 1) * 0.05;
init_w = [init_w l*(2bitrand(num_w).- 1) hcat(pol2cart.(2π*(rand(num_w) .- 0.5), r)...)'];
\end{lstlisting}
 \jl{w_history}を用いてアニメーションを作成すると発達の過程が可視化される．
\begin{lstlisting}[language=julia]
w = som_2d(v, init_w, α0=1.5, σ0=5.0, T=50, return_history=false); # faster
#w = som(v, init_w, α0=1.5, σ0=5.0, T=50, return_history=false);
\end{lstlisting}
\begin{lstlisting}[language=julia]
rw = reshape(w, (num_w_sqrt, num_w_sqrt, dims))
ORmap = atan.(rw[:, :, 5], rw[:, :, 4]) / 2; # get angle of polar 
x = 0:num_w_sqrt-1
X = ones(num_w_sqrt) * x';
\end{lstlisting}
\begin{lstlisting}[language=julia]
fig, axes = subplots(2,2, figsize=(7, 6), subplot_kw=Dict("box_aspect"=>1))#, adjustable="box", aspect=1)
fig.subplots_adjust(hspace=0)
axes[1,1].set_title("Retinotopic map")
plot_som(v, w, axes[1,1])
axes[1,2].set_title("Ocular dominance (OD) map")
od_map = axes[1,2].imshow(rw[:, :, 3], cmap="gray", origin="lower")
ins1 = axes[1,2].inset_axes([1.05,0,0.05,1])
fig.colorbar(od_map, cax=ins1, aspect=40, pad=0.08, shrink=0.6)
ins1.text(0, -0.16, "Left", ha="left", va="center")
ins1.text(0, 0.16, "Right", ha="left", va="center")
axes[2,1].set_title("Contours of OD and OR")
axes[2,1].contour(X, X', ORmap, cmap="hsv")
axes[2,1].contour(X, X', rw[:, :, 3], colors="k", levels=1)
axes[2,2].set_title("Orientation (OR) angle map")
axes[2,2].imshow(ORmap, cmap="hsv", origin="lower")

cm = get_cmap(:hsv)
lines, colors = [], []
for i in 1:9
    θ = (i-1)/8*π - pi/2
    c, s = cos(θ), sin(θ)
    push!(lines, [(-c/2, 15-1.5i -s/2), (c/2, 15-1.5i + s/2)])
    push!(colors, cm(1/8*(i-1)))
end

ins2 = axes[2,2].inset_axes([1,0,0.2,1])
ins2.add_collection(matplotlib.collections.LineCollection(lines, linewidths=3,color=colors))
ins2.set_aspect("equal")
ins2.axis("off")
ins2.set_xlim(-1, 1); ins2.set_ylim(0, 15)
tight_layout()
\end{lstlisting}
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.8, max width=\linewidth]{./fig/local-learning-rule/self-organizing-map/cell033.png}
	\caption{cell033.png}
	\label{cell033.png}
\end{figure}
\begin{lstlisting}[language=julia]
方位選択性マップにはピンホイール (pinwheel) という特異点 (singular point) が存在する．
\end{lstlisting}
\begin{lstlisting}[language=julia]
px = range(-1,1,length=50)
pX = ones(length(px)) * px';

fig, axes = subplots(1,2, figsize=(3.5, 2), subplot_kw=Dict("box_aspect"=>1))
axes[1].imshow(atan.(pX, pX'), cmap="hsv")
axes[2].imshow(atan.(-pX, pX'), cmap="hsv")
axes[1].set_title("CCW (pos.) pinwheel")
axes[2].set_title("CW (neg.) pinwheel")
axes[1].axis("off")
axes[2].axis("off")
fig.tight_layout()
\end{lstlisting}
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.8, max width=\linewidth]{./fig/local-learning-rule/self-organizing-map/cell035.png}
	\caption{cell035.png}
	\label{cell035.png}
\end{figure}
correlation to the azimuthでdetectをする
Ohki先生の論文を引用する
https://pubmed.ncbi.nlm.nih.gov/16906137/
https://www.nature.com/articles/25736
の密度を計算する．(Kaschube et al. 2010).
\begin{lstlisting}[language=julia]

\end{lstlisting}
