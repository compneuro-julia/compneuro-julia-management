結果は図\ref{fig:LIF_FORCE_1}, \ref{fig:LIF_FORCE_2}のようになります。また、同様のシミュレーションをIzhikevichニューロンで行った\footnote{コードは\texttt{./TrainingSNN/Izhikevich\_FORCE\_sinewave.py}です。}結果も示しています。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.4]{figs/LIF_FORCE_prepost.pdf}
    \includegraphics[scale=0.4]{figs/Iz_FORCE_prepost.pdf}
    \caption{FORCE法による学習前(左)と学習後(右)の発火率の変化。(上)LIFニューロン、(下)をIzhikevichニューロン}
    \label{fig:LIF_FORCE_1}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figs/LIF_FORCE_decoded.pdf}
    \caption{FORCE法による学習前(左)と学習後(右)のデコード結果の変化(LIFニューロンの場合)。教師信号は実線、デコード結果は破線で示している。}
    \label{fig:LIF_FORCE_2}
\end{figure}
\subsection{鳥の鳴き声の再現と海馬の記憶と再生}
(Nicola \& Clopath, 2017)では教師信号として正弦波以外にもVan der Pol方程式やLorenz方程式の軌道を用いて実験しています。さらに教師信号としてベートーヴェンの歓喜の歌(Ode to joy)や鳥の鳴き声を用いても学習可能であったようです。\par
話は少しずれますが、小鳥の運動前野である\textbf{HVC}には連鎖的に結合したニューロン群が存在します。これはリズムを生み出すための計時に関わっているといわれています。カナリアのHVCニューロンを実験的に損傷(ablation)させると歌が歌えなくなるという実験がありますが、同様にSNNのHVCパターンをablationすると学習した歌が再生できなくなったようです。このような計時に関わるパターンを\textbf{HDTS}(high-dimentional temporal signal)とNicolaらは呼んでいます。HDTSを学習させた後に歓喜の歌を学習させると、HDTSがない場合よりも短い時間かつ高精度で学習できたようです。\par
さらにHDTSを外部入力とし、同時に映像を学習させる、という実験もしています(HDTSを内的に学習させる場合も行っています)。ネットワークは記録した映像を実時間で再生することができましたが、外部信号のHDTSを加速させることで圧縮再生が可能だったそうです。さらにHDTSを逆にすると、逆再生もできたそうです。\par
ニューロンの発火のタスク依存的な圧縮は実験的に観察されています(例えばEuston, et al., 2007)。空間的な課題(箱の中に入れて探索させるなど)をラットにさせると、課題中に記憶された場所細胞の順序だった活動は、ラットの睡眠中に圧縮再生されるという実験結果があります。その圧縮比は5.4〜8.1だったそうですが、この比率はSNNが映像を大きな損失なく再生できる圧縮比とほぼ同じであったようです。Nicolaらはさらに進んでSNNを用いて海馬における急速圧縮学習の機構における介在細胞の働きについての研究も行っています(Nicola \& Clopath, 2019)。
\section{RLS法の導出}
ここからはRLS法の導出を行います(cf. Haykin, 2002)。RLS法では次の損失関数$C\in \mathbb{R}^{N_\text{out}}$を最小化するような重み$\phi=[\boldsymbol{\phi}_j]\in \mathbb{R}^{N\times N_\text{out}}$を求めます。シミュレーション時間を$T$とすると、$C$は
\begin{equation}
C=\int_{0}^T(\hat{\boldsymbol{x}}(t)-\boldsymbol{x}(t))^{2} \mathrm{d} t+\lambda \phi^\intercal \phi
\end{equation}
です。ただし、$\hat{\boldsymbol{x}}(t), \boldsymbol{x}(t) \in \mathbb{R}^{N_\text{out}}$です。\par
さて、式の$C$を最小化するような$\phi$を数値的に求めるためには、損失関数の近似が必要です。まず、
時間幅$\Delta t$で$C$を離散化します。さらに$n$ステップ目における重み$\phi(n)$により、$\hat{\boldsymbol{x}}(i)\simeq \phi(n)^\intercal \boldsymbol{r}(i)$と近似します。このとき、$n$ステップ目の損失関数$C(n)$は
\begin{align}
C(n)&\simeq \sum_{i=0}^{n}(\hat{\boldsymbol{x}}(i)-\boldsymbol{x}(i))^{2}+\lambda \phi(n)^\intercal \phi(n)\\     
&\simeq \sum_{i=0}^{n}(\phi(n)^\intercal \boldsymbol{r}(i)-\boldsymbol{x}(i))^{2}+\lambda \phi(n)^\intercal \phi(n)
\end{align}
となります。ここでL2正則化(ridge)付きの(通常の)最小二乗法の\textbf{正規方程式}(normal equation)により、$C(n)$を最小化する$\phi(n)$は
\begin{align}
\phi(n) &= \left[\sum_{i=0}^{n}(\boldsymbol{r}(i)\boldsymbol{r}(i)^\intercal+\lambda I_N)\right]^{-1}\left[\sum_{i=0}^{n}\boldsymbol{r}(i)\boldsymbol{x}(i)^\intercal\right]\\
&=P(n)\psi(n)
\end{align}
となります\footnote{重み$\phi$で$C$を微分し、勾配が0となるときの方程式の解です。}。ただし、
\begin{align}
P(n)^{-1}&= \sum_{i=0}^{n}(\boldsymbol{r}(i)\boldsymbol{r}(i)^\intercal+\lambda I_N)\ \left(=\int_{0}^T \boldsymbol{r}(t) \boldsymbol{r}(t)^\intercal \mathrm{d} t+\lambda I_{N}\right)\\
\psi(n)&=\sum_{i=0}^{n}\boldsymbol{r}(i)\boldsymbol{x}(i)^\intercal
\end{align}
です。$P(n)$は$\boldsymbol{r}(n)$の相関行列の時間積分と係数倍した単位行列の和の逆行列となっています。また、
\begin{equation}
P(n)^{-1}=P(n-1)^{-1}+\boldsymbol{r}(n) \boldsymbol{r}(n)^\intercal
\end{equation}
となります。ここで、\textbf{逆行列の補助定理}(Matrix Inversion Lemma, またはSherman-Morrison-Woodbury Identity)より、
\begin{align}
X&=A+BCD\\
\Rightarrow X^{-1}&=A^{-1} - A^{-1}B(C^{-1}+DA^{-1}B)^{-1}DA^{-1}
\end{align}
となるので、$X={P}(n)^{-1}, A=P(n-1)^{-1}, B= \boldsymbol{r}(n), C=I_{N}, D=\boldsymbol{r}(n)^\intercal$とすると、
\begin{align}
P(n)&=P(n-1)-\frac{P(n-1) \boldsymbol{r}(n) \boldsymbol{r}(n)^\intercal P(n-1)}{1+\boldsymbol{r}(n)^\intercal P(n-1) \boldsymbol{r}(n)} 
\end{align}
が成り立ちます(右辺2項目の分母はスカラーとなります)。
さらに
\begin{align}
\psi(n)&=\psi(n-1)+\boldsymbol{r}(n)\boldsymbol{x}(n)^\intercal\\
&=P(n-1)^{-1}\phi(n-1)+\boldsymbol{r}(n)\boldsymbol{x}(n)^\intercal\\
&=\left\{P(n)^{-1}-\boldsymbol{r}(n) \boldsymbol{r}(n)^\intercal\right\}\phi(n-1)+\boldsymbol{r}(n)\boldsymbol{x}(n)^\intercal
\end{align}
となります。式(6.22)から式(6.23)へは
\begin{equation}
\phi(n)=P(n)\psi(n) \Rightarrow \psi(n)=P(n)^{-1}\phi(n)
\end{equation}
であること、式(6.23)から式(6.24)へは式(6.18)により、
\begin{equation}
P(n-1)^{-1}=P(n)^{-1}-\boldsymbol{r}(n) \boldsymbol{r}(n)^\intercal
\end{equation}
であることを用いています。よって、
\begin{align}
\phi(n)&=P(n)\psi(n)\notag\\
&=P(n)\left[\left\{P(n)^{-1}-\boldsymbol{r}(n) \boldsymbol{r}(n)^\intercal\right\}\phi(n-1)+\boldsymbol{r}(n)\boldsymbol{x}(n)^\intercal\right]\notag\\
&=\phi(n-1)-P(n)\boldsymbol{r}(n)\boldsymbol{r}(n)^\intercal\phi(n-1)+P(n)\boldsymbol{r}(n)\boldsymbol{x}(n)^\intercal\notag\\
&=\phi(n-1)-P(n)\boldsymbol{r}(n)\left[\boldsymbol{r}(n)^\intercal\phi(n-1)-\boldsymbol{x}(n)^\intercal\right]\notag\\
&=\phi(n-1)-P(n)\boldsymbol{r}(n)\boldsymbol{e}(n)^\intercal
\end{align}
となります。式(6.22)と式(6.27)を連続時間での表記法にすると、式(6. 9,10)の更新式となります。
