\subsection{FORCE法とRecurrent SNNへの適用}
Reservoir Computingにおける教師あり学習の手法の1つとして、\textbf{FORCE法}と呼ばれるものがあります。\textbf{FORCE} (First-Order Reduced and Controlled Error)法は(Sussillo \& Abbott, 2009)で提案された学習法で、元々は発火率ベースのRNNに対するオンラインの学習法です (具体的な方法については次節で解説します)。さらに(Nicola \& Clopath, 2017)はFORCE法がRecurrent SNNの学習に直接的に使用できる、ということを示しました。この章では(Nicola \& Clopath, 2017)の手法を用いてReservoir ComputingとしてのRecurrent SNNの教師あり学習を行います。
\subsubsection{Recurrent SNNに正弦波を学習させる}
今回はRecurrent SNNのニューロンの活動をデコードしたものが正弦波となるように(すなわち正弦波を教師信号として)訓練することを目標とします。先になりますが、結果は図のようになります。

\subsubsection{ネットワークの構造と教師信号}
ネットワークの構造は図のようになっています。ネットワークには特別な入力があるわけではなく、再帰的な入力によって活動が持続しています(膜電位の初期値をランダムにしているため開始時に発火するニューロン\footnote{ここでの「ニューロン」はこれ以後も含め、Reservoirのユニットを指します。}があり、またバイアス電流もあります)。\par
まず、Reservoirニューロンの数を$N$とし、出力の数を$N_\text{out}$とします。$i$番目のニューロンの入力はバイアス電流を$I_{\text{Bias}}$として、


\begin{equation}
I_i=s_i+I_{\text{Bias}}    
\end{equation}


と表されます。ただし、$s_i$は 


\begin{equation}
s_{i}=\sum_{j=1}^{N} \omega_{i j} r_{j}    
\end{equation}


として計算されます。$r_j$が$j$番目のニューロンの出力(シナプスフィルターをかけられたスパイク列), $\omega_{i j}$は$j$番目のニューロンから$i$番目のニューロンへの結合重みを意味します。\par
次にニューロンの活動$r_j$を重み$\phi\in \mathbb{R}^{N\times N_\text{out}}$で線形にデコードし、その出力$\hat{\boldsymbol{x}}(t)$を教師信号$\boldsymbol{x}(t)$に近づけます。すなわち、


\begin{equation}
\hat{\boldsymbol{x}}(t)=\sum_{j=1}^{N} \boldsymbol{\phi}_j r_{j}=\phi^\intercal\boldsymbol{r}
\end{equation}


とします。ただし、$^\intercal$を転置記号とし、$\boldsymbol{x}$を列ベクトル、$\boldsymbol{x}^\intercal$を行ベクトルとします。また、$\boldsymbol{\phi}_j\in \mathbb{R}^{N_\text{out}}$です。\par
ここから少しややこしいのですが、ネットワークの重み$\Omega=[\omega_{ij}]\in \mathbb{R}^{N\times N}$は 


\begin{equation}
\omega_{i j}=G \omega_{i j}^{0}+Q \boldsymbol{\eta}_{i}^\intercal \boldsymbol{\phi}_j 
\end{equation}


となっています。$\omega_{i j}^{0}$は固定された再帰重みです。$G, Q$は定数で、$\eta=[\boldsymbol{\eta}_{i}^\intercal]\in \mathbb{R}^{N\times N_\text{out}}$は$-1$か1に等確率に決められた行列です。よって学習するパラメータは$\phi$のみです。よってバイアスを抜いた入力電流$s_{i}$は次のように分割できます。


\begin{align}
s_{i}&=\sum_{j=1}^{N} \omega_{i j} r_{j}\\
&=\sum_{j=1}^{N} \left(G \omega_{i j}^{0}+Q \boldsymbol{\eta}_{i}^\intercal \boldsymbol{\phi}_j \right)r_{j}\\
&=Q\boldsymbol{\eta}_{i}^\intercal \hat{\boldsymbol{x}}(t)+\sum_{j=1}^{N} G \omega_{i j}^{0}r_{j}
\end{align}


\subsubsection{固定重みの初期化}
固定された結合重み$\omega_{i j}^{0}$は$\mathcal{N}(0, (Np)^{-1})$の正規分布からランダムサンプリングした値です($N$はニューロンの数、$p$は定数)。ただし、各ニューロンが投射される重みの平均が0になるようにスケーリングします。
\subsection{RLS法による重みの更新}
\footnote{ModelDBにおいて公開されているMATLABのコード(\url{https://senselab.med.yale.edu/ModelDB/ShowModel.cshtml?model=190565})を参考にしました。}
FORCE法は\textbf{RLSフィルタ}(recursive least squares filter, 再帰的最小二乗法フィルタ)という\textbf{適応フィルタ}(adaptive filter)の一種を学習するアルゴリズムを、RNNの学習に適応したものです\footnote{なお、(Sussillo \& Abbott, 2009)ではDelta則を用いることで、RLS法を用いない重みの更新則も紹介されています。}。
誤差を 


\begin{equation}
\boldsymbol{e}(t)=\hat{\boldsymbol{x}}(t)-\boldsymbol{x}(t)=\phi(t-\Delta t)^\intercal \boldsymbol{r}(t)-\boldsymbol{x}(t)    
\end{equation}


とした場合\footnote{実際にはこれは真の誤差ではなく、事前誤差(apriori error)と呼ばれるものです。真の誤差は$\phi(t)^\intercal \boldsymbol{r}(t)-\boldsymbol{x}(t)$と表されます。}、出力重み$\phi$を次の式で更新します。


\begin{align}
\phi(t)&=\phi(t-\Delta t)-P(t) \boldsymbol{r}(t)\boldsymbol{e}(t)^\intercal\\
P(t)&=P(t-\Delta t)-\frac{P(t-\Delta t) \boldsymbol{r}(t) \boldsymbol{r}(t)^\intercal P(t-\Delta t)}{1+\boldsymbol{r}(t)^\intercal P(t-\Delta
t) \boldsymbol{r}(t)} 
\end{align}


また、初期値は$\phi(0)=0,
P(0)=I_{N}\lambda^{-1}$です。$I_{N}$は$N$次の単位行列を意味します。$\lambda$は正則化のための定数です。

\subsubsection{FORCE法の実装}
それではFORCE法の実装をしてみましょう\footnote{コードは\texttt{./TrainingSNN/LIF\_FORCE\_sinewave.py}です。}。Reservoirネットワークは2000個のLIFニューロンで構成されているとします。また出力ユニットの個数は1です。まず、各種定数と教師信号を定義します。
