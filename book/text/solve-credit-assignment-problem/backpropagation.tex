\section{勾配法と誤差逆伝播法}
\textbf{誤差逆伝播法(back-propagation)}\index{ごさぎゃくでんぱほう(back-propagation)@誤差逆伝播法(back-propagation)}
\subsection{ニューラルネットワークモデル}
この節では入力層，隠れ層，出力層からなる3層ニューラルネットワークを実装する．
\begin{lstlisting}[language=julia]
using Base: @kwdef
using Parameters: @unpack # or using UnPack
using LinearAlgebra, Random, Statistics, PyPlot, ProgressMeter
\end{lstlisting}
\begin{lstlisting}[language=julia]
abstract type NeuralNet end

@kwdef struct MLP <: NeuralNet
    L::Int # num. of layers
    f::Vector{Function}; ∇f::Vector{Function};
    params::Dict{String, Dict} # weights and bias
    grads::Dict{String, Dict} # gradient of params
    states::Dict{String, Dict} # state of forward/backward activity
end;
\end{lstlisting}
\begin{lstlisting}[language=julia]
function MLP(num_units; activation="sigmoid")#, bias=true)
    L = length(num_units) - 1 # num of layers
    # initialization of parameters
    params, grads = Dict(), Dict()
    params["W"] = Dict{Int, Array}(l => 2 * (rand(num_units[l], num_units[l+1]) .- 0.5) / sqrt(num_units[l]) for l in 1:L)
    params["b"] = Dict{Int, Array}(l => zeros(1, num_units[l+1]) for l in 1:L)
    for key in keys(params)
        grads["∇$key"] = Dict{Int, Array}(l => zero(params[key][l]) for l in 1:L)
    end
    states = Dict(key => Dict{Int, Array}() for key in ["a", "z", "δ"])

    # set activation functions
    if activation isa Vector{String}
        @assert length(activation) == L "length of activation muskey = t be equal to L=$L, or use string"
        f = [eval(Symbol(activation[l])) for l in 1:L]
        ∇f = [eval(Symbol("∇$(activation[l])")) for l in 1:L]
    elseif activation isa String
        f = [eval(Symbol(activation)) for l in 1:L]
        ∇f = [eval(Symbol("∇$(activation)")) for l in 1:L]
    end
    return MLP(L=L, f=f, ∇f=∇f, params=params, grads=grads, states=states)
end;
\end{lstlisting}
mutable struct \jl{MLP}を用意し，\textbf{重みの初期化(weight initialization)}\index{おもみのしょきか(weight initialization)@重みの初期化(weight initialization)} を行う同名の関数\jl{MLP}を用意する．重みの初期化の手法は複数ある(Glorot & Bengio, 2010)が，ここでは重みを$W$として，$w \sim U\left(-1/\sqrt{n}, 1/\sqrt{n}\right)$とする．ただし，$n$は入力ユニット数である．
\subsubsection{順伝播 (forward propagation)}
$f(\cdot)$を活性化関数とする．順伝播(feedforward propagation)は以下のようになる．
\begin{align}
\text{入力層 : }&\mathbf{z}^{(0)}=\mathbf{x}\\
\text{隠れ層 : }&\mathbf{z}^{(\ell)}=f\left(\mathbf{a}^{(\ell)}\right)\\
&\mathbf{a}^{(\ell+1)}=W^{(\ell+1)}\mathbf{z}^{(\ell)}+\mathbf{b}^{(\ell+1)}\\
\text{出力層 : }&\hat{\mathbf{y}}=\mathbf{z}^{(L)}
\end{align}
\begin{lstlisting}[language=julia]
sigmoid(x) = 1 ./ (1 .+ exp.(-x));
relu(x) = max.(x, 0);

∇sigmoid(z) = z .* (1 .- z)
∇relu(z) = z .> 0
∇tanh(z) = 1 .- z.^2
\end{lstlisting}
\begin{lstlisting}[language=julia]
function softmax(x; dims=1)
    expx = exp.(x .- maximum(x))
    return expx ./ sum(expx, dims=dims)
end
\end{lstlisting}
\begin{lstlisting}[language=julia]
#∇softmax
\end{lstlisting}
最後に活性化関数を付けたくない場合は恒等関数 \jl{identity} を用いる．
\begin{lstlisting}[language=julia]
function forward!(mlp::MLP, x::Array)
    @unpack L, params, states, f = mlp
    @unpack W, b = params # parameters
    @unpack a, z = states # state of forward/backward activity

    z[0] = x
    for l in 1:L
        a[l] = z[l-1] * W[l] .+ b[l]
        z[l] = f[l](a[l])
    end
    return z[L]
end
\end{lstlisting}
\subsubsection{逆伝播 (backward propagation)}
\begin{align}
\text{目的関数 : }&\mathcal{L}=\frac{1}{2}\left\|\hat{\mathbf{y}}-\mathbf{y}\right\|^{2}\\
\text{最急降下法 : }&\Delta W^{(\ell)}=-\eta \frac{\partial \mathcal{L}}{\partial W^{(\ell)}}\\
&\Delta \mathbf{b}^{(\ell)}=-\eta \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(\ell)}}\\
\text{誤差逆伝播法 : }&\frac{\partial \mathcal{L}}{\partial \hat{\mathbf{y}}}=\frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(L)}}=\hat{\mathbf{y}}-\mathbf{y}\\
&\delta^{(L)}=\frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(L)}} \frac{\partial \mathbf{z}^{(L)}}{\partial \mathbf{a}^{(L)}}=\left(\hat{\mathbf{y}}-\mathbf{y}\right) \odot f^{\prime}\left(\mathbf{a}^{(L)}\right)\\
&\mathbf{\delta}^{(\ell)}=\frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(\ell)}} \frac{\partial \mathbf{z}^{(\ell)}}{\partial \mathbf{a}^{(\ell)}}=\left(W^{(\ell+1)}\right)^\top \delta^{(\ell+1)} \odot f^{\prime}\left(\mathbf{a}^{(\ell)}\right)\\
&\frac{\partial \mathcal{L}}{\partial W^{(\ell)}}=\frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(\ell)}} \frac{\partial \mathbf{z}^{(\ell)}}{\partial \mathbf{a}^{(\ell)}} \frac{\partial \mathbf{a}^{(\ell)}}{\partial W^{(\ell)}}=\delta^{(\ell)}\left(\mathbf{z}^{(\ell-1)}\right)^\top\\
&\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(\ell)}}=\frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(\ell)}} \frac{\partial \mathbf{z}^{(\ell)}}{\partial \mathbf{a}^{(\ell)}} \frac{\partial \mathbf{a}^{(\ell)}}{\partial \mathbf{b}^{(\ell)}}=\delta^{(\ell)}
\end{align}
バッチ処理を考慮すると，行列を乗ずる順番が変わる．
以下では$z=f(a), g(z)=f'(a)$として膜電位を使わず，発火率情報のみを使うようにしている．このようにできない関数もあるが，今回はこのように書き下せる活性化関数のみを扱う．
\frac{d}{dx} \text{Sigmoid}(x) = \text{Sigmoid}(x) \cdot \left(1 - \text{Sigmoid}(x)\right)
であることに注意．
\begin{lstlisting}[language=julia]
function backward!(mlp::MLP; losstype::String="binary_crossentropy")
    @unpack L, params, states, grads, ∇f = mlp
    @unpack W, b = params    # parameters
    @unpack ∇W, ∇b = grads # gradient of params
    @unpack a, z, δ = states # state of forward/backward activity

    n_batch = size(z[0])[1]
    # backprop
    for l in L:-1:1
        if l > 1
            δ[l-1] = δ[l] * W[l]' .* ∇f[l].(z[l-1])
        end
        ∇W[l] = z[l-1]' * δ[l] / n_batch
        ∇b[l] = sum(δ[l], dims=1) / n_batch
    end
end
\end{lstlisting}
\begin{lstlisting}[language=julia]
clog(x) = max(log(x), -1e2) # clamped log

function binary_crossentropy!(nn::NeuralNet, ŷ::Array, y::Array)
    @unpack L, states = nn
    @unpack δ = states
    error = ŷ - y
    loss = sum(-y .* clog.(ŷ) + (1 .- y) .* clog.(1 .- ŷ))
    δ[L] = error
    return loss
end

function squared_error!(nn::NeuralNet, ŷ::Array, y::Array)
    @unpack L, states, ∇f = mlp
    @unpack δ = states
    error = ŷ - y
    loss = sum(error .^ 2)
    δ[L] = error .* ∇f[L].(ŷ)
    return loss
end
\end{lstlisting}
\subsubsection{Optimizerの作成}
abstract typeとして\jl{Optimizer}タイプを作成する．
\begin{lstlisting}[language=julia]
abstract type Optimizer end
\end{lstlisting}
\textbf{確率的勾配降下法(stochastic gradient descent; SGD)}\index{かくりつてきこうばいこうかほう(stochastic gradient descent; SGD)@確率的勾配降下法(stochastic gradient descent; SGD)} を実装する．
\begin{lstlisting}[language=julia]
# SGD optimizer
@kwdef struct SGD{FT} <: Optimizer
    η::FT = 1e-2
end

function optimizer_update!(param, grad, optimizer::SGD)
    @unpack η = optimizer
    param[:, :] -= η * grad
end
\end{lstlisting}
次に\textbf{Adam}\index{Adam} ([Kingma & Ba, 2014](https://arxiv.org/abs/1412.6980)) を実装する．
\begin{lstlisting}[language=julia]
# Adam optimizer
@kwdef mutable struct Adam{FT} <: Optimizer
    α::FT  = 1e-4; β1::FT = 0.9; β2::FT = 0.999; ϵ::FT = 1e-8
    ms = Dict(); vs = Dict();
end

# Adam optimizer
function optimizer_update!(param, grad, optimizer::Adam)
    @unpack α, β1, β2, ϵ, ms, vs = optimizer
    key = objectid(param)
    if !haskey(ms, key) 
        ms[key], vs[key] = zeros(size(param)), zeros(size(param))
    end    
    m, v = ms[key], vs[key]
    m += (1 - β1) * (grad - m)
    v += (1 - β2) * (grad .* grad - v)
    param[:, :] -= α * m ./ (sqrt.(v) .+ ϵ)
end
\end{lstlisting}
\begin{lstlisting}[language=julia]
function optim_step!(nn::NeuralNet, optimizer::Optimizer)
    @unpack L, params, grads = nn
    for key in keys(params)
        for l in 1:L
            optimizer_update!(params[key][l], grads["∇$key"][l], optimizer)
        end
    end
end
\end{lstlisting}
\begin{lstlisting}[language=julia]
function train_step!(nn::NeuralNet, x::Array, y::Array, loss_fun::Function, optimizer::Optimizer=SGD())
    ŷ = forward!(nn, x)
    loss = loss_fun(nn, ŷ, y)
    backward!(nn)
    optim_step!(nn, optimizer) # update params
    return loss
end
\end{lstlisting}
## MNIST
\subsection{Zipser-Andersenモデル}
Zipser-Andersenモデル \citep{Zipser1988-nc} は頭頂葉の7a野のモデルであり，網膜座標系における物体の位置と眼球位置を入力として，頭部中心座標(head centered coordinate)に変換する．隠れ層はPPC(Posterior parietal cortex)の細胞のモデルになっている．
\subsubsection{データセットの生成}
物体位置の表現にはGaussian形式とmonotonic形式があるが，簡単のために，Gaussian形式を用いる．なお，monotonic形式については末尾の補足を参照してほしい．
\begin{lstlisting}[language=julia]
# Gaussian 2d
function Gaussian2d(pos, sizex=8, sizey=8, σ=1)
    x, y = 0:sizex-1, 0:sizey-1
    X, Y = [i for i in x, j in 1:length(y)], [j for i in 1:length(x), j in y]
    x0, y0 = pos
    return exp.(-((X .- x0) .^2 + (Y .- y0) .^2) / 2σ^2)
end
\end{lstlisting}
入力は64(網膜座標系での位置)+2(眼球位置信号)=66とする．眼球位置信号は原著ではmonotonic形式による32(=8ユニット×2(x, y方向)×2 (傾き正負))ユニットで構成されるが，簡単のために眼球位置信号も$x, y$の2次元とする．視覚刺激は-40度から40度までの範囲であり，10度で離散化する．よって，網膜座標系での位置は$8\times 8$の行列で表現される．位置は2次元のGaussianで表現する．ただし，1/e幅 (ピークから1/eに減弱する幅) は15度である．$1/e$の代わりに$1/2$とすれば半値全幅(FWHM)となる．スポットサイズを$W$，Gaussianを$G(x)$とすると．$G(x+w/2)=G/e$より，$\sigma=\frac{\sqrt{2}w}{4}$と求まる．
\begin{lstlisting}[language=julia]
# dataset θeter
θmax = 40.0 # degree, θ∈[-θmax, θmax]
Δθ = 10.0 # degree
stimuli_size = Int(2θmax / Δθ)
w = 15.0 # degree; 1/e width
σ = √2w/(4Δθ);

# training θeter
n_data = 10000
n_traindata = Int(n_data*0.95)
n_batch = 100 # batch size
n_iter_per_epoch = Int(n_traindata/n_batch)
n_epoch = 2000; # number of epoch
\end{lstlisting}
\begin{lstlisting}[language=julia]
# generate positions
Random.seed!(0)
retinal_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]
head_centered_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]
#retinal_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]
#head_centered_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]
eye_pos = head_centered_pos - retinal_pos; # ∈ [-80, 80]

# convert
input_retina = [hcat(Gaussian2d((retinal_pos[i, :] .+ θmax)/Δθ, stimuli_size, stimuli_size, σ)...) for i in 1:n_data];
input_retina = vcat(input_retina...)
eye_pos /= 2θmax;

# concat
x_data = hcat(input_retina, eye_pos) #_encoded)
y_data = vcat([hcat(Gaussian2d((head_centered_pos[i, :] .+ θmax)/Δθ, stimuli_size, stimuli_size, σ)...) for i in 1:n_data]...);

# split
x_traindata, y_traindata = x_data[1:n_traindata, :], y_data[1:n_traindata, :]
x_testdata, y_testdata = x_data[n_traindata+1:end, :], y_data[n_traindata+1:end, :];
\end{lstlisting}
\begin{lstlisting}[language=julia]
product(sets...) = hcat([collect(x) for x in Iterators.product(sets...)]...)' # Array of Cartesian product of sets 
\end{lstlisting}
モデルの定義を行う．
\begin{lstlisting}[language=julia]
# model θeter
n_in = stimuli_size^2 + 2 # number of inputs
n_hid = 16   # number of hidden units
n_out = stimuli_size^2   # number of outputs
η = 1e-2  # learning rate
#losstype = "binary_crossentropy" # "squared_error"
\end{lstlisting}
\begin{lstlisting}[language=julia]
nn = MLP([n_in, n_hid, n_out])#, bias=false)
optimizer = SGD(η=η);
loss_fun = binary_crossentropy!
#optimizer = Adam();
\end{lstlisting}
\begin{lstlisting}[language=julia]
n_in, n_hid, n_out
\end{lstlisting}
学習を行う．
\begin{lstlisting}[language=julia]
error_arr = zeros(n_epoch); # memory array of each epoch error

@showprogress "Training..." for e in 1:n_epoch
    for iter in 1:n_iter_per_epoch
        idx = (iter-1)*n_batch+1:iter*n_batch
        x, y = x_traindata[idx, :], y_traindata[idx, :]
        loss = train_step!(nn, x, y, loss_fun, optimizer)
        error_arr[e] += loss
    end 
    error_arr[e] /= n_traindata
end
\end{lstlisting}
損失の変化を描画する．
\begin{lstlisting}[language=julia]
figure(figsize=(4,2))
#semilogy(error_arr)
plot(error_arr)
ylabel("Error"); xlabel("Epoch"); xlim(0, n_epoch)
tight_layout()
\end{lstlisting}
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.8, max width=\linewidth]{./fig/solve-credit-assignment-problem/backpropagation/cell039.png}
	\caption{cell039.png}
	\label{cell039.png}
\end{figure}
テストデータを用いて，出力を確認する．
\begin{lstlisting}[language=julia]
x, y = x_testdata[1:2, :], y_testdata[1:2, :]
ŷ = forward!(nn, x)

id = 1
figure(figsize=(6,2))
ax1 = subplot(1,3,1); title("input")
ax1.imshow(reshape(x[id, 1:64], (stimuli_size, stimuli_size))', interpolation="gaussian", extent=[-θmax, θmax, θmax, -θmax])
ax1.add_patch(plt.Circle((x[id, 65:66])*2θmax, radius=2, color="tab:red", fill=false))
xlabel("x"); ylabel("y");

ax2 = subplot(1,3,2); title("output")
ax2.imshow(reshape(ŷ[id, :], (stimuli_size, stimuli_size))', interpolation="gaussian", extent=[-θmax, θmax, θmax, -θmax])
ax2.add_patch(plt.Circle((x[id, 65:66])*2θmax, radius=2, color="tab:red", fill=false))
xlabel("x");

ax3 = subplot(1,3,3); title("target")
ax3.imshow(reshape(y[id, :], (stimuli_size, stimuli_size))', interpolation="gaussian", extent=[-θmax, θmax, θmax, -θmax])
ax3.add_patch(plt.Circle((x[id, 65:66])*2θmax, radius=2, color="tab:red", fill=false))
xlabel("x");

tight_layout()
\end{lstlisting}
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.8, max width=\linewidth]{./fig/solve-credit-assignment-problem/backpropagation/cell041.png}
	\caption{cell041.png}
	\label{cell041.png}
\end{figure}
重み\jl{W1}におけるゲインフィールドの描画を行う．
\begin{lstlisting}[language=julia]
# Plot Gain fields
figure(figsize=(3.2, 3))
suptitle("Gain fields", fontsize=12)
subplots_adjust(hspace=0.1, wspace=0.1, top=0.925)
for i in 1:n_hid
    #subplot(3, 3, i)
    subplot(4, 4, i)
    imshow(reshape(nn.params["W"][1][1:stimuli_size^2, i], (stimuli_size, stimuli_size)), cmap="hot")
    axis("off")
end
\end{lstlisting}
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.8, max width=\linewidth]{./fig/solve-credit-assignment-problem/backpropagation/cell043.png}
	\caption{cell043.png}
	\label{cell043.png}
\end{figure}
\subsection{補足：Monotonic formatによる位置のエンコーディング}
monotonic形式を入力の眼球位置と出力の頭部中心座標で用いるという仮定には，視覚刺激を中心窩で捉えた際，得られる眼球位置信号を頭部中心座標での位置の教師信号として使用できるという利点がある．([Andersen & Mountcastle, J. Neurosci. 1983](https://pubmed.ncbi.nlm.nih.gov/6827308/))では Parietal visual neurons (PVNs)の活動を調べ，傾き正あるいは負．0度をピークとして減少あるいは上昇の4種類あることを示した．前者は一次関数 (とReLU関数) で記述可能である．
\begin{lstlisting}[language=julia]
get_line(p1, p2) = [(p2[2]-p1[2])/(p2[1]-p1[1]), (p2[1]*p1[2] - p1[1]*p2[2])/(p2[1]-p1[1])] # [slope, intercept]
eye_pos_coding(x; linear_θ) = relu.(linear_θ[1, :] * x .+ linear_θ[2, :])

x = -2θmax:1:2θmax
slope_θ = hcat([get_line([80, 1], [-80, -2(i-1)/stimuli_size]) for i in 1:stimuli_size]...)
y = hcat(eye_pos_coding.(x; linear_θ=slope_θ)...)
eye_pos_encoded = eye_pos_coding(-10; linear_θ=slope_θ)
\end{lstlisting}
\begin{lstlisting}[language=julia]
figure(figsize=(5,3))
subplot(2,1,1); plot(x, y'); xlabel("Eye position"); ylabel("Firing rate")
subplot(2,1,2); imshow(eye_pos_encoded[:, :]'); title(L"Eye position $=-10^\circ$"); xlabel("Units") 
tight_layout()
\end{lstlisting}
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.8, max width=\linewidth]{./fig/solve-credit-assignment-problem/backpropagation/cell046.png}
	\caption{cell046.png}
	\label{cell046.png}
\end{figure}
