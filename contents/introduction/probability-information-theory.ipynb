{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f981d1",
   "metadata": {},
   "source": [
    "# 確率論\n",
    "## 期待値 (Expectation)\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{x\\sim p(x)}\\left[f(x)\\right]:=\\int f(x)p(x)dx\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$x\\sim p(x)$ が明示的な場合は $\\mathbb{E}_{p(x)}\\left[f(x)\\right]$ や $\\mathbb{E}\\left[f(x)\\right]$ と表す．\n",
    "\n",
    "## 情報量 (Information)\n",
    "出現頻度が低い事象は多くの情報量を持つ (Shannon, 1948)．\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{I}(x):=\\ln\\left(\\frac{1}{p(x)}\\right)=-\\ln p(x)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\mathbf{I}$は単位行列なので注意．\n",
    "\n",
    "## 平均情報量 (エントロピー, entropy)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{H}(x)&:=\\mathbb{E}[-\\ln p(x)]\\\\\n",
    "\\mathbb{H}(x\\vert y)&:=\\mathbb{E}[-\\ln p(x\\vert y)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Kullback-Leibler 情報量\n",
    "Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}}\\left(p(x) \\Vert\\ q(x)\\right)&:=\\int p(x) \\ln \\frac{p(x)}{q(x)} dx\\\\\n",
    "&=\\int p(x) \\ln p(x) dx-\\int p(x) \\ln q(x) dx\\\\\n",
    "&=\\mathbb{E}_{x\\sim p(x)}[\\ln p(x)]-\\mathbb{E}_{x\\sim p(x)}[\\ln q(x)]\\\\\n",
    "&=-\\mathbb{H}(x)-\\mathbb{E}_{x\\sim p(x)}[\\ln q(x)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## 相互情報量 (Mutual information)\n",
    "aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05df9b-0147-456e-b71e-29b9652a708b",
   "metadata": {},
   "source": [
    "「確率論と情報理論」の入門を書く際に触れておくべき重要な項目を以下に列挙します。それぞれの項目について簡単な説明も付けています。\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 1. **確率論の基礎**\r\n",
    "   - **確率の定義と性質**: 古典的確率、頻度主義、ベイズ確率。\r\n",
    "   - **事象と確率空間**: サンプル空間、事象、加法定理、乗法定理。\r\n",
    "   - **条件付き確率と独立性**: 条件付き確率、ベイズの定理、確率変数の独立性。\r\n",
    "\r\n",
    "### 2. **確率変数と分布**\r\n",
    "   - **確率変数**: 離散型と連続型。\r\n",
    "   - **確率分布**: 二項分布、正規分布、指数分布など。\r\n",
    "   - **期待値と分散**: 線形性、共分散、標準偏差。\r\n",
    "   - **大数の法則と中心極限定理**: 確率分布の収束の概念。\r\n",
    "\r\n",
    "### 3. **エントロピーと情報量**\r\n",
    "   - **情報の定義**: 情報量（シャノン情報量）の直感的説明。\r\n",
    "   - **エントロピー**: エントロピーの定義、直感的な解釈、性質。\r\n",
    "   - **相互情報量**: 共通情報の測定方法。\r\n",
    "\r\n",
    "### 4. **符号化とデータ圧縮**\r\n",
    "   - **エントロピー符号化**: ハフマン符号、算術符号。\r\n",
    "   - **シャノンの符号化定理**: 最小限の平均符号長。\r\n",
    "   - **データ圧縮の限界**: 可逆圧縮と非可逆圧縮。\r\n",
    "\r\n",
    "### 5. **通信路モデルと通信路容量**\r\n",
    "   - **通信路の定義**: 入力、出力、ノイズのモデル化。\r\n",
    "   - **通信路容量**: 定義、シャノンの通信路容量定理。\r\n",
    "   - **例としての通信路モデル**: バイナリ対称通信路（BSC）、ガウス通信路。\r\n",
    "\r\n",
    "### 6. **情報理論と確率論の関連**\r\n",
    "   - **エントロピーと確率分布**: エントロピーの最大化問題。\r\n",
    "   - **KLダイバージェンス**: 確率分布間の距離。\r\n",
    "   - **エントロピーと統計力学の関係**: 熱力学とのアナロジー。\r\n",
    "\r\n",
    "### 7. **ベイズ確率と情報理論**\r\n",
    "   - **事後分布のエントロピー**: 不確実性の測定。\r\n",
    "   - **ベイズ更新と情報量**: 新しいデータからの学習。\r\n",
    "\r\n",
    "### 8. **応用例**\r\n",
    "   - **データ圧縮**: ZIP形式、音声圧縮（MP3、AAC）。\r\n",
    "   - **誤り訂正符号**: ハミング符号、LDPC符号。\r\n",
    "   - **機械学習**: クロスエントロピー損失関数。\r\n",
    "   - **暗号理論**: エントロピーの応用。\r\n",
    "\r\n",
    "### 9. **数学的背景と拡張**\r\n",
    "   - **確率測度の定式化**: 確率論の厳密な定義。\r\n",
    "   - **情報理論の一般化**: レニイエントロピー、ツァリスエントロピー。\r\n",
    "   - **多次元分布と共分散行列**: 情報理論的な解析。\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "これらの項目を基に、具体例や図解を交えながら説明を進めることで、初心者にも理解しやすい入門書が作れるでしょう。構成についてさらに深掘りしたい場合や各項目の詳細な展開が必要な場合はお知らせください！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
