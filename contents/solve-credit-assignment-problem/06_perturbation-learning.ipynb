{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d544aa-ec6e-435d-b290-367be9bb2826",
   "metadata": {},
   "source": [
    "Node perturbation, weight perturbationは統一的に捉えることができる．node perturbationはbias onlyのweight perturbationと解釈できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd56e09-e099-471c-80d2-9c1d5ad4f31c",
   "metadata": {},
   "source": [
    "https://www.science.org/doi/10.1126/sciadv.abh0146\n",
    "\n",
    "Chaotic neural dynamics facilitate probabilistic computations through sampling\n",
    "\n",
    "Effective Learning with Node Perturbation in Multi-Layer Neural Networks (fig1は図の参考になる．)\n",
    "On the stability and scalability of node perturbation learning\n",
    "Node perturbation learning without noiseless baseline\n",
    "\n",
    "どれも効率は良くない．\n",
    "\n",
    "### Node perturbation (NP)\n",
    "ノード摂動法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44e54b-c2e1-4b65-adec-93205e2b0a8f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{a}_\\ell=\\mathbf{W}_\\ell \\mathbf{z}_\\ell +\\mathbf{b}_\\ell\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_{\\ell+1}=f_\\ell\\left(\\mathbf{a}_\\ell\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{z}}_{\\ell+1}=f_\\ell\\left(\\mathbf{W}_\\ell \\tilde{\\mathbf{z}}_\\ell +\\mathbf{b}_\\ell+\\sigma \\boldsymbol{\\xi}_\\ell \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta \\mathcal{L}=\\mathcal{L}(\\tilde{\\mathbf{z}}_{L+1}; \\mathbf{x})-\\mathcal{L}(\\mathbf{z}_{L+1}; \\mathbf{x})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{W}_\\ell^{\\mathrm{NP}} =- \\eta  \\frac{\\delta \\mathcal{L}}{\\sigma} \\boldsymbol{\\xi}_\\ell \\mathbf{z}_{\\ell}^\\top,\\quad\n",
    "\\Delta \\mathbf{b}_\\ell^{\\mathrm{NP}} =- \\eta  \\frac{\\delta \\mathcal{L}}{\\sigma} \\boldsymbol{\\xi}_\\ell\n",
    "$$\n",
    "\n",
    "簡単に示す．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf6a6a-2e2f-4f1c-b7d5-4e9a9efa6757",
   "metadata": {},
   "source": [
    "$\\tilde{\\mathbf{z}}_{L+1}$ を $\\sigma\\to 0$ においてTaylor展開し，1次近似すると，\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{z}}_{L+1}\\approx \\mathbf{z}_{L+1} + \\sigma \\sum_{k=1}^L \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_{k}}\\boldsymbol{\\xi}_{k}\n",
    "$$\n",
    "\n",
    "となる．また，$\\mathcal{L}(\\tilde{\\mathbf{z}}_{L+1}; \\mathbf{x})$ をTaylor展開して，1次近似すると\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\tilde{\\mathbf{z}}_{L+1}; \\mathbf{x})\\approx \\mathcal{L}(\\mathbf{z}_{L+1}; \\mathbf{x}) + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}} \\left(\\tilde{\\mathbf{z}}_{L+1} - \\mathbf{z}_{L+1}\\right)\n",
    "$$\n",
    "\n",
    "となる．よって，\n",
    "\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\sigma}\\approx \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}\\left(\\sum_{k=1}^L \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_{k}}\\boldsymbol{\\xi}_{k}\\right)\n",
    "$$ \n",
    "であるので\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{W}_\\ell^{\\mathrm{NP}} &\\approx -\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}\\left(\\sum_{k=1}^L \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_{k}}\\boldsymbol{\\xi}_{k}\\right)\\boldsymbol{\\xi}_\\ell \\mathbf{z}_{\\ell}^\\top=-\\eta \\sum_{k=1}^L \\boldsymbol{\\xi}_\\ell\\boldsymbol{\\xi}_{k}^\\top \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{k}} \\mathbf{z}_{\\ell}^\\top\\\\\n",
    "\\Delta \\mathbf{b}_\\ell^{\\mathrm{NP}} &\\approx -\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}\\left(\\sum_{k=1}^L \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_{k}}\\boldsymbol{\\xi}_{k}\\right)\\boldsymbol{\\xi}_\\ell=-\\eta \\sum_{k=1}^L \\boldsymbol{\\xi}_\\ell\\boldsymbol{\\xi}_{k}^\\top \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{k}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\mathbb{E}[\\boldsymbol{\\xi}_\\ell\\boldsymbol{\\xi}_{\\ell}^\\top]=\\mathbf{I}, \\mathbb{E}[\\boldsymbol{\\xi}_\\ell\\boldsymbol{\\xi}_{k}^\\top]=\\mathbf{0}$ としたため，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\Delta \\mathbf{W}_\\ell^{\\mathrm{NP}}] &\\approx -\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell}}\\mathbf{z}_{\\ell}^\\top=-\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell}} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial \\mathbf{W}_\\ell}=-\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_\\ell}\\\\\n",
    "\\mathbb{E}[\\Delta \\mathbf{b}_\\ell^{\\mathrm{NP}}] &\\approx -\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell}}=-\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell}} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial \\mathbf{b}_\\ell}=-\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_\\ell}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "が成り立つ．不偏推定量(unbiased estimator)ではあるが，分散は大きい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662c69e-0dbd-4e9d-91ca-36fa3507c727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc719d-b273-45a9-91a4-62de4dae4928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19aea6fd-3ed5-4c85-b965-37406814a471",
   "metadata": {},
   "source": [
    "### Weight perturbation (WP)\n",
    "重み摂動法 (Weight perturbation; WP)\n",
    "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.13.021006\n",
    "Weight Perturbation Learning Performs Similarly or Better than Node Perturbation on Broad Classes of Temporally Extended Tasks\n",
    "\n",
    "A. Dembo and T. Kailath, Model-Free Distributed Learning, IEEE Trans. Neural Networks 1, 58 (1990).\n",
    "\n",
    "G. Cauwenberghs, A Fast Stochastic Error-Descent Algorithm for Supervised Learning and Optimization, in Advances in Neural Information Processing Systems (Morgan Kaufmann, Burlington, 1993), Vol. 5, pp. 244–251"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbab4e3-2df7-4753-bdd1-4bd7a1aa15e1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{z}_{\\ell+1}=f_\\ell\\left(\\mathbf{W}_\\ell \\mathbf{z}_\\ell +\\mathbf{b}_\\ell\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{z}}_{\\ell+1}=f_\\ell\\left((\\mathbf{W}_\\ell+\\Xi_\\ell) \\tilde{\\mathbf{z}}_\\ell +\\mathbf{b}_\\ell \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta \\mathcal{L}=\\mathcal{L}(\\tilde{\\mathbf{z}}_{L+1}; \\mathbf{x})-\\mathcal{L}(\\mathbf{z}_{L+1}; \\mathbf{x})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{W}_\\ell^{\\mathrm{WP}} = -\\delta \\mathcal{L} \\frac{\\Xi_\\ell}{\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621997fc-a8e8-431b-b652-92a7253923d7",
   "metadata": {},
   "source": [
    "関数$f$について点$\\mathbf{u}$における方向$\\mathbf{v}$の方向微分 (Directional derivative) は\n",
    "\n",
    "$$\n",
    "\\nabla_\\mathbf{v}f(\\mathbf{u})= \\lim_{h\\to 0} \\frac{f(\\mathbf{u}+h\\mathbf{v}) - f(\\mathbf{u})}{h}\n",
    "$$\n",
    "\n",
    "として定義される．また$f$が点$\\mathbf{u}$において微分可能なら\n",
    "\n",
    "$$\n",
    "\\nabla_\\mathbf{v}f(\\mathbf{u})=\\nabla f(\\mathbf{u})\\cdot \\mathbf{v}=\\frac{\\partial f(\\mathbf{u})}{\\partial \\mathbf{u}}\\cdot \\mathbf{v}\n",
    "$$\n",
    "\n",
    "が成り立つ．ここで右辺を**Jacobian-vector product** (JVP) と呼ぶ．有限差分(finite difference)を用いてJacobian-vector productを近似計算すると ($\\epsilon$は小さい値)．\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{u})\\cdot \\mathbf{v} \\approx \\frac{f(\\mathbf{u}+\\epsilon \\mathbf{v}) - f(\\mathbf{u})}{\\epsilon}\n",
    "$$\n",
    "\n",
    "なお，$f(\\mathbf{u})\\in \\mathbb{R}$の場合，$\\nabla f(\\mathbf{u})\\cdot \\mathbf{v}\\in \\mathbb{R}$となる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1014530-9e45-4399-991a-244ef5d38654",
   "metadata": {},
   "source": [
    "よって\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\theta}}\\cdot \\boldsymbol{\\xi} \\approx \\frac{\\mathcal{L}(\\boldsymbol{\\theta}+\\sigma \\boldsymbol{\\xi}) - \\mathcal{L}(\\boldsymbol{\\theta})}{\\sigma}\n",
    "$$\n",
    "\n",
    "と近似できるので，\n",
    "\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\sigma}\\boldsymbol{\\xi} \\approx (\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\theta}}\\cdot \\boldsymbol{\\xi})\\boldsymbol{\\xi}\n",
    "$$\n",
    "\n",
    "となり，\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\Delta \\mathbf{W}_\\ell^{\\mathrm{WP}}] = \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\theta}} \\mathbb{E}[\\boldsymbol{\\xi} \\boldsymbol{\\xi}^\\top]=\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\theta}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7742d8-08f1-40d3-b543-827b25348885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88d5a4-a320-47bb-99c4-fffac2eebf55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54348e0e-1042-47e2-9310-0cd3e3015baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "209b2eb6-cc29-47a7-ac16-4ddbcf796aa1",
   "metadata": {},
   "source": [
    "この手法に順方向自動微分 (Forward Mode AD) を適応したのが，Forward Gradient法である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494c729-db0c-483b-b765-65066e65f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finite difference\n",
    "eps = 1e-3\n",
    "\n",
    "def grad_estimation(f, u, n, mode=\"dodge\"):\n",
    "    if mode == \"dodge\":\n",
    "        v = 2*(torch.rand(n, 2) > 0.5) - 1   \n",
    "    elif mode == \"fgd\":\n",
    "        v = torch.randn(n, 2)\n",
    "    else:\n",
    "        assert False, \"mode is dodge or fgd\"\n",
    "    estimate_grad = 0\n",
    "    for i in range(n):\n",
    "        f_v, f = func(u + eps*v), func(u)\n",
    "        jvp = (f_v - f) / eps\n",
    "        estimate_grad += jvp*v[i]\n",
    "    estimate_grad /= n\n",
    "    return estimate_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993745c-918d-4437-8bba-769a32733cb5",
   "metadata": {},
   "source": [
    "### 方向微分による訓練\r\n",
    "同じ構造のモデルを2つ (`model`, `model_v`) 用意し，以下のような手順でパラメータを更新します．\r\n",
    "1. `model`で順伝播を行い，`loss`を計算する．\r\n",
    "2. 勾配の推定値を保存するDict`grad_estimate`を用意する．\r\n",
    "3. 摂動`v`を生成する．この際，`model`のパラメータのkeyを辞書形式 `model.state_dict()`で取得し同じkeyで登録．同時に`model`と同じ構造の`model_v`のパラメータを`model`のパラメータに摂動`v`を加えたもので置換する．\r\n",
    "4. `model_v`で順伝播を行い，`loss_v`を計算する．\r\n",
    "5. 方向微分を`loss_v`と`loss`を用いて有限差分で計算する．\r\n",
    "6. `grad_estimate`に勾配の推定値を加算する．\r\n",
    "7. `num_direction`の数だけ3-6を繰り返す．\r\n",
    "8. `grad_estimate`の値を`num_direction`で平均化し`torch.clamp()`でgradient clippingする (数値的に不安定なため)．\r\n",
    "9. `optimizer.zero_grad()`で`model`のパラメータの勾配をzeroにする．\r\n",
    "10. `param.grad`に推定した勾配値を代入する．\r\n",
    "11. `optimizer.step()`でパラメータを更新する．\r\n",
    "\r\n",
    "前節のシミュレーションでは`num_directions`を増やさないと推定された勾配が真の勾配に近づきませんでしたが，`num_directions=1`でも学習は進行します．もちろん増やしてもいいですが，計算量が増えます．また，学習率$lr$は0.001とbackpropの0.01よりも小さいものを用いていますが，これは`lr`を0.01にすると発散したためです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05ab89-7b97-4c40-b570-5b544d4da56a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
