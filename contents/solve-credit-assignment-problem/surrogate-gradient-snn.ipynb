{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710525b3",
   "metadata": {},
   "source": [
    "# 代理勾配法によるSNNの訓練\n",
    "\n",
    "前節では再帰的発火率モデルをBPTTによって学習する方法を考えた．この節ではspiking neural network (SNN)をBPTTにより学習させる方法について紹介する．BPTTが生理的でないことは一度棚上げし，工学的な問題としてSNNを訓練する方法について考えるということである．\n",
    "\n",
    "spiking neural networks (SNNs), binary neural networks (BNNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f7382",
   "metadata": {},
   "source": [
    "Perceptron (McCulloch-Pitts neuron)の学習法もいれる？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281438ad",
   "metadata": {},
   "source": [
    "パーセプトロンもstep関数を用いていた．\n",
    "straight through estimator (STE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925aca0b",
   "metadata": {},
   "source": [
    "UNDERSTANDING STRAIGHT-THROUGH ESTIMATOR IN TRAINING ACTIVATION QUANTIZED NEURAL NETS\n",
    "\n",
    "Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d38f8",
   "metadata": {},
   "source": [
    "Hinton (2012) in his lecture 15b\n",
    "\n",
    "G. Hinton. Neural networks for machine learning, 2012.\n",
    "https://www.cs.toronto.edu/~hinton/coursera_lectures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086fccc",
   "metadata": {},
   "source": [
    "\\section{RNNとしてのSNNのBPTTを用いた教師あり学習}\n",
    "この節では発火率ベースのRecurrent neural networks (RNN) の一種のアーキテクチャとしてSpiking neural networksを構成し、\\textbf{Backpropagation Through Time (BPTT)法}を用いて教師あり学習をする方法について説明します。これにより、TensorflowやPytorch, Chainerなどの通常のANNのフレームワークでSNNを学習させることができます。この節では、\\textbf{Spiking Neural Unit }(SNU) と呼ばれる、LSTMやGRUのような状態(state)を持つRNNのユニットを紹介します(Woźniak et al., 2018)。他の類似の研究としては(Wu et al., 2018; Neftci et al., 2019)などを参照してください\\footnote{特に(Neftci et al., 2019)にはJupyter Notebookも用意されています(\\url{https://github.com/fzenke/spytorch})。サーベイも詳しく参考になります。}。\\par\n",
    "Spiking Neural Unit (SNU)は次式で表される、Current-based LIFニューロンが元となっています。\n",
    "\\begin{equation}\n",
    "\\tau \\frac{dV_{m}(t)}{dt}=-V_{m}(t)+R I(t)    \n",
    "\\end{equation}\n",
    "ただし、$\\tau=RC$です。ここでは静止膜電位を0としています\\footnote{静止膜電位を考慮する場合は、定数項$V_{\\text{rest}}$を加えるとよいです。}。これをEuler近似で時間幅$\\Delta t$で離散化し、\n",
    "\\begin{equation}\n",
    "V_{m, t}=\\frac{\\Delta t}{C} I_{t}+\\left(1-\\frac{\\Delta t}{\\tau}\\right)V_{m, t-1}\n",
    "\\end{equation}\n",
    "となります。$V_m$が一定の閾値$V_{\\text{th}}$を超えるとニューロンは発火し、膜電位はリセットされて静止膜電位に戻ります。閾値を超えると発火、ということを表すために次式で表される変数$y_t$を導入し、ステップ関数により発火した際に$y_t=1$となるようにします。\n",
    "\\begin{equation}\n",
    "y_{t}=f\\left(V_{m, t}-V_{\\text{th}}\\right) \n",
    "\\end{equation}\n",
    "ただし、$f(\\cdot)$はステップ関数で、\n",
    "\\begin{equation}\n",
    "f(x) = \\begin{cases}\n",
    "    1 & (x>0) \\\\\n",
    "    0 & (x\\leq0)\n",
    "  \\end{cases}    \n",
    "\\end{equation}\n",
    "と表されます。さらに$y_{t-1}=1$なら膜電位がリセットされるように$\\left(1-y_{t-1}\\right)$を膜電位$V_{m, t-1}$に乗じて膜電位を更新します。\n",
    "\\begin{equation}\n",
    "V_{m, t}=\\frac{\\Delta t}{C} I_{t}+\\left(1-\\frac{\\Delta t}{\\tau}\\right)V_{m, t-1}\\cdot \\left(1-y_{t-1}\\right)\n",
    "\\end{equation}\n",
    "ここで、膜電位を$V_{m, t} \\to s_t$とし、入力電流を$I_{t} \\to Wx_t$とします(ただし、$x_t$は入力、$W$は結合重みの行列)。さらに以前の膜電位を保持する割合を表す変数として$l(\\tau)=(1-\\frac{\\Delta t}{\\tau})$を定義します。このとき、SNUの状態を計算する式は\n",
    "\\begin{align} \n",
    "s_t&=g\\left(Wx_t+l(\\tau)\\odot s_{t-1}\\odot (1-y_{t-1})\\right)\\\\\n",
    "y_t&=h(s_t +b) \n",
    "\\end{align}\n",
    "となります。ただし、$g(\\cdot)$はReLU関数、$h(\\cdot)$はステップ関数です\\footnote{なお、$h(\\cdot)$をシグモイド関数とするsoft-SNUも提案されています。この場合、特に新しく関数を定義する必要はありません。}。このようにLSTMのような状態$y_t$を上手く設定することで、RNNのユニットとしてモデル化できています。\\par\n",
    "しかし、このモデルはステップ関数を含むため、このままでは学習できません。というのも、ステップ関数は微分するとDiracのデルタ関数となり、誤差逆伝搬できないためです。そこで(Woźniak et al., 2018)ではステップ関数の\\textbf{疑似勾配}(pseudo-derivative)としてtanhの微分を用いています。なお疑似勾配と同じ概念が、(Neftci et al., 2019)では\\textbf{代理勾配}(Surrogate Gradient)と呼ばれています。\\par\n",
    "実装方法としてはステップ関数を新しく定義し、逆伝搬時の勾配にtanhの微分などの関数を用いるようにします。コードは示しませんが\\footnote{\\url{https://github.com/takyamamoto/SNU_Chainer}を参考にしてください。}、Chainerで実装した結果を示します。この実装では2値化したMNISTデータセットをポアソン過程モデルでエンコードし(これをJittered MNISTと呼びます)、1つの画像につき10 ms(10タイムステップ)の間、ネットワークにエンコードしたポアソンスパイクを入力します。ネットワークは4層(ユニット数は順に784-256-256-10)から構成され、最後の層で最も発火率の高いユニットに対応するラベルを、刺激画像の予測ラベルとします。注意点として、このネットワークではシナプス入力を考えておらず(シナプスフィルターがなく)、重みづけされたスパイク列が直接次の層のユニットに伝わります。\\par\n",
    "その他、論文の実装と変えたこととしては4点あります。1点目に、ReLUだとdying ReLUが起こっているようで学習がうまく進まなかったので、活性化関数としてExponential Linear Unit (ELU)を代わりに用いました\\footnote{この変更は発火特性に影響を与えません。}。2点目に、ステップ関数の疑似勾配を、tanhの微分では学習が進まなかったので、hard sigmoidのような関数の微分\n",
    "\\begin{equation}\n",
    "f'(x) = \\begin{cases} 1 & (-0.5<x<0.5) \\\\ 0 & (\\text{otherwise}) \\end{cases}\n",
    "\\end{equation}\n",
    "を用いました。3点目に、損失関数を変更しました。平均二乗誤差 (Mean Squared Error)だと学習が進まなかったので、出力ユニットの全スパイク数の和を取り、Softmaxをかけて、正解ラベルとの交差エントロピー(cross entropy)を取りました。また出力ユニットの発火数を抑えるため、代謝コスト(metabolic cost)を損失に加えました。これには正則化の効果もあります。出力層の $i$ 番目のユニットの出力を $y_t^{(i)}$とすると、代謝コスト $C_{\\text{met}}$は $$\n",
    "C_{\\text{met}}=\\frac{10^{-2}}{N_t \\cdot N_{\\text{out}}}\\sum_{t=1}^{N_t}\\sum_{i=1}^{N_{\\text{out}}} \\left(y_t^{(i)}\\right)^2 $$\n",
    "となります。ただし、$N_t$はシミュレーションの総タイムステップ数、$N_{\\text{out}}$は出力ユニットの数（今回だと10個）です。あまり大きくすると、分類誤差よりも代謝コストの方が大きくなってしまうので低めに設定しました。4点目に、optimizerをAdamに変更しました。\\par\n",
    "この実装により100 epoch学習を行った結果を示します。図は誤差と正解率の学習時における変化です。\\par\n",
    "この手法の欠点としてはナイーブにBPTTを実行するのであまりシミュレーションの時間ステップを長くできないということが挙げられます。ただし、通常のANNのフレームワークを用いることができるというのは大きな利点であると思います。\n",
    "\\begin{figure}[htbp]\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.4]{figs/loss_acc.png}\n",
    "    \\caption{(左)誤差の変化、(右)正解率の変化。100 epoch目におけるvalidationの正解率は83\\%程度となりました。}\n",
    "    \\label{fig:snu_1}\n",
    "\\end{figure}\n",
    "\\begin{figure}[htbp]\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.7]{figs/results.pdf}\n",
    "    \\caption{(左)入力のポアソンスパイクの時間軸における和。(右)出力ユニットの活動。7番のニューロンがよく活動していることが分かります。}\n",
    "    \\label{fig:snu_2}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b107e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0eaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3929f703",
   "metadata": {},
   "source": [
    "### 誤差逆伝搬法の近似による教師あり学習\n",
    "\n",
    "ANNは誤差逆伝搬法(Back-propagation)を用いてパラメータを学習することができますが、SNNは誤差逆伝搬法を直接使用することはできません。しかし、誤差逆伝搬法の近似をすることでSNNを訓練することができるようになります。SNNを誤差逆伝搬法で訓練することは\\textbf{SpikeProp法}(Bohte et al., 2000)や\\textbf{ReSuMe}(Ponulak, Kasiński, 2010)など多数の手法が考案されてきました(他の方針としては Lee et al. 2016\\footnote{この論文のポイントは実数値の膜電位で確率的勾配降下を実行することです。}; Huh \\& Sejnowski, 2018; Wu et al., 2018; Shrestha \\& Orchard, 2018; Tavanaei \\& Maida, 2019; Thiele et al., 2019; Comsa et al., 2019など多数)。この章の初めでは、代表してSpikeProp法の改善手法であ\\textbf{るSuperSpike法}(Zenke and Ganguli, 2018)の実装をしてみます。\n",
    "\\section{SuperSpike法}\n",
    "\\textbf{SuperSpike法} (supervised learning rule for spiking neurons)(Zenke and Ganguli, 2018)はオンラインの教師あり学習でSpikeProp法と同様にスパイク列を教師信号とし、そのスパイク列を出力するようにネットワークを最適化します。SpikeProp法と異なるのはスパイクの微分ではなく、膜電位についての関数の微分を用いていることです。このため、発火が生じなくても学習が進行します。\n",
    "\\subsection{損失関数の導関数の近似}\n",
    "まず最小化したい損失関数$L$から考えましょう。$i$番目のニューロンの教師信号となるスパイク列$\\hat{S}_{i}$に出力${S}_{i}$を近づけます(スパイク列は$S_i(t)=\\sum_{t_{k}< t} \\delta\\left(t-t_i^{k}\\right)$と表されます)\\footnote{通常、予測値に$\\hat{}$を付けることが多いですが、ここでは論文の表記に従って$\\hat{S}$を教師信号としています。}。SpikeProp法ではこれらの二乗誤差を損失関数としていましたが、SuperSpike法ではそれぞれのスパイク列を二重指数関数フィルター$\\alpha$で畳み込みした後に二乗誤差を取ります。\n",
    "\\begin{equation}\n",
    "L(t)=\\frac{1}{2} \\int_{-\\infty}^{t} d s\\left[\\left(\\alpha * \\hat{S}_{i}-\\alpha * S_{i}\\right)(s)\\right]^{2}\n",
    "\\end{equation}\n",
    "ただし、$*$は畳み込み演算子です。これは\\textbf{van Rossum 距離} (van Rossum, 2001)\\footnote{スパイク列の類似度を計算する手法としては他にVictor-Purpura 距離や、 Schreiber \\textit{et al.}類似度など、数多く考案されています。(Dauwels et al., 2008)やScholarpediaの\"Measures of spike train synchrony\"(\\url{http://www.scholarpedia.org/article/Measures_of_spike_train_synchrony})を参照してください。}を表します。損失関数をこのように設定することで、SpikePropと異なり、完全にスパイク列が一致するまで誤差信号は0になりません。\n",
    "損失関数$L$を$j$番目のシナプス前ニューロンから$i$番目のシナプス後ニューロンへのシナプス強度$w_{ij}$で微分すると、次のようになります。\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w_{i j}}=-\\int_{-\\infty}^{t} d s\\left[\\left(\\alpha * \\hat{S}_{i}-\\alpha * S_{i}\\right)(s)\\right]\\left(\\alpha * \\frac{\\partial S_{i}}{\\partial w_{i j}}\\right)(s)    \n",
    "\\end{equation}\n",
    "目標はこの$\\dfrac{\\partial L}{\\partial w_{i j}}$を計算し、確率的勾配降下法(stochastic gradient discent; SGD)により$w_{ij}\\leftarrow w_{ij}-r \\dfrac{\\partial L}{\\partial w_{i j}}$と最適化することです(ただし$r$は学習率)。ここでの問題点は$\\dfrac{\\partial S_{i}}{\\partial w_{i j}}$の部分です。$S_i$は$\\delta$関数を含むため、微分すると発火時は$\\infty$, 非発火時は0となり、学習が進みません。そこで$S_i(t)$をLIFニューロンの膜電位\\footnote{これまで$V$や$v$を使っていましたが、論文にあわせて$U$を用います。}$U_i(t)$の非線形関数$\\sigma(U_i(t))$で近似します。非線形関数としては高速シグモイド関数(fast sigmoid) $\\sigma(x)=x/(1+|x|)$を使用しています。ここまでの近似計算を纏めると\n",
    "\\begin{equation}\n",
    "\\frac{\\partial S_{i}}{\\partial w_{ij}}\\approx\\frac{\\partial \\sigma\\left(U_{i}\\right)}{\\partial w_{ij}}=\\sigma^{\\prime}\\left(U_{i}\\right) \\frac{\\partial U_{i}}{\\partial w_{i j}}    \n",
    "\\end{equation}\n",
    "となります。ただし、$\\sigma^{\\prime}\\left(U_{i}\\right)=(1+|\\beta(U_i-\\vartheta)|)^{-2}$です。$\\vartheta$はLIFニューロンの発火閾値で$-$50 mVとされています。$\\beta$は係数で(1 mV)$^{-1}$です。\\par\n",
    "残った$\\dfrac{\\partial U_{i}}{\\partial w_{i j}}$の部分ですが、シナプス強度$w_{ij}$の変化により$j$番目のシナプス前ニューロンの発火$S_j(t)$が$i$番目のシナプス後細胞の膜電位変化に与える影響が変化するという観点から、$\\dfrac{\\partial U_{i}}{\\partial w_{i j}}\\approx \\epsilon* S_j(t)$と近似します。ただし、$\\epsilon$は$\\alpha$と同じ二重指数関数フィルターです。また、これはシナプスでの神経伝達物質の濃度として解釈できるとされています。\\par\n",
    "ここまでの近似を用いると、時刻$t$におけるシナプス強度の変化率$\\dfrac{\\partial w_{ij}}{\\partial t}$は\n",
    "\\begin{align}\n",
    "\\frac{\\partial w_{ij}}{\\partial t}&=-r \\dfrac{\\partial L}{\\partial w_{i j}}\\\\\n",
    "&\\approx r\\int_{-\\infty}^{t} ds\\underbrace{\\left[\\alpha * \\left(\\hat{S}_{i}-S_{i}\\right)(s)\\right]}_{誤差信号}\\quad\\alpha *\\left[ \\underbrace{\\sigma^{\\prime}\\left(U_{i}(s)\\right)}_{後細胞}\\underbrace{\\left(\\epsilon * S_{j}\\right)(s)}_{前細胞}\\right]\\\\\n",
    "&=r\\int_{-\\infty}^{t} ds\\ \\ e_i(s)\\cdot \\lambda_{ij}(s)\n",
    "\\end{align}\n",
    "と表せます。ここで、$e_i(t)=\\alpha * \\left(\\hat{S}_{i}-S_{i}\\right)$, $\\lambda_{ij}(t)=\\alpha *\\left[\\sigma^{\\prime}\\left(U_{i}(s)\\right)\\left(\\epsilon * S_{j}\\right)(s)\\right]$としました。$e_i(t)$は誤差信号(error signal)で、シナプス前細胞にフィードバックされます。$\\lambda_{ij}(t)$はシナプス適格度トレース(synaptic eligibility trace)を表します\\footnote{これは遅延報酬問題(distal reward problem)を解決していると説明されています。また、生理学的にはCa$^{2+}$トランジェント(calcium transient)や関連するシグナル伝達カスケード(signaling cascade)として実現可能であるとされています。}。\\par\n",
    "\\subsection{離散化した重みの更新とRMaxProp}\n",
    "前項における$\\dfrac{\\partial w_{ij}}{\\partial t}$ は時刻$t$までの全ての誤差情報を積分していますが、実装する上での利便性を考え、時刻$[t_k, t_{k+1}]$ の間の積分を用いて重みを更新します\\footnote{これはミニバッチによる更新に類似しています。}。\n",
    "\\begin{equation}\n",
    "\\Delta w_{i j}^{k}=r_{ij} \\int_{t_{k}}^{t_{k+1}} e_{i}(s) \\lambda_{ij}(s) ds      \n",
    "\\end{equation}\n",
    "ただし、$r_{ij}$は重み$w_{ij}$ごとの学習率です(これは後で説明します)。さらに実装時には$t_b:={t_{k+1}}-{t_{k}}\\ (=0.5$ s)とし、0で初期化されている配列[$m_{ij}$]をステップごとに\n",
    "\\begin{equation}\n",
    "m_{ij} \\leftarrow m_{ij} + g_{ij}    \n",
    "\\end{equation}\n",
    "という式により更新します。ただし、$g_{ij}=e_{i}(t) \\lambda_{ij}(t)$です。$t_b$だけ経過すると、\n",
    "\\begin{equation}\n",
    "w_{ij} \\leftarrow w_{ij} + r_{ij}m_{ij}\\cdot \\Delta t\n",
    "\\end{equation}\n",
    "として重み$w_{ij}$を更新し、$m_{ij}$を0にリセットします\\footnote{$\\Delta t$は元の論文には記載されていないですが、タイムステップの長さが変化しても良いようにするためにつけています。}。さらに更新時は重みに$-1<w_{ij}<1$という制限をつけています。\\par\n",
    "学習率$r$は全ての重みに対して同じものを用いても学習は可能ですが、安定はしません。そこで、ANNのOptimizerの一種である\\textbf{RMSprop}と類似した更新を行います。\\par\n",
    "まず、新しく配列$[v_{ij}]$を用意します。タイムステップごとに\n",
    "$$v_{ij} \\leftarrow \\max(\\gamma v_{ij}, g_{ij}^2)$$\n",
    "で更新します。ただし、$\\gamma$はハイパーパラメータです(明確な値の記載がありませんが、実験の結果から0.8程度の値がよいでしょう)。この$v_{ij}$を用いて重みごとの学習率$r_{ij}$を次のように定義します。\n",
    "\\begin{equation}\n",
    "r_{ij}=\\frac{r_0}{\\sqrt{v_{ij}}+\\varepsilon}\n",
    "\\end{equation}\n",
    "ただし、$r_0$は学習係数、$\\varepsilon$はゼロ除算を避けるための小さい値(典型的には$\\varepsilon=10^{-8}$)です。記載はありませんが、学習係数の減衰(learning rate decay)を行うと学習がよく進みました。\\par\n",
    "以上の更新法を著者らは\\textbf{RMaxProp}と名付けています。なお、RMSpropの場合は$g_{ij}^2$の移動平均を次式のように行います。\n",
    "$$v_{ij} \\leftarrow \\gamma v_{ij}+(1-\\gamma)\\cdot g_{ij}^2$$\n",
    "\\subsection{誤差信号の逆伝搬について}\n",
    "出力層において誤差信号は$e_i(t)=\\alpha * \\left(\\hat{S}_{i}-S_{i}\\right)$と計算されます。これを低次の層に逆伝搬すること、つまり$l$層目の$k$番目のニューロンの誤差信号$e_k$を$l-1$層目の$i$番目のニューロンに投射することを考えます。対称なフィードバックをする場合、$W=[w_{ik}]$の転置行列$W^\\intercal=[w_{ki}]$を用いて、\n",
    "\\begin{equation}\n",
    "e_i=\\sum_k w_{ki} e_k\n",
    "\\end{equation}\n",
    "となります。ここでANNの誤差逆伝搬のように、$l-1$番目の層の出力を引数とする活性化関数の勾配を乗じません。\\par\n",
    "この対称フィードバックは順伝搬の重みの転置行列を用いるため、生物学的には妥当ではありません。そこで誤差逆伝搬法の対称な重みを使う問題を解消する手法として\\textbf{Feedback alignment} (Lillicrap et al., 2016)があります\\footnote{Feedback alignmentの発展については(Nøkland 2016; Akrout et al., 2019; Lansdell et al 2019)を参照してください。}。Feedback alignmentでは逆伝搬時に用いる重みをランダムに固定したものとします\\footnote{なぜこれが上手くいくかを書く時間がありませんでした。論文を読んでください。}。このとき、ランダムな固定重みを$B=[b_{ki}]$とすると、Feedback alignmentの場合は\n",
    "\\begin{equation}\n",
    "e_i=\\sum_k b_{ki} e_k\n",
    "\\end{equation}\n",
    "となります。また、重みを均一なものとするUniform feedbackによる学習も紹介されています。この場合は、\n",
    "\\begin{equation}\n",
    "e_i=\\sum_k e_k\n",
    "\\end{equation}\n",
    "となります。後の実装ではFeedback alignmentによる学習も行います。\n",
    "\n",
    "\\subsection{SuperSpike法の実装}\n",
    "それではSuperSpike法を実装していきましょう。今回は3層のネットワーク(ユニット数は順に50, 4, 1)の出力ユニットが100 msごとに発火するように訓練します。 訓練後の結果と全体のアルゴリズムは図\\ref{fig:super_spike}にまとめていますので、適宜参照すると良いでしょう。。\\par\n",
    "まず、\\texttt{Models}ディレクトリの親ディレクトリに実行するファイルを作成します。次に今回使うモデルをimportしておきます。\n",
    "\\begin{minted}[frame=lines, framesep=2mm, baselinestretch=1.2, bgcolor=shadecolor,fontsize=\\small]{python}\n",
    "from Models.Neurons import CurrentBasedLIF\n",
    "from Models.Synapses import DoubleExponentialSynapse\n",
    "from Models.Connections import FullConnection, DelayConnection\n",
    "\\end{minted}\n",
    "\\subsubsection{誤差信号と適格度トレースの実装}\n",
    "まず、誤差信号と適格度トレースの計算を行うclassを実装します。とはいえ、二重指数関数型シナプスのコードに少し変更を加えるだけでよいです。異なる点として、誤差信号では出力層のスパイク列(\\texttt{output\\_spike})と教師信号のスパイク列(\\texttt{target\\_spike})を引数とし、それらの差分を取ります。さらにこの時、出力が$[-1, 1]$となるように規格化を行います。\n",
    "\\begin{minted}[frame=lines, framesep=2mm, baselinestretch=1.2, bgcolor=shadecolor,fontsize=\\small]{python}\n",
    "class ErrorSignal:\n",
    "    def __init__(self, N, dt=1e-4, td=1e-2, tr=5e-3):\n",
    "        self.dt = dt\n",
    "        self.td = td\n",
    "        self.tr = tr\n",
    "        self.N = N\n",
    "        self.r = np.zeros(N)\n",
    "        self.hr = np.zeros(N)\n",
    "        self.b = (td/tr)**(td/(tr-td)) # 規格化定数\n",
    "    \n",
    "    def initialize_states(self):\n",
    "        self.r = np.zeros(self.N)\n",
    "        self.hr = np.zeros(self.N)    \n",
    "\n",
    "    def __call__(self, output_spike, target_spike):\n",
    "        r = self.r*(1-self.dt/self.tr) + self.hr/self.td*self.dt \n",
    "        hr = self.hr*(1-self.dt/self.td)+(target_spike-output_spike)/self.b\n",
    "        self.r = r\n",
    "        self.hr = hr\n",
    "        return r\n",
    "\\end{minted}\n",
    "次に、適格度トレースはシナプス前細胞のシナプスフィルターをかけられたスパイク列と、シナプス後細胞の膜電位を引数とします。シナプス後細胞の膜電位は高速シグモイド関数の微分した式に代入され、Online STDP\\footnote{5章参照}の計算のように列ベクトル(postの活動)と行ベクトル(preの活動)の積を取ります。\n",
    "\\begin{minted}[frame=lines, framesep=2mm, baselinestretch=1.2, bgcolor=shadecolor,fontsize=\\small]{python}\n",
    "lass EligibilityTrace:\n",
    "    def __init__(self, N_in, N_out, dt=1e-4, td=1e-2, tr=5e-3):\n",
    "        self.dt = dt\n",
    "        self.td = td\n",
    "        self.tr = tr\n",
    "        self.N_in = N_in\n",
    "        self.N_out = N_out\n",
    "        self.r = np.zeros((N_out, N_in))\n",
    "        self.hr = np.zeros((N_out, N_in))\n",
    "    \n",
    "    def initialize_states(self):\n",
    "        self.r = np.zeros((self.N_out, self.N_in))\n",
    "        self.hr = np.zeros((self.N_out, self.N_in))\n",
    "    \n",
    "    def surrogate_derivative_fastsigmoid(self, u, beta=1, vthr=-50):\n",
    "        return 1 / (1 + np.abs(beta*(u-vthr)))**2\n",
    "\n",
    "    def __call__(self, pre_current, post_voltage):\n",
    "        # (N_out, 1) x (1, N_in) -> (N_out, N_in) \n",
    "        pre_ = np.expand_dims(pre_current, axis=0)\n",
    "        post_ = np.expand_dims(\n",
    "                self.surrogate_derivative_fastsigmoid(post_voltage), \n",
    "                axis=1)\n",
    "        r = self.r*(1-self.dt/self.tr) + self.hr*self.dt \n",
    "        hr = self.hr*(1-self.dt/self.td) + (post_ @ pre_)/(self.tr*self.td)\n",
    "        self.r = r\n",
    "        self.hr = hr\n",
    "        return r\n",
    "\\end{minted}\n",
    "\\subsubsection{定数とモデルの定義}\n",
    "それでは準備が終わったので、定数とモデルのインスタンスを定義しましょう。\n",
    "\\begin{minted}[frame=lines, framesep=2mm, baselinestretch=1.2, bgcolor=shadecolor,fontsize=\\small]{python}\n",
    "dt = 1e-4; T = 0.5; nt = round(T/dt)\n",
    "\n",
    "t_weight_update = 0.5 #重みの更新時間\n",
    "nt_b = round(t_weight_update/dt) #重みの更新ステップ\n",
    "\n",
    "num_iter = 200 # 学習のイテレーション数\n",
    "\n",
    "N_in = 50 # 入力ユニット数\n",
    "N_mid = 4 # 中間ユニット数\n",
    "N_out = 1 # 出力ユニット数\n",
    "\n",
    "# 入力(x)と教師信号(y)の定義\n",
    "fr_in = 10 # 入力のPoisson発火率 (Hz)\n",
    "x = np.where(np.random.rand(nt, N_in) < fr_in * dt, 1, 0)\n",
    "y = np.zeros((nt, N_out)) \n",
    "y[int(nt/10)::int(nt/5), :] = 1 # T/5に1回発火\n",
    "\n",
    "# モデルの定義\n",
    "neurons_1 = CurrentBasedLIF(N_mid, dt=dt)\n",
    "neurons_2 = CurrentBasedLIF(N_out, dt=dt)\n",
    "delay_conn1 = DelayConnection(N_in, delay=8e-4)\n",
    "delay_conn2 = DelayConnection(N_mid, delay=8e-4)\n",
    "synapses_1 = DoubleExponentialSynapse(N_in, dt=dt, td=1e-2, tr=5e-3)\n",
    "synapses_2 = DoubleExponentialSynapse(N_mid, dt=dt, td=1e-2, tr=5e-3)\n",
    "es = ErrorSignal(N_out)\n",
    "et1 = EligibilityTrace(N_in, N_mid)\n",
    "et2 = EligibilityTrace(N_mid, N_out)\n",
    "\n",
    "connect_1 = FullConnection(N_in, N_mid, \n",
    "                           initW=0.1*np.random.rand(N_mid, N_in))\n",
    "connect_2 = FullConnection(N_mid, N_out, \n",
    "                           initW=0.1*np.random.rand(N_out, N_mid))\n",
    "#B = np.random.rand(N_mid, N_out) # Feedback alignment\n",
    "\n",
    "r0 = 1e-3\n",
    "gamma = 0.7\n",
    "\n",
    "# 記録用配列\n",
    "current_arr = np.zeros((N_mid, nt))\n",
    "voltage_arr = np.zeros((N_out, nt))\n",
    "error_arr = np.zeros((N_out, nt))\n",
    "lambda_arr = np.zeros((N_out, N_mid, nt))\n",
    "dw_arr = np.zeros((N_out, N_mid, nt))\n",
    "cost_arr = np.zeros(num_iter)\n",
    "\\end{minted}\n",
    "ここで配列\\texttt{B}はFeedback alignmentの際に用います。\n",
    "\\subsubsection{シミュレーションの実装}\n",
    "\\texttt{for}ループ内でモデルを構築し、\\texttt{nt\\_b}ステップごとに重みの更新を行います。また、最後の訓練イテレーション時に、出力層の膜電位の時間変化などを記録しておきます。\n",
    "\\begin{minted}[frame=lines, framesep=2mm, baselinestretch=1.2, bgcolor=shadecolor,fontsize=\\small]{python}\n",
    "for i in tqdm(range(num_iter)):\n",
    "    if i%15 == 0:\n",
    "        r0 /= 2 # 重み減衰\n",
    "    \n",
    "    # 状態の初期化\n",
    "    neurons_1.initialize_states()\n",
    "    neurons_2.initialize_states()\n",
    "    synapses_1.initialize_states()\n",
    "    synapses_2.initialize_states()\n",
    "    delay_conn1.initialize_states()\n",
    "    delay_conn2.initialize_states()\n",
    "    es.initialize_states()\n",
    "    et1.initialize_states()\n",
    "    et2.initialize_states()\n",
    "    \n",
    "    m1 = np.zeros((N_mid, N_in))\n",
    "    m2 = np.zeros((N_out, N_mid))\n",
    "    v1 = np.zeros((N_mid, N_in))\n",
    "    v2 = np.zeros((N_out, N_mid))\n",
    "    cost = 0\n",
    "    count = 0\n",
    "    \n",
    "    # one iter.\n",
    "    for t in range(nt):\n",
    "        # Feed-forward\n",
    "        c1 = synapses_1(delay_conn1(x[t])) # input current\n",
    "        h1 = connect_1(c1)\n",
    "        s1 = neurons_1(h1) # spike of mid neurons\n",
    "        \n",
    "        c2 = synapses_2(delay_conn2(s1))\n",
    "        h2 = connect_2(c2)\n",
    "        s2 = neurons_2(h2)\n",
    "        \n",
    "        # Backward(誤差の伝搬)\n",
    "        e2 = np.expand_dims(es(s2, y[t]), axis=1) / N_out\n",
    "        e1 = connect_2.backward(e2) / N_mid\n",
    "        # e1 = np.dot(B, e2) / N_mid\n",
    "\n",
    "        # コストの計算\n",
    "        cost += np.sum(e2**2)\n",
    "        \n",
    "        lambda2 = et2(c2, neurons_2.v_)\n",
    "        lambda1 = et1(c1, neurons_1.v_)\n",
    "        \n",
    "        g2 = e2 * lambda2\n",
    "        g1 = e1 * lambda1\n",
    "        \n",
    "        v1 = np.maximum(gamma*v1, g1**2)\n",
    "        v2 = np.maximum(gamma*v2, g2**2)\n",
    "        \n",
    "        m1 += g1\n",
    "        m2 += g2\n",
    "    \n",
    "        count += 1\n",
    "        if count == nt_b:\n",
    "            # 重みの更新\n",
    "            lr1 = r0/np.sqrt(v1+1e-8)\n",
    "            lr2 = r0/np.sqrt(v2+1e-8)\n",
    "            dW1 = np.clip(lr1*m1*dt, -1e-3, 1e-3)\n",
    "            dW2 = np.clip(lr2*m2*dt, -1e-3, 1e-3)\n",
    "            connect_1.W = np.clip(connect_1.W+dW1, -0.1, 0.1)\n",
    "            connect_2.W = np.clip(connect_2.W+dW2, -0.1, 0.1)\n",
    "            \n",
    "            # リセット\n",
    "            m1 = np.zeros((N_mid, N_in))\n",
    "            m2 = np.zeros((N_out, N_mid))\n",
    "            v1 = np.zeros((N_mid, N_in))\n",
    "            v2 = np.zeros((N_out, N_mid))\n",
    "            count = 0\n",
    "            \n",
    "        # 保存\n",
    "        if i == num_iter-1:\n",
    "            current_arr[:, t] = c2\n",
    "            voltage_arr[:, t] = neurons_2.v_\n",
    "            error_arr[:, t] = e2\n",
    "            lambda_arr[:, :, t] = lambda2\n",
    "    \n",
    "    cost_arr[i] = cost\n",
    "    print(\"\\n　cost:\", cost)\n",
    "\\end{minted}\n",
    "なお、\\texttt{r}は重みの係数ですが、これを減衰させる(つまりweight decay)するとパフォーマンスが上がったので、今回冒頭に入れています。また、\\texttt{lr}は更新時の値を用いていますが、これはANNにおいて入力ごとの勾配を加算し、重みの更新はミニバッチ内の全ての要素に対して同じ学習率で行うということに対応します。また、Feedback alignmentの場合は誤差逆伝搬に\\colorbox{shadecolor}{\\texttt{e1 = np.dot(B, e2) / N\\_mid}}を用います。\n",
    "\\subsubsection{結果の描画}\n",
    "最後に結果を描画します。描画するのは出力層の膜電位$U_i$, 高速シグモイドによる膜電位の微分の近似$\\sigma^\\prime (U_i)$, 出力層における誤差信号$e_i$, 適格度トレース$\\lambda_{ij}$, 2層目の$j=0$番目のシナプス後電流$\\epsilon * S_j$, 入力のポアソンスパイク、誤差関数の推移です。\n",
    "\\begin{minted}[frame=lines, framesep=2mm, baselinestretch=1.2, bgcolor=shadecolor,fontsize=\\small]{python}\n",
    "t = np.arange(nt)*dt*1e3\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.subplot(6,1,1)\n",
    "plt.plot(t, voltage_arr[0])\n",
    "plt.ylabel('Membrane\\n potential (mV)')\n",
    "plt.subplot(6,1,2)\n",
    "plt.plot(t, et1.surrogate_derivative_fastsigmoid(u=voltage_arr[0]))\n",
    "plt.ylabel('Surrogate derivative')\n",
    "plt.subplot(6,1,3)\n",
    "plt.plot(t, error_arr[0])\n",
    "plt.ylabel('Error')\n",
    "plt.subplot(6,1,4)\n",
    "plt.plot(t, lambda_arr[0, 0], color=\"k\")\n",
    "plt.ylabel('$\\lambda$')\n",
    "plt.subplot(6,1,5)\n",
    "plt.plot(t, current_arr[0], color=\"k\")\n",
    "plt.ylabel('Input current (pA)')\n",
    "plt.subplot(6,1,6)\n",
    "for i in range(N_in):    \n",
    "    plt.plot(t, x[:, i]*(i+1), 'ko', markersize=2)\n",
    "plt.xlabel('Time (ms)'); plt.ylabel('Neuron index') \n",
    "plt.xlim(0, t.max()); plt.ylim(0.5, N_in+0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(cost_arr, color=\"k\")\n",
    "plt.xlabel('Iter'); plt.ylabel('Cost') \n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "結果は図\\ref{fig:super_spike}のようになります。また、Feedback alignmentの場合と比較した誤差関数の推移は図のようになります。\n",
    "\\begin{figure}[htbp]\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.45]{figs/super_spike_2.pdf}\n",
    "    \\caption{SuperSpike法の結果とアルゴリズム。上から出力層の膜電位$U_i$, 高速シグモイドによる膜電位の微分の近似$\\sigma^\\prime (U_i)$, 出力層における誤差信号$e_i$, 適格度トレース$\\lambda_{ij}$, 2層目の$j=0$番目のシナプス後電流$\\epsilon * S_j$, 入力のポアソンスパイクを表します。}\n",
    "    \\label{fig:super_spike}\n",
    "\\end{figure}\n",
    "\\begin{figure}[htbp]\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.45]{figs/super_spike_cost.pdf}\n",
    "    \\caption{誤差関数の推移。(左)対称フィードバックの場合。(右)Feedback alignmentの場合}\n",
    "    \\label{fig:super_spike_2}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f244b",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
