{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配法と誤差逆伝播法\n",
    "ニューラルネットワークにおいて，効率よく各重みの勾配を推定することで貢献度割り当て問題を解決する方法が**誤差逆伝播法** (backpropagation) である．本節では入力層，隠れ層，出力層からなる多層ニューラルネットワークを実装し，誤差逆伝播法による勾配推定を用いて学習を行う．\n",
    "\n",
    "本書では誤差逆伝播法を用いない学習法を実施することも考慮し，数式と対応するような実装を行う．そのため，Deep Learningライブラリ (PyTorch, Flux.jl等) のようにLayerを定義し，それを繋げてモデルを定義するということや計算グラフの構築は行わない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base: @kwdef\n",
    "#using Parameters: @unpack # or using UnPack\n",
    "using LinearAlgebra, Random, PyPlot, ProgressMeter\n",
    "#using LinearAlgebra, Random, Statistics, PyPlot, ProgressMeter\n",
    "#using BenchmarkTools; @btime\n",
    "rc(\"axes.spines\", top=false, right=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数 (activation function) の構造体を`ActivationFunction` と定義する．この構造体には`forward` と `backward`の2種類の関数フィールド (field; 構造体の要素のこと) を持たせておく．2種類の関数はそれぞれ順伝播時と逆伝播時に使用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ActivationFunction\n",
    "    forward::Function   # function for forward propagation\n",
    "    backward::Function  # function for back-propagation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，次のコードにより，構造体のインスタンスを関数として使用できるようにしておく．これはcallable objectと呼ばれる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f::ActivationFunction)(x) = f.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代表的な活性化関数を紹介する．なお，`backward` における `y` は `forward` での出力に対応する．これは活性化関数を作用させる前の変数 ($x$であり，膜電位に対応する) を保持しておかなくても良いようにするためである．\n",
    "\n",
    "シグモイド関数 (sigmoid function) あるいはロジスティック関数 (logistic function) の場合，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textrm{forward: } y = \\frac{1}{1+e^{-x}}\\\\\n",
    "&\\textrm{backward: } \\frac{dy}{dx} =\\frac{e^{-x}}{(1+e^{-x})^2}= y\\cdot (1-y)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "tanh関数の場合，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textrm{forward: } y = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\\\\\n",
    "&\\textrm{backward: } \\frac{dy}{dx} =\\frac{(e^x+e^{-x})(e^x+e^{-x})-(e^x-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}= 1-y^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ReLU関数 (rectified linear unit function, 正規化線形関数) あるいはランプ関数 (ramp function)の場合，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textrm{forward: } y = \\max(x, 0)\\\\\n",
    "&\\textrm{backward: } \\frac{dy}{dx} = \\mathbf{1}_{x > 0}(x) = \\mathbf{1}_{y > 0}(y)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ただし，$\\max(a, b)$は, $a, b$のうち，大きい値を返す関数である．また，$\\mathbf{1}_{A}(x)$ は指示関数 (indicator function)であり，$x\\in A$ ならば $\\mathbf{1}_A(x)=1$ であり，それ以外の場合は $\\mathbf{1}_A(x)=0$ となる関数である．ReLU関数は $x=0$ で折れ曲がるが，その他では線形であるため，区分線形関数 (piecewise linear function) の一種であると言える．\n",
    "\n",
    "これらの活性化関数を構造体 `ActivationFunction`を用いて実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigmoid = ActivationFunction(\n",
    "    x -> 1.0 ./ (1.0 .+ exp.(-x)),\n",
    "    y -> y .* (1 .- y)\n",
    ")\n",
    "\n",
    "Tanh = ActivationFunction(\n",
    "    x -> tanh.(x),\n",
    "    y -> 1 .- y .^ 2\n",
    ")\n",
    "\n",
    "Relu = ActivationFunction(\n",
    "    x -> max.(0, x),\n",
    "    y -> y .> 0\n",
    ")\n",
    "\n",
    "Identity = ActivationFunction(\n",
    "    x -> x, # or use `identity` function\n",
    "    y -> fill!(similar(y), 1)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークのパラメータを保持する構造体を定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNet end\n",
    "(f::NeuralNet)(x) = forward!(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワーク内の学習可能なパラメータ (learnable parameter) を保持する構造体を定義する．各パラメータはそれ自体の値 (value) と損失関数に対する勾配 (gradient) を持つ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Param\n",
    "    v::Array\n",
    "    grad::Array\n",
    "\n",
    "    Param(value) = new(value, zero(value))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MLP <: NeuralNet\n",
    "    L::Int # num. of layers\n",
    "    W::Vector{Param}; b::Vector{Param}   # weights and bias\n",
    "    z::Vector{Array}; δ::Vector{Array}   # state of forward/backward activity\n",
    "    f::Vector{ActivationFunction} # activation functions of layers\n",
    "\n",
    "    function MLP(n_units::Vector{Int}; f_hid::ActivationFunction=Sigmoid, f_out::ActivationFunction=Identity)\n",
    "        L = length(n_units) - 1\n",
    "        # initialization of parameters\n",
    "        #W = [Param(2 * (rand(n_units[l], n_units[l+1]) .- 0.5) * sqrt(6/n_units[l])) for l in 1:L] # He\n",
    "        W = [Param(2 * (rand(n_units[l], n_units[l+1]) .- 0.5) * sqrt(6/(n_units[l]+n_units[l+1]))) for l in 1:L] # Xavier\n",
    "        b = [Param(zeros(1, n_units[l+1])) for l in 1:L]\n",
    "        \n",
    "        # initialization of forward / backward states\n",
    "        z, δ = Vector{Array}(undef, L+1), Vector{Array}(undef, L)\n",
    "        f = vcat([repeat([f_hid], L-1)..., f_out])\n",
    "        new(L, W, b, z, δ, f)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "struct `MLP`を用意し，**重みの初期化(weight initialization)** を行う同名の関数`MLP`を用意する．重みの初期化に関しては，各層の出力および勾配の分散が一定となるような初期化をすることで学習が進行することが知られている．出力は活性化関数に依存するため，初期化についても活性化関数に応じて変更することが推奨され，sigmoid関数やtanh関数を用いる場合はXavierの初期化 \\citep{Glorot2010-iu}，ReLU関数を用いる場合はHeの初期化 \\citep{He2015-fs} が用いられる．入力ユニット数を $n_{\\textrm{in}}$, 出力ユニット数を $n_{\\textrm{out}}$ とすると，Xavierの初期化では重み $w$ の平均が0, 分散が $\\frac{2}{n_{\\textrm{in}}+n_{\\textrm{out}}}$ となるように一様分布 $U\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right)$ や正規分布 $\\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right)$ 等から重みをサンプリングする．Heの初期化ではReLUを用いる場合，重み $w$ の平均が0, 分散が$\\frac{2}{n_{\\textrm{in}}}$ あるいは $\\frac{2}{n_{\\textrm{out}}}$ となるようにし，前者の分散を使用する場合は一様分布 $U\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}}}\\right)$ や正規分布 $\\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}}}\\right)$ 等から重みをサンプリングする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 順伝播 (forward propagation)\n",
    "$f(\\cdot)$を活性化関数とする．順伝播(feedforward propagation)は以下のようになる．$(\\ell=1,\\ldots,L)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{入力層 : }&\\mathbf{z}_1=\\mathbf{x}\\\\\n",
    "\\text{隠れ層 : }&\\mathbf{a}_\\ell=W_\\ell \\mathbf{z}_\\ell +\\mathbf{b}_\\ell\\\\\n",
    "&\\mathbf{z}_{\\ell+1}=f_\\ell\\left(\\mathbf{a}_\\ell\\right)\\\\\n",
    "\\text{出力層 : }&\\hat{\\mathbf{y}}=\\mathbf{z}_{L+1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装における取り回しの都合上，以下では$f:=f_\\ell\\ (1, \\ldots, L-1)$ とし，$g:=f_{L}$ は損失関数において記述されることとする．以下では`z[l]`は$\\mathbf{z}_\\ell \\ (1, \\ldots, L)$に対応するが，`z[L+1]`は$\\mathbf{z}_{L+1}$ではなく$\\mathbf{a}_{L}$に対応することに注意．\n",
    "\n",
    "とするか？混乱をきたしかねないし，余計な計算が増える．softmaxをどうするか．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward computation of MLP\n",
    "function forward!(mlp::MLP, x::Array)\n",
    "    (;L, f, W, b, z) = mlp\n",
    "    z[1] = x # input (n_batch x n_neurons)\n",
    "    for l in 1:L\n",
    "        z[l+1] = f[l](z[l] * W[l].v .+ b[l].v) # hidden layers\n",
    "    end\n",
    "    return z[L+1] # output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W[l].v` は`l`番目の`Param`のインスタンスにおける`v`を取り出す操作である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆伝播 (backward propagation)\n",
    "ニューラルネットワークの学習 (learning) あるいは訓練 (training) とは，目的関数 (objective function) あるいは損失関数 (loss function) と呼ばれる評価指標を可能な限り小さく (場合によっては大きく) するようなパラメータ集合 $\\Theta = \\{W_\\ell, b_\\ell\\}_{\\ell=1}^{L}$ を求める過程のことである．学習においてパラメータを最適化するアルゴリズムを**オプティマイザ** (optimizer) という．オプティマイザは多数提案されており，代表的なものを後ほど紹介する．まず，最も単純なオプティマイザである **勾配降下法** (gradient descent; GD) を紹介する．勾配降下法では全データを用いてパラメータ $\\theta \\in \\Theta$ の更新量 $\\Delta \\theta$ を \n",
    "\n",
    "$$\n",
    "\\Delta \\theta = -\\eta \\frac{\\partial \\mathcal{L}_{\\textrm{GD}}}{\\partial \\theta} = -\\frac{\\eta}{N} \\sum_{i=1}^N \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "として計算する (パラメータは$\\theta\\leftarrow \\theta + \\Delta \\theta$により更新される)．ただし，$\\mathcal{L}_{\\textrm{GD}}:=\\frac{1}{N}\\sum_{i=1}^N \\mathcal{L}^{(i)}$ であり，$\\mathcal{L}^{(i)}$は$i$ 番目のサンプルに対する目的関数であり，$N$ は全データのサンプル数を意味する．$\\eta$ は学習率 (learning rate) である．オプティマイザは一般的に勾配 $\\dfrac{\\partial \\mathcal{L}}{\\partial \\theta}$ の計算を必要とする．この計算を効率よく行う手法が**誤差逆伝播法** (backpropagation) である．誤差逆伝播法は連鎖律 (chain rule; 合成関数の微分の関係式) を用いて導くことができる．$\\mathbf{a}_\\ell=W_\\ell \\mathbf{z}_\\ell +\\mathbf{b}_\\ell$ および $\\mathbf{z}_{\\ell+1}=f_\\ell\\left(\\mathbf{a}_\\ell\\right)$ であることを踏まえると，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}\\\\\n",
    "\\delta_L&:=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_L}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}} \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_L}\\\\\n",
    "\\mathbf{\\delta}_\\ell&:=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell}}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{\\ell+1}} \\frac{\\partial \\mathbf{z}_{\\ell+1}}{\\partial \\mathbf{a}_\\ell}\\\\\n",
    "&=\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell+1}}\\frac{\\partial \\mathbf{a}_{\\ell+1}}{\\partial \\mathbf{z}_{\\ell+1}}\\right)\\frac{\\partial \\mathbf{z}_{\\ell+1}}{\\partial \\mathbf{a}_{\\ell}}\\\\\n",
    "&={W_{\\ell+1}}^\\top \\delta_{\\ell+1} \\odot f_\\ell^{\\prime}\\left(\\mathbf{a}_{\\ell}\\right)\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_\\ell}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_\\ell} \\frac{\\partial \\mathbf{z}_\\ell}{\\partial \\mathbf{a}_\\ell} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial W_\\ell}=\\delta_\\ell \\mathbf{z}_\\ell^\\top\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_\\ell}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_\\ell} \\frac{\\partial \\mathbf{z}_\\ell}{\\partial \\mathbf{a}_\\ell} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial \\mathbf{b}_\\ell}=\\delta_\\ell\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "が成り立つ．バッチ処理を考慮すると，行列を乗ずる順番が変わる．以下では$z=f(a), g(z)=f'(a)$として膜電位を使わず，発火率情報のみを使うようにしている．このようにできない関数もあるが，今回はこのように書き下せる活性化関数のみを扱う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward!(mlp::MLP)\n",
    "    (;L, W, b, z, δ, f) = mlp\n",
    "    n_batch = size(z[1])[1]\n",
    "    # backprop\n",
    "    for l in L:-1:1\n",
    "        if l < L\n",
    "            δ[l] = δ[l+1] * W[l+1].v' .* f[l].backward(z[l+1])\n",
    "        end\n",
    "        W[l].grad = z[l]' * δ[l] / n_batch\n",
    "        b[l].grad = sum(δ[l], dims=1) / n_batch\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "回帰問題において，代表的に用いられるのが平均二乗誤差 (mean squared error) である．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{y}} &= \\mathbf{z}_{L+1}\\\\\n",
    "\\mathcal{L}&:=\\frac{1}{2}\\left\\|\\hat{\\mathbf{y}}-\\mathbf{y}\\right\\|^{2}\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}=\\hat{\\mathbf{y}}-\\mathbf{y}\\\\\n",
    "\\delta_L&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_L}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}} \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_L}=\\left(\\hat{\\mathbf{y}}-\\mathbf{y}\\right) \\odot f_L^{\\prime}\\left(\\mathbf{a}_L\\right)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "squared_error! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function squared_error!(nn::NeuralNet, y::Array)\n",
    "    (;z, δ, f) = nn\n",
    "    ŷ = z[end]\n",
    "    n_batch = size(y)[1]\n",
    "    error = ŷ - y\n",
    "    loss = sum(error .^ 2) / n_batch\n",
    "    δ[end] = error .* f[end].backward(ŷ)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2クラス分類で用いられるのがバイナリ交差エントロピー (binary cross entropy) である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binary_crossentropy! (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clog(x) = max(log(x), -1e2) # clamped log\n",
    "\n",
    "function binary_crossentropy!(nn::NeuralNet, y::Array)\n",
    "    (;z, δ, f) = nn\n",
    "    @assert f[end] == Sigmoid # f[end] must be sigmoid function\n",
    "    ŷ = z[end]\n",
    "    n_batch = size(y)[1]\n",
    "    error = ŷ - y\n",
    "    loss = -sum(y .* clog.(ŷ) + (1 .- y) .* clog.(1 .- ŷ)) / n_batch\n",
    "    δ[end] = error\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多クラス分類課題で用いられるのが，softmaxおよびcross entropy lossである．\n",
    "softmax関数は $\\mathbf{y} = \\text{softmax}(\\mathbf{z})$ とすると，各成分を以下のように定義される．\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy_loss! (generic function with 2 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(x; dims=2)\n",
    "    expx = exp.(x .- maximum(x, dims=dims))\n",
    "    return expx ./ sum(expx, dims=dims)\n",
    "end\n",
    "\n",
    "Softmax = ActivationFunction(softmax, identity);\n",
    "\n",
    "# t: labels (1 dims)\n",
    "function cross_entropy_loss!(nn::NeuralNet, t::Vector)\n",
    "    (;z, δ, f) = nn\n",
    "    @assert f[end] == Softmax # f[end] must be softmax function\n",
    "    ŷ = z[end]\n",
    "    n_batch = length(t)\n",
    "    idx = CartesianIndex.([(i, t[i]) for i in 1:n_batch])\n",
    "    loss = -sum(clog.(ŷ[idx])) / n_batch\n",
    "    grad = copy(ŷ)\n",
    "    grad[idx] .-= 1\n",
    "    δ[end] = grad / n_batch\n",
    "    return loss\n",
    "end\n",
    "\n",
    "# t: probability (2 dims)\n",
    "function cross_entropy_loss!(nn::NeuralNet, t::Array)\n",
    "    (;z, δ, f) = nn\n",
    "    @assert f[end] == Softmax # f[end] must be softmax function\n",
    "    ŷ = z[end]\n",
    "    n_batch = size(t)[1]\n",
    "    loss = -sum(t .* clog.(ŷ)) / n_batch\n",
    "    δ[end] = (ŷ - t) / n_batch\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmaxのbackwardは基本的に使用しないため，適当に`identity`関数などを入れておく．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オプティマイザ\n",
    "abstract typeとして`Optimizer`タイプを作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Optimizer end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配降下法は$N$が大きい場合，あるいは1つのサンプルのデータサイズが大きい場合は非効率であるので，ニューラルネットワークの学習においては，データの部分集合であるミニバッチ (mini-bacth) を用いた **確率的勾配降下法** (stochastic gradient descent; SGD) が用いられる．\n",
    "**確率的勾配降下法(stochastic gradient descent; SGD)** を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimizer_update! (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD optimizer\n",
    "@kwdef struct SGD{FT} <: Optimizer\n",
    "    lr::FT=1e-2 # learning rate\n",
    "end\n",
    "\n",
    "function optimizer_update!(param::Param, optimizer::SGD; weight_decay=0)\n",
    "    (;lr) = optimizer\n",
    "    if weight_decay > 0\n",
    "        param.grad += weight_decay * param.v\n",
    "    end\n",
    "    param.v -= lr * param.grad\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に**Adam** {cite:p}`Kingma2014-fm` を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimizer_update! (generic function with 2 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam optimizer\n",
    "@kwdef mutable struct Adam <: Optimizer\n",
    "    lr=1e-4; β1=0.9; β2=0.999; ϵ=1e-8\n",
    "    ms=Dict(); vs=Dict();\n",
    "end\n",
    "\n",
    "# Adam optimizer\n",
    "function optimizer_update!(param::Param, optimizer::Adam; weight_decay=0)\n",
    "    (;lr, β1, β2, ϵ, ms, vs) = optimizer\n",
    "    key = objectid(param)\n",
    "    if !haskey(ms, key) \n",
    "        ms[key], vs[key] = zero(param.v), zero(param.v)\n",
    "    end\n",
    "    if weight_decay > 0\n",
    "        param.v -= lr * weight_decay * param.v\n",
    "    end\n",
    "    m, v = ms[key], vs[key]\n",
    "    m += (1 - β1) * (param.grad - m)\n",
    "    v += (1 - β2) * (param.grad .* param.grad - v)\n",
    "    param.v -= lr * m ./ (sqrt.(v) .+ ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みのL2正則化 (Weight decay) を加える．正則化があることにより，実際のニューロンの活動を人工神経回路で再現できる研究も複数ある．バイアス項にはweight decayをしないため，optimizerの構造体の外からweight decayの値を与えることとする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optim_step! (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function optim_step!(nn::NeuralNet, optimizer::Optimizer; weight_decay=0)\n",
    "    (;L, W, b) = nn\n",
    "    # update weights\n",
    "    for param in W\n",
    "        optimizer_update!(param, optimizer, weight_decay=weight_decay)\n",
    "    end\n",
    "\n",
    "    # update bias (without weight decay)\n",
    "    for param in b\n",
    "        optimizer_update!(param, optimizer, weight_decay=0)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_step! (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_step!(nn::NeuralNet, x::Array, y::Array, loss_fun::Function; optimizer::Optimizer=SGD(), weight_decay=0)\n",
    "    _ = nn(x)\n",
    "    loss = loss_fun(nn, y)\n",
    "    backward!(nn)\n",
    "    optim_step!(nn, optimizer, weight_decay=weight_decay) # update params\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset MNIST:\n",
       "  metadata  =>    Dict{String, Any} with 3 entries\n",
       "  split     =>    :test\n",
       "  features  =>    28×28×10000 Array{Float32, 3}\n",
       "  targets   =>    10000-element Vector{Int64}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = MNIST(:train)\n",
    "testset = MNIST(:test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初回はdownload時に[y/n]を聞かれる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(features = Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], targets = 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[1]  # return first observation as a NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = trainset[:] # return all observations\n",
    "y_train .+= 1; # 0-9 to 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training θeter\n",
    "n_traindata = 10000 #or length(trainset)\n",
    "n_batch = 100 # batch size\n",
    "n_iter_per_epoch = round(Int, n_traindata/n_batch)\n",
    "n_epoch = 100; # number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "nn1 = MLP([28^2, 128, n_classes], f_hid=Relu, f_out=Softmax)\n",
    "loss_fun1 = cross_entropy_loss!\n",
    "\n",
    "lr = 0.01  # learning rate\n",
    "weight_decay = 0 # weight decay (L2 norm) strength\n",
    "optimizer1 = Adam(lr=lr); # SGD(lr=lr)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mTraining... 100%|████████████████████████████████████████| Time: 0:00:31\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "error_arr = zeros(n_epoch); # memory array of each epoch error\n",
    "\n",
    "@showprogress \"Training...\" for e in 1:n_epoch\n",
    "    for iter in 1:n_iter_per_epoch\n",
    "        idx = (iter-1)*n_batch+1:iter*n_batch\n",
    "        x, y = Matrix(reshape(X_train[:, :, idx], (:, n_batch))'), y_train[idx]\n",
    "        loss = train_step!(nn1, x, y, loss_fun1, optimizer=optimizer1, weight_decay=weight_decay)\n",
    "        error_arr[e] += loss\n",
    "    end \n",
    "    error_arr[e] /= n_traindata\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 300x200 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(figsize=(3,2))\n",
    "semilogy(1:n_epoch, error_arr)\n",
    "ylabel(\"Train loss\"); xlabel(\"Epoch\"); xlim(0, n_epoch)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(features = Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], targets = [7, 2, 1, 0, 4, 1, 4, 9, 5, 9  …  7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = testset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test = length(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ParseError:\n\u001b[90m# Error @ \u001b[0;0m\u001b]8;;file://D:/github/compneuro-julia-management/contents/solve-credit-assignment-problem/In[81]#3:15\u001b\\\u001b[90mIn[81]:3:15\u001b[0;0m\u001b]8;;\u001b\\\n    subplots()\n    imshow(x')\u001b[48;2;120;70;70m\u001b[0;0m\n\u001b[90m#             └ ── \u001b[0;0m\u001b[91mExpected `end`\u001b[0;0m",
     "output_type": "error",
     "traceback": [
      "ParseError:\n\u001b[90m# Error @ \u001b[0;0m\u001b]8;;file://D:/github/compneuro-julia-management/contents/solve-credit-assignment-problem/In[81]#3:15\u001b\\\u001b[90mIn[81]:3:15\u001b[0;0m\u001b]8;;\u001b\\\n    subplots()\n    imshow(x')\u001b[48;2;120;70;70m\u001b[0;0m\n\u001b[90m#             └ ── \u001b[0;0m\u001b[91mExpected `end`\u001b[0;0m",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[81]:3"
     ]
    }
   ],
   "source": [
    "for i in 1:5\n",
    "    subplots()\n",
    "    imshow(x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = Matrix(reshape(X_test, (28^2, :))');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn1(x_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = getindex.(argmax(y_pred, dims=2), 2) .- 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.37"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(t_pred .== y_test) / n_test * 100"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
