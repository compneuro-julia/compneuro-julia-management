{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配法と誤差逆伝播法\n",
    "ニューラルネットワークにおいて，効率よく各重みの勾配を推定することで貢献度割り当て問題を解決する方法が**誤差逆伝播法** (backpropagation) である．本節では入力層，隠れ層，出力層からなる多層ニューラルネットワークを実装し，誤差逆伝播法による勾配推定を用いて学習を行う．\n",
    "\n",
    "本書では誤差逆伝播法を用いない学習法を実施することも考慮し，数式と対応するような実装を行う．そのため，Deep Learningライブラリ (PyTorch, Flux.jl等) のようにLayerを定義し，それを繋げてモデルを定義するということや計算グラフの構築は行わない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base: @kwdef\n",
    "using LinearAlgebra, Random, PyPlot, ProgressMeter, Statistics\n",
    "rc(\"axes.spines\", top=false, right=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数 (activation function) の構造体を`ActivationFunction` と定義する．この構造体には`forward` と `backward`の2種類の関数フィールド (field; 構造体の要素のこと) を持たせておく．2種類の関数はそれぞれ順伝播時と逆伝播時に使用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ActivationFunction\n",
    "    forward::Function   # function for forward propagation\n",
    "    backward::Function  # function for back-propagation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，次のコードにより，構造体のインスタンスを関数として使用できるようにしておく．これはcallable objectと呼ばれる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f::ActivationFunction)(x) = f.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代表的な活性化関数を紹介する．なお，`backward` における `y` は `forward` での出力に対応する．これは活性化関数を作用させる前の変数 ($x$であり，膜電位に対応する) を保持しておかなくても良いようにするためである．\n",
    "\n",
    "シグモイド関数 (sigmoid function) あるいはロジスティック関数 (logistic function) の場合，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textrm{forward: } y = \\frac{1}{1+e^{-x}}\\\\\n",
    "&\\textrm{backward: } \\frac{dy}{dx} =\\frac{e^{-x}}{(1+e^{-x})^2}= y\\cdot (1-y)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "tanh関数の場合，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textrm{forward: } y = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\\\\\n",
    "&\\textrm{backward: } \\frac{dy}{dx} =\\frac{(e^x+e^{-x})(e^x+e^{-x})-(e^x-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}= 1-y^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ReLU関数 (rectified linear unit function, 正規化線形関数) あるいはランプ関数 (ramp function)の場合，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textrm{forward: } y = \\max(x, 0)\\\\\n",
    "&\\textrm{backward: } \\frac{dy}{dx} = \\mathbf{1}_{x > 0}(x) = \\mathbf{1}_{y > 0}(y)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ただし，$\\max(a, b)$は, $a, b$のうち，大きい値を返す関数である．また，$\\mathbf{1}_{A}(x)$ は指示関数 (indicator function)であり，$x\\in A$ ならば $\\mathbf{1}_A(x)=1$ であり，それ以外の場合は $\\mathbf{1}_A(x)=0$ となる関数である．ReLU関数は $x=0$ で折れ曲がるが，その他では線形であるため，区分線形関数 (piecewise linear function) の一種であると言える．\n",
    "\n",
    "これらの活性化関数を構造体 `ActivationFunction`を用いて実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity (generic function with 1 method)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Sigmoid()\n",
    "    ActivationFunction(\n",
    "        x -> 1.0 ./ (1.0 .+ exp.(-x)),\n",
    "        y -> y .* (1 .- y)\n",
    "    )\n",
    "end\n",
    "\n",
    "function Tanh()\n",
    "    ActivationFunction(\n",
    "        x -> tanh.(x),\n",
    "        y -> 1 .- y .^ 2\n",
    "    )\n",
    "end\n",
    "\n",
    "function ReLU()\n",
    "    ActivationFunction(\n",
    "        x -> max.(0, x),\n",
    "        y -> y .> 0\n",
    "    )\n",
    "end\n",
    "\n",
    "function LeakyReLU(a=0.01)\n",
    "    ActivationFunction(\n",
    "        x -> max.(0, x) + a * min.(0, x),\n",
    "        y -> (y .> 0) + a * (y .<= 0)\n",
    "    )\n",
    "end   \n",
    "\n",
    "function Identity()\n",
    "    ActivationFunction(\n",
    "        x -> x, # or use `identity` function\n",
    "        y -> fill!(similar(y), 1)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークのパラメータを保持する構造体を定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNet end\n",
    "(f::NeuralNet)(x) = forward!(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワーク内の学習可能なパラメータ (learnable parameter) を保持する構造体を定義する．各パラメータはそれ自体の値 (value) と損失関数に対する勾配 (gradient) を持つ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Param\n",
    "    v::Array\n",
    "    grad::Array\n",
    "\n",
    "    Param(value) = new(value, zero(value))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MLP <: NeuralNet\n",
    "    L::Int # num. of layers\n",
    "    W::Vector{Param}; b::Vector{Param}   # weights and bias\n",
    "    z::Vector{Array}; δ::Vector{Array}   # state of forward/backward activity\n",
    "    f::Vector{ActivationFunction} # activation functions of layers\n",
    "\n",
    "    function MLP(n_units::Vector{Int}; f_hid::ActivationFunction=Sigmoid(), f_out::ActivationFunction=Identity())\n",
    "        L = length(n_units) - 1\n",
    "        # initialization of parameters\n",
    "        W = [Param(2 * (rand(n_units[l], n_units[l+1]) .- 0.5) * sqrt(6/n_units[l])) for l in 1:L] # He\n",
    "        #W = [Param(2 * (rand(n_units[l], n_units[l+1]) .- 0.5) * sqrt(6/(n_units[l]+n_units[l+1]))) for l in 1:L] # Xavier\n",
    "        b = [Param(zeros(1, n_units[l+1])) for l in 1:L]\n",
    "        \n",
    "        # initialization of forward / backward states\n",
    "        z, δ = Vector{Array}(undef, L+1), Vector{Array}(undef, L)\n",
    "        f = vcat([repeat([f_hid], L-1)..., f_out])\n",
    "        new(L, W, b, z, δ, f)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "struct `MLP`を用意し，**重みの初期化(weight initialization)** を行う同名の関数`MLP`を用意する．重みの初期化に関しては，各層の出力および勾配の分散が一定となるような初期化をすることで学習が進行することが知られている．出力は活性化関数に依存するため，初期化についても活性化関数に応じて変更することが推奨され，sigmoid関数やtanh関数を用いる場合はXavierの初期化 \\citep{Glorot2010-iu}，ReLU関数を用いる場合はHeの初期化 \\citep{He2015-fs} が用いられる．入力ユニット数を $n_{\\textrm{in}}$, 出力ユニット数を $n_{\\textrm{out}}$ とすると，Xavierの初期化では重み $w$ の平均が0, 分散が $\\frac{2}{n_{\\textrm{in}}+n_{\\textrm{out}}}$ となるように一様分布 $U\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right)$ や正規分布 $\\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right)$ 等から重みをサンプリングする．Heの初期化ではReLUを用いる場合，重み $w$ の平均が0, 分散が$\\frac{2}{n_{\\textrm{in}}}$ あるいは $\\frac{2}{n_{\\textrm{out}}}$ となるようにし，前者の分散を使用する場合は一様分布 $U\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}}}\\right)$ や正規分布 $\\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}}}\\right)$ 等から重みをサンプリングする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 順伝播 (forward propagation)\n",
    "$f(\\cdot)$を活性化関数とする．順伝播(feedforward propagation)は以下のようになる．$(\\ell=1,\\ldots,L)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{入力層 : }&\\mathbf{z}_1=\\mathbf{x}\\\\\n",
    "\\text{隠れ層 : }&\\mathbf{a}_\\ell=W_\\ell \\mathbf{z}_\\ell +\\mathbf{b}_\\ell\\\\\n",
    "&\\mathbf{z}_{\\ell+1}=f_\\ell\\left(\\mathbf{a}_\\ell\\right)\\\\\n",
    "\\text{出力層 : }&\\hat{\\mathbf{y}}=\\mathbf{z}_{L+1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装における取り回しの都合上，以下では$f:=f_\\ell\\ (1, \\ldots, L-1)$ とし，$g:=f_{L}$ は損失関数において記述されることとする．以下では`z[l]`は$\\mathbf{z}_\\ell \\ (1, \\ldots, L)$に対応するが，`z[L+1]`は$\\mathbf{z}_{L+1}$ではなく$\\mathbf{a}_{L}$に対応することに注意．\n",
    "\n",
    "とするか？混乱をきたしかねないし，余計な計算が増える．softmaxをどうするか．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward computation of MLP\n",
    "function forward!(mlp::MLP, x::Array)\n",
    "    (;L, f, W, b, z) = mlp\n",
    "    z[1] = x # input (n_batch x n_neurons)\n",
    "    for l in 1:L\n",
    "        z[l+1] = f[l](z[l] * W[l].v .+ b[l].v) # hidden layers\n",
    "    end\n",
    "    return z[L+1] # output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W[l].v` は`l`番目の`Param`のインスタンスにおける`v`を取り出す操作である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆伝播 (backward propagation)\n",
    "ニューラルネットワークの学習 (learning) あるいは訓練 (training) とは，目的関数 (objective function) あるいは損失関数 (loss function) と呼ばれる評価指標を可能な限り小さく (場合によっては大きく) するようなパラメータ集合 $\\Theta = \\{W_\\ell, b_\\ell\\}_{\\ell=1}^{L}$ を求める過程のことである．学習においてパラメータを最適化するアルゴリズムを**オプティマイザ** (optimizer) という．オプティマイザは多数提案されており，代表的なものを後ほど紹介する．まず，最も単純なオプティマイザである **勾配降下法** (gradient descent; GD) を紹介する．勾配降下法では全データを用いてパラメータ $\\theta \\in \\Theta$ の更新量 $\\Delta \\theta$ を \n",
    "\n",
    "$$\n",
    "\\Delta \\theta = -\\eta \\frac{\\partial \\mathcal{L}_{\\textrm{GD}}}{\\partial \\theta} = -\\frac{\\eta}{N} \\sum_{i=1}^N \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "として計算する (パラメータは$\\theta\\leftarrow \\theta + \\Delta \\theta$により更新される)．ただし，$\\mathcal{L}_{\\textrm{GD}}:=\\frac{1}{N}\\sum_{i=1}^N \\mathcal{L}^{(i)}$ であり，$\\mathcal{L}^{(i)}$は$i$ 番目のサンプルに対する目的関数であり，$N$ は全データのサンプル数を意味する．$\\eta$ は学習率 (learning rate) である．オプティマイザは一般的に勾配 $\\dfrac{\\partial \\mathcal{L}}{\\partial \\theta}$ の計算を必要とする．この計算を効率よく行う手法が**誤差逆伝播法** (backpropagation) である．誤差逆伝播法は連鎖律 (chain rule; 合成関数の微分の関係式) を用いて導くことができる．$\\mathbf{a}_\\ell=W_\\ell \\mathbf{z}_\\ell +\\mathbf{b}_\\ell$ および $\\mathbf{z}_{\\ell+1}=f_\\ell\\left(\\mathbf{a}_\\ell\\right)$ であることを踏まえると，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}\\\\\n",
    "\\delta_L&:=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_L}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}} \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_L}\\\\\n",
    "\\mathbf{\\delta}_\\ell&:=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell}}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{\\ell+1}} \\frac{\\partial \\mathbf{z}_{\\ell+1}}{\\partial \\mathbf{a}_\\ell}\\\\\n",
    "&=\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell+1}}\\frac{\\partial \\mathbf{a}_{\\ell+1}}{\\partial \\mathbf{z}_{\\ell+1}}\\right)\\frac{\\partial \\mathbf{z}_{\\ell+1}}{\\partial \\mathbf{a}_{\\ell}}\\\\\n",
    "&={W_{\\ell+1}}^\\top \\delta_{\\ell+1} \\odot f_\\ell^{\\prime}\\left(\\mathbf{a}_{\\ell}\\right)\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_\\ell}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_\\ell} \\frac{\\partial \\mathbf{z}_\\ell}{\\partial \\mathbf{a}_\\ell} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial W_\\ell}=\\delta_\\ell \\mathbf{z}_\\ell^\\top\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_\\ell}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_\\ell} \\frac{\\partial \\mathbf{z}_\\ell}{\\partial \\mathbf{a}_\\ell} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial \\mathbf{b}_\\ell}=\\delta_\\ell\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "が成り立つ．バッチ処理を考慮すると，行列を乗ずる順番が変わる．以下では$z=f(a), g(z)=f'(a)$として膜電位を使わず，発火率情報のみを使うようにしている．このようにできない関数もあるが，今回はこのように書き下せる活性化関数のみを扱う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward!(mlp::MLP)\n",
    "    (;L, W, b, z, δ, f) = mlp\n",
    "    n_batch = size(z[1])[1]\n",
    "    # backprop\n",
    "    for l in L:-1:1\n",
    "        if l < L\n",
    "            δ[l] = δ[l+1] * W[l+1].v' .* f[l].backward(z[l+1])\n",
    "        end\n",
    "        W[l].grad = z[l]' * δ[l] / n_batch\n",
    "        b[l].grad = sum(δ[l], dims=1) / n_batch\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "回帰問題において，代表的に用いられるのが平均二乗誤差 (mean squared error) である．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{y}} &= \\mathbf{z}_{L+1}\\\\\n",
    "\\mathcal{L}&:=\\frac{1}{2}\\left\\|\\hat{\\mathbf{y}}-\\mathbf{y}\\right\\|^{2}\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}=\\hat{\\mathbf{y}}-\\mathbf{y}\\\\\n",
    "\\delta_L&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_L}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}} \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_L}=\\left(\\hat{\\mathbf{y}}-\\mathbf{y}\\right) \\odot f_L^{\\prime}\\left(\\mathbf{a}_L\\right)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "squared_error! (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function squared_error!(nn::NeuralNet, y::Array)\n",
    "    (;z, δ, f) = nn\n",
    "    ŷ = z[end]\n",
    "    n_batch = size(y)[1]\n",
    "    error = ŷ - y\n",
    "    loss = sum(error .^ 2) / n_batch\n",
    "    δ[end] = error .* f[end].backward(ŷ)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2クラス分類で用いられるのがバイナリ交差エントロピー (binary cross entropy) である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binary_crossentropy! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clog(x) = max(log(x), -1e2) # clamped log\n",
    "\n",
    "function binary_crossentropy!(nn::NeuralNet, y::Array)\n",
    "    (;z, δ, f) = nn\n",
    "    @assert f[end] == Sigmoid # f[end] must be sigmoid function\n",
    "    ŷ = z[end]\n",
    "    n_batch = size(y)[1]\n",
    "    error = ŷ - y\n",
    "    loss = -sum(y .* clog.(ŷ) + (1 .- y) .* clog.(1 .- ŷ)) / n_batch\n",
    "    δ[end] = error\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多クラス分類課題で用いられるのが，softmaxおよびcross entropy lossである．\n",
    "softmax関数は $\\mathbf{y} = \\text{softmax}(\\mathbf{z})$ とすると，各成分を以下のように定義される．\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy_loss! (generic function with 2 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(x; dims=2)\n",
    "    expx = exp.(x .- maximum(x, dims=dims))\n",
    "    return expx ./ sum(expx, dims=dims)\n",
    "end\n",
    "\n",
    "Softmax = ActivationFunction(softmax, identity);\n",
    "\n",
    "# t: labels (1 dims)\n",
    "function cross_entropy_loss!(nn::NeuralNet, t::Vector)\n",
    "    (;z, δ, f) = nn\n",
    "    @assert f[end] == Softmax # f[end] must be softmax function\n",
    "    ŷ = z[end]\n",
    "    n_batch = length(t)\n",
    "    idx = CartesianIndex.([(i, t[i]) for i in 1:n_batch])\n",
    "    loss = -sum(clog.(ŷ[idx])) / n_batch\n",
    "    grad = copy(ŷ)\n",
    "    grad[idx] .-= 1\n",
    "    δ[end] = grad / n_batch\n",
    "    return loss\n",
    "end\n",
    "\n",
    "# t: probability (2 dims)\n",
    "function cross_entropy_loss!(nn::NeuralNet, t::Array)\n",
    "    (;z, δ, f) = nn\n",
    "    @assert f[end] == Softmax # f[end] must be softmax function\n",
    "    ŷ = z[end]\n",
    "    n_batch = size(t)[1]\n",
    "    loss = -sum(t .* clog.(ŷ)) / n_batch\n",
    "    δ[end] = (ŷ - t) / n_batch\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmaxのbackwardは基本的に使用しないため，適当に`identity`関数などを入れておく．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オプティマイザ\n",
    "abstract typeとして`Optimizer`タイプを作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Optimizer end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配降下法は$N$が大きい場合，あるいは1つのサンプルのデータサイズが大きい場合は非効率であるので，ニューラルネットワークの学習においては，データの部分集合であるミニバッチ (mini-bacth) を用いた **確率的勾配降下法** (stochastic gradient descent; SGD) が用いられる．\n",
    "**確率的勾配降下法(stochastic gradient descent; SGD)** を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimizer_update! (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD optimizer\n",
    "@kwdef struct SGD{FT} <: Optimizer\n",
    "    lr::FT=1e-2 # learning rate\n",
    "end\n",
    "\n",
    "function optimizer_update!(param::Param, optimizer::SGD; weight_decay=0)\n",
    "    (;lr) = optimizer\n",
    "    if weight_decay > 0\n",
    "        param.grad += weight_decay * param.v\n",
    "    end\n",
    "    param.v -= lr * param.grad\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に**Adam** {cite:p}`Kingma2014-fm` を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimizer_update! (generic function with 2 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam optimizer\n",
    "@kwdef mutable struct Adam <: Optimizer\n",
    "    lr=1e-4; β1=0.9; β2=0.999; ϵ=1e-8\n",
    "    ms=Dict(); vs=Dict();\n",
    "end\n",
    "\n",
    "# Adam optimizer\n",
    "function optimizer_update!(param::Param, optimizer::Adam; weight_decay=0)\n",
    "    (;lr, β1, β2, ϵ, ms, vs) = optimizer\n",
    "    key = objectid(param)\n",
    "    if !haskey(ms, key) \n",
    "        ms[key], vs[key] = zero(param.v), zero(param.v)\n",
    "    end\n",
    "    if weight_decay > 0\n",
    "        param.v -= lr * weight_decay * param.v\n",
    "    end\n",
    "    m, v = ms[key], vs[key]\n",
    "    m += (1 - β1) * (param.grad - m)\n",
    "    v += (1 - β2) * (param.grad .* param.grad - v)\n",
    "    param.v -= lr * m ./ (sqrt.(v) .+ ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みのL2正則化 (Weight decay) を加える．正則化があることにより，実際のニューロンの活動を人工神経回路で再現できる研究も複数ある．バイアス項にはweight decayをしないため，optimizerの構造体の外からweight decayの値を与えることとする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optim_step! (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function optim_step!(nn::NeuralNet, optimizer::Optimizer; weight_decay=0)\n",
    "    (;L, W, b) = nn\n",
    "    # update weights\n",
    "    for param in W\n",
    "        optimizer_update!(param, optimizer, weight_decay=weight_decay)\n",
    "    end\n",
    "\n",
    "    # update bias (without weight decay)\n",
    "    for param in b\n",
    "        optimizer_update!(param, optimizer, weight_decay=0)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_step! (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_step!(nn::NeuralNet, x::Array, y::Array, loss_fun::Function; optimizer::Optimizer=SGD(), weight_decay=0)\n",
    "    _ = nn(x)\n",
    "    loss = loss_fun(nn, y)\n",
    "    backward!(nn)\n",
    "    optim_step!(nn, optimizer, weight_decay=weight_decay) # update params\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "`MNIST` の代わりに`FashionMNIST` を用いることもできる．MNISTは易しい課題であるため，MNISTを訓練できるからと言って複雑な課題でも機能する保証はない．とは言え，基本的なデータセットであるため，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset = MNIST(:train)\n",
    "testset = MNIST(:test)\n",
    "\n",
    "X_train, y_train = trainset[:] # return all observations\n",
    "y_train .+= 1; # 0-9 to 1-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初回実行時はデータセットのダウンロードを行うか`[y/n]` (yes/no) の入力を求められるので`y`と入力する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_mean = mean(X_train)\n",
    "#X_std = std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_normalized = (reshape(X_train, (:, length(trainset)))' .- X_mean) ./ X_std;\n",
    "X_train_normalized = Matrix(reshape(X_train, (:, length(trainset)))') #.- X_mean) ./ X_std;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training θeter\n",
    "n_traindata = 60000 #or length(trainset)\n",
    "n_batch = 200 # batch size\n",
    "n_iter_per_epoch = round(Int, n_traindata/n_batch)\n",
    "n_epoch = 100; # number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 10\n",
    "#n_hid = 128#36\n",
    "#nn1 = MLP([28^2, 100, n_classes], f_hid=LeakyReLU(), f_out=Softmax)\n",
    "nn1 = MLP([28^2, 100, 50, n_classes], f_hid=ReLU(), f_out=Softmax)\n",
    "loss_fun1 = cross_entropy_loss!\n",
    "\n",
    "lr = 1e-4  # learning rate\n",
    "weight_decay = 1e-4 # weight decay (L2 norm) strength\n",
    "optimizer1 = Adam(lr=lr); #SGD(lr=lr)# \n",
    "shuffle_data = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/100] train loss: 0.003438110223173517\n",
      "[002/100] train loss: 0.0013609936281982862\n",
      "[003/100] train loss: 0.001077225876930464\n",
      "[004/100] train loss: 0.0009061483246731504\n",
      "[005/100] train loss: 0.0007894606879116187\n",
      "[006/100] train loss: 0.0007032581270467751\n",
      "[007/100] train loss: 0.000634654533352238\n",
      "[008/100] train loss: 0.0005788150172848021\n",
      "[009/100] train loss: 0.0005327677748701048\n",
      "[010/100] train loss: 0.0004942835348048979\n",
      "[011/100] train loss: 0.0004622961624474385\n",
      "[012/100] train loss: 0.00043468685364104413\n",
      "[013/100] train loss: 0.0004099682670827161\n",
      "[014/100] train loss: 0.0003870731999475121\n",
      "[015/100] train loss: 0.00036769376019761335\n",
      "[016/100] train loss: 0.0003503262088194202\n",
      "[017/100] train loss: 0.0003370220124736802\n",
      "[018/100] train loss: 0.0003217413419335054\n",
      "[019/100] train loss: 0.00031024122788375993\n",
      "[020/100] train loss: 0.0002980338028579896\n",
      "[021/100] train loss: 0.00028901197289505774\n",
      "[022/100] train loss: 0.00027701144208815826\n",
      "[023/100] train loss: 0.0002698976674738769\n",
      "[024/100] train loss: 0.0002615214538704848\n",
      "[025/100] train loss: 0.0002512505178754944\n",
      "[026/100] train loss: 0.00024673115920395675\n",
      "[027/100] train loss: 0.00023688740176950536\n",
      "[028/100] train loss: 0.00023292425435256456\n",
      "[029/100] train loss: 0.00022407136402146545\n",
      "[030/100] train loss: 0.00022178888652356723\n",
      "[031/100] train loss: 0.0002156160851668178\n",
      "[032/100] train loss: 0.00020735543254327563\n",
      "[033/100] train loss: 0.0002014781206516255\n",
      "[034/100] train loss: 0.00019604933087916796\n",
      "[035/100] train loss: 0.0001907665418257627\n",
      "[036/100] train loss: 0.0001871607182553172\n",
      "[037/100] train loss: 0.00018526469424400493\n",
      "[038/100] train loss: 0.00017892901822447416\n",
      "[039/100] train loss: 0.0001763707218551509\n",
      "[040/100] train loss: 0.00017101427375957261\n",
      "[041/100] train loss: 0.00016770806145498704\n",
      "[042/100] train loss: 0.00016539326795797217\n",
      "[043/100] train loss: 0.00016277014108932706\n",
      "[044/100] train loss: 0.00015768559256696348\n",
      "[045/100] train loss: 0.00015759167218562357\n",
      "[046/100] train loss: 0.00015333206344407573\n",
      "[047/100] train loss: 0.0001506333218868484\n",
      "[048/100] train loss: 0.0001461985980002646\n",
      "[049/100] train loss: 0.0001461877840656234\n",
      "[050/100] train loss: 0.0001406344594743371\n",
      "[051/100] train loss: 0.00013933263296098832\n",
      "[052/100] train loss: 0.00013546724013916383\n",
      "[053/100] train loss: 0.00013457870627249806\n",
      "[054/100] train loss: 0.00013074651787983404\n",
      "[055/100] train loss: 0.0001288734898446922\n",
      "[056/100] train loss: 0.0001283469371151169\n",
      "[057/100] train loss: 0.00012550839207011574\n",
      "[058/100] train loss: 0.00012398452196832075\n",
      "[059/100] train loss: 0.00012231465546631504\n",
      "[060/100] train loss: 0.00011683055545177559\n",
      "[061/100] train loss: 0.0001155813535937912\n",
      "[062/100] train loss: 0.00011807193933212551\n",
      "[063/100] train loss: 0.00011318276852277113\n",
      "[064/100] train loss: 0.000112275999503234\n",
      "[065/100] train loss: 0.00010535282585817645\n",
      "[066/100] train loss: 0.00010666989808533566\n",
      "[067/100] train loss: 0.0001041740420648388\n",
      "[068/100] train loss: 0.00010021143863112368\n",
      "[069/100] train loss: 0.00010274944782272561\n",
      "[070/100] train loss: 0.00010070928679069146\n",
      "[071/100] train loss: 9.636080300487201e-5\n",
      "[072/100] train loss: 9.819472854154391e-5\n",
      "[073/100] train loss: 9.455403743837295e-5\n",
      "[074/100] train loss: 9.368376709326927e-5\n",
      "[075/100] train loss: 8.951119221404912e-5\n",
      "[076/100] train loss: 9.262714035587073e-5\n",
      "[077/100] train loss: 9.070814149882389e-5\n",
      "[078/100] train loss: 8.769642302894622e-5\n",
      "[079/100] train loss: 8.531852773881272e-5\n",
      "[080/100] train loss: 8.513557462711934e-5\n",
      "[081/100] train loss: 8.128564206748397e-5\n",
      "[082/100] train loss: 8.104156585821548e-5\n",
      "[083/100] train loss: 8.02177089309678e-5\n",
      "[084/100] train loss: 7.737909386469312e-5\n",
      "[085/100] train loss: 7.720659127274172e-5\n",
      "[086/100] train loss: 7.890566849587678e-5\n",
      "[087/100] train loss: 7.37849173292768e-5\n",
      "[088/100] train loss: 7.421339716535846e-5\n",
      "[089/100] train loss: 7.227594346385364e-5\n",
      "[090/100] train loss: 6.967233575425203e-5\n",
      "[091/100] train loss: 7.12634116741835e-5\n",
      "[092/100] train loss: 7.034609499415837e-5\n",
      "[093/100] train loss: 6.614479783703832e-5\n",
      "[094/100] train loss: 6.713932581486017e-5\n",
      "[095/100] train loss: 6.801648867820648e-5\n",
      "[096/100] train loss: 6.710766534596132e-5\n",
      "[097/100] train loss: 6.524348479952748e-5\n",
      "[098/100] train loss: 5.930409299823498e-5\n",
      "[099/100] train loss: 6.281641528548523e-5\n",
      "[100/100] train loss: 5.895601528410828e-5\n"
     ]
    }
   ],
   "source": [
    "error_arr = zeros(n_epoch); # memory array of each epoch error\n",
    "\n",
    "#@showprogress \"Training...\" \n",
    "for e in 1:n_epoch\n",
    "    if shuffle_data\n",
    "        shuffle_indices = shuffle(1:n_traindata)\n",
    "        X_train_normalized = X_train_normalized[shuffle_indices, :];\n",
    "        y_train = y_train[shuffle_indices]\n",
    "    end\n",
    "    for iter in 1:n_iter_per_epoch\n",
    "        idx = (iter-1)*n_batch+1:iter*n_batch\n",
    "        x, y = X_train_normalized[idx, :], y_train[idx]\n",
    "        loss = train_step!(nn1, x, y, loss_fun1, optimizer=optimizer1, weight_decay=weight_decay)\n",
    "        error_arr[e] += loss\n",
    "    end \n",
    "    error_arr[e] /= n_traindata\n",
    "    println(\"[$(lpad(e, ndigits(n_epoch), '0'))/$(n_epoch)] train loss: $(error_arr[e])\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 300x200 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(figsize=(3,2))\n",
    "semilogy(1:n_epoch, error_arr)\n",
    "ylabel(\"Train loss\"); xlabel(\"Epoch\"); xlim(0, n_epoch)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = testset[:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test = length(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(nn1.W[1].v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dying Relu現象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "weight = reshape(nn1.W[1].v', (64, 28, 28));\n",
    "\n",
    "fig, axes = subplots(6, 6, figsize=(6,6))\n",
    "axf = axes[:]\n",
    "for i in 1:n_hid\n",
    "    axf[i].imshow(weight[i, :, :], cmap=\"Greys\")\n",
    "    axf[i].axis(\"off\")\n",
    "end\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in 1:5\n",
    "    subplots()\n",
    "    imshow(x')\n",
    "end\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = Matrix(reshape(((X_test .- X_mean) ./ X_std), (28^2, :))');\n",
    "x_test = Matrix(reshape(X_test, (28^2, :))');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn1(x_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = getindex.(argmax(y_pred, dims=2), 2) .- 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.41"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(t_pred .== y_test) / n_test * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = zeros(n_classes, n_classes)\n",
    "for i in 1:n_test\n",
    "    confusion_matrix[y_test[i]+1, t_pred[i]+1] += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 300x300 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(figsize=(3,3))\n",
    "imshow(confusion_matrix)\n",
    "xlabel(\"Predicted labels\")\n",
    "ylabel(\"True labels\")\n",
    "tight_layout()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
