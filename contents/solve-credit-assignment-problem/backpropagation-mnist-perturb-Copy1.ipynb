{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例3. MNIST\n",
    "\n",
    "`MNIST` の代わりに`FashionMNIST` を用いることもできる．MNISTは易しい課題であるため，MNISTを訓練できるからと言って複雑な課題でも機能する保証はない．とは言え，基本的なデータセットであるため，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base: @kwdef\n",
    "using LinearAlgebra, Random, PyPlot, ProgressMeter, Statistics\n",
    "using MLDatasets\n",
    "include(\"../codes/neural_networks.jl\")\n",
    "rc(\"axes.spines\", top=false, right=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (28, 28, 60000), y_train: (60000,)\n"
     ]
    }
   ],
   "source": [
    "trainset = MNIST(:train)\n",
    "X_train, y_train = trainset[:]; # return all observations\n",
    "println(\"X_train: $(size(X_train)), y_train: $(size( y_train))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初回実行時はデータセットのダウンロードを行うか`[y/n]` (yes/no) の入力を求められるので`y`と入力する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像の空間次元はflattenする．また，labelは0-9の範囲から1-10の範囲に変更する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward_perturb! (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward computation of MLP\n",
    "function forward_perturb!(mlp::MLP, x::Array; mode=\"weight\", σ=1e-2)\n",
    "    (;L, f, W, b) = mlp\n",
    "    n_batch = size(x)[1]\n",
    "    z = Vector{Array}(undef, L+1) # define z (perturbed)\n",
    "    z[1] = x # input (n_batch x n_neurons)\n",
    "    # perturbation noise\n",
    "    if mode == \"weight\"\n",
    "        V = [σ * randn(n_batch, size(W[l].v)...) for l in 1:L]\n",
    "    end\n",
    "    v = [σ * randn(n_batch, size(b[l].v)[2]) for l in 1:L]\n",
    "    \n",
    "    for l in 1:L\n",
    "        if mode == \"weight\"\n",
    "            z[l+1] = f[l](vcat([z[l][i, :]' * (W[l].v + V[l][i, :, :]) for i in 1:n_batch]...) .+ b[l].v + v[l])\n",
    "        elseif mode == \"node\"\n",
    "            z[l+1] = f[l](z[l] * W[l].v .+ b[l].v + v[l])\n",
    "        end\n",
    "    end\n",
    "    if mode == \"weight\"\n",
    "        return z[L+1], V, v, σ\n",
    "    elseif mode == \"node\"\n",
    "        return z[L+1], v, σ\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t: labels (1 dims)\n",
    "function cross_entropy_loss(ŷ::Array, t::Vector)\n",
    "    n_batch = length(t)\n",
    "    idx = CartesianIndex.([(i, t[i]) for i in 1:n_batch])\n",
    "    loss = -clog.(ŷ[idx])\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "param_grad! (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight\n",
    "function param_grad!(mlp::MLP, δL, V, v)\n",
    "    (;L, W, b, z, δ, f) = mlp\n",
    "    n_batch = size(z[1])[1]\n",
    "    # backprop\n",
    "    for l in L:-1:1\n",
    "        W[l].grad = dropdims(sum(δL .* V[l], dims=1), dims=1)\n",
    "        b[l].grad = sum(δL .* v[l], dims=1) / n_batch\n",
    "    end\n",
    "end\n",
    "\n",
    "# node\n",
    "function param_grad!(mlp::MLP, δL, v)\n",
    "    (;L, W, b, z, δ, f) = mlp\n",
    "    n_batch = size(z[1])[1]\n",
    "    # backprop\n",
    "    for l in L:-1:1\n",
    "        W[l].grad = (δL .* z[l])' * v[l] / n_batch\n",
    "        b[l].grad = sum(δL .* v[l], dims=1) / n_batch\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_step! (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_step!(nn::NeuralNet, x::Array, y::Array, loss_fun::Function; optimizer::Optimizer=SGD(), weight_decay=0, mode=\"weight\")\n",
    "    n_batch = size(x)[1]\n",
    "    ŷ = nn(x)\n",
    "    if mode == \"weight\"\n",
    "        ŷ_p, V, v, σ = forward_perturb!(nn, x, mode=\"weight\")\n",
    "    elseif mode == \"node\"\n",
    "        ŷ_p, v, σ = forward_perturb!(nn, x, mode=\"node\")\n",
    "    end\n",
    "    loss = loss_fun(ŷ, y)\n",
    "    loss_p = loss_fun(ŷ_p, y)\n",
    "    δL = (loss_p - loss) / σ\n",
    "    if mode == \"weight\"\n",
    "        param_grad!(nn, δL, V, v)\n",
    "    elseif mode == \"node\"\n",
    "        param_grad!(nn, δL, v)\n",
    "    end\n",
    "    optim_step!(nn, optimizer, weight_decay=weight_decay) # update params\n",
    "    return sum(loss) / n_batch\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "n_traindata = length(trainset)\n",
    "n_batch = 200 # batch size\n",
    "n_iter_per_epoch = round(Int, n_traindata/n_batch)\n",
    "n_epoch = 10; # number of epoch\n",
    "\n",
    "X_train = Matrix(reshape(X_train[:, :, 1:n_traindata], (:, n_traindata))') \n",
    "y_train = y_train[1:n_traindata] .+ 1; # 0-9 to 1-10\n",
    "\n",
    "n_classes = 10\n",
    "model = MLP([28^2, 128, 64, n_classes], [ReLU(), ReLU(), Softmax()]; init_type=\"He\")\n",
    "loss_fn = cross_entropy_loss\n",
    "\n",
    "lr = 1e-3  # learning rate\n",
    "weight_decay = 1e-4 # weight decay (L2 norm) strength\n",
    "optimizer = Adam(lr=lr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを定義する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練の途中で学習率を10分の1に減少させる．これを学習率スケジュール (learning rate schedule) という．機械学習においては学習率の変更には線形で減少させる方法，指数関数的に減少させる方法，学習率を振動させる方法 (cosine annealing schedule) などがある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.454959452970205"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 1\n",
    "\n",
    "# shuffling data\n",
    "shuffle_indices = shuffle(1:n_traindata)\n",
    "X_train = X_train[shuffle_indices, :];\n",
    "y_train = y_train[shuffle_indices]\n",
    "\n",
    "# update\n",
    "iter = 1\n",
    "idx = (iter-1)*n_batch+1:iter*n_batch\n",
    "x, y = X_train[idx, :], y_train[idx]\n",
    "loss = train_step!(model, x, y, loss_fn, optimizer=optimizer, weight_decay=weight_decay, mode=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss = zeros(n_epoch); #training loss of each epoch\n",
    "for epoch in 1:n_epoch\n",
    "    # learning rate schedule\n",
    "    if epoch == round(Int, n_epoch/2)\n",
    "        optimizer.lr *= 0.1\n",
    "    end\n",
    "\n",
    "    # shuffling data\n",
    "    shuffle_indices = shuffle(1:n_traindata)\n",
    "    X_train = X_train[shuffle_indices, :];\n",
    "    y_train = y_train[shuffle_indices]\n",
    "\n",
    "    # update\n",
    "    for iter in 1:n_iter_per_epoch\n",
    "        idx = (iter-1)*n_batch+1:iter*n_batch\n",
    "        x, y = X_train[idx, :], y_train[idx]\n",
    "        loss = train_step!(model, x, y, loss_fn, optimizer=optimizer, weight_decay=weight_decay, mode=\"weight\")\n",
    "        train_loss[epoch] += loss\n",
    "    end\n",
    "    train_loss[epoch] /= n_iter_per_epoch\n",
    "    println(\"[$(lpad(epoch, ndigits(n_epoch), '0'))/$(n_epoch)] train loss: $(train_loss[epoch])\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,2))\n",
    "semilogy(1:n_epoch, train_loss)\n",
    "ylabel(\"Train loss\"); xlabel(\"Epoch\"); xlim(1, n_epoch)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習に使用しなかったデータを用いて，モデルの性能を確認しよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = MNIST(:test)\n",
    "X_test, y_test = testset[:]\n",
    "n_test = length(testset)\n",
    "\n",
    "x_test = Matrix(reshape(X_test, (28^2, :))')\n",
    "y_pred = model(x_test) # prediction (label probabilities)\n",
    "t_pred = getindex.(argmax(y_pred, dims=2), 2) .- 1 # prediction of label\n",
    "accuracy = sum(t_pred .== y_test) / n_test * 100\n",
    "println(\"Test accuracy: $(accuracy)%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータでの正答率は概ね98%となった．正答率を上げるには，モデルの中間層のユニットを増やす，Epoch数を増やす，データ拡張 (Data augmentation) を行う，等を行えば可能である．次に混同行列 (Confusion Matrix) を確認する．混同行列はサンプルごとの予測ラベルと正解ラベルを集計した2次元ヒストグラムであり，あるラベルのサンプルを別のラベルであると予測（混同）していないかを確認するものである．モデルの予測性能が高い場合は対角線上の値が大きくなる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = zeros(n_classes, n_classes)\n",
    "for i in 1:n_test\n",
    "    confusion_matrix[y_test[i]+1, t_pred[i]+1] += 1\n",
    "end\n",
    "\n",
    "figure(figsize=(2,2))\n",
    "imshow(confusion_matrix)\n",
    "xlabel(\"Predicted labels\")\n",
    "ylabel(\"True labels\")\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に初期層の最初の36個の重みを確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = reshape(model.W[1].v'[1:36, :], (36, 28, 28));\n",
    "\n",
    "fig, axes = subplots(6, 6, figsize=(4,4))\n",
    "axf = axes[:]\n",
    "for i in 1:36\n",
    "    axf[i].imshow(weights[i, :, :], cmap=\"gray\")\n",
    "    axf[i].axis(\"off\")\n",
    "end\n",
    "subplots_adjust(hspace=0.1, wspace=0.1)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
