{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配法と誤差逆伝播法\n",
    "ニューラルネットワークにおいて，効率よく各重みの勾配を推定することで貢献度割り当て問題を解決する方法が**誤差逆伝播法** (backpropagation) である．本節では入力層，隠れ層，出力層からなる多層ニューラルネットワークを実装し，誤差逆伝播法による勾配推定を用いて学習を行う．\n",
    "\n",
    "本書では誤差逆伝播法を用いない学習法を実施することも考慮し，数式と対応するような実装を行う．そのため，Deep Learningライブラリ (PyTorch, Flux.jl等) のようにLayerを定義し，それを繋げてモデルを定義するということや計算グラフの構築は行わない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base: @kwdef\n",
    "#using Parameters: @unpack # or using UnPack\n",
    "using LinearAlgebra, Random, Statistics, PyPlot, ProgressMeter\n",
    "#using BenchmarkTools; @btime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数 (activation function) の構造体を`ActivationFunction` と定義する．この構造体には`forward` と `backward`の2種類の関数フィールド (field; 構造体の要素のこと) を持たせておく．2種類の関数はそれぞれ順伝播時と逆伝播時に使用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ActivationFunction\n",
    "    forward::Function   # function for forward propagation\n",
    "    backward::Function  # function for back-propagation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，構造体のインスタンスを関数として使用できるようにしておく (Pythonのclassにおける `__call__` 関数のような働きをする)．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f::ActivationFunction)(x) = f.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代表的な活性化関数を実装する．`backward` における `y` は `forward` での出力に対応する．これは記録しておく変数を少なくするためである．\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{d}{dx} \\text{Sigmoid}(x) = \\text{Sigmoid}(x) \\cdot \\left(1 - \\text{Sigmoid}(x)\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "であることに注意．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = ActivationFunction(\n",
    "    x -> 1.0 ./ (1.0 .+ exp.(-x)),\n",
    "    y -> y .* (1 .- y)\n",
    ")\n",
    "\n",
    "tan_h = ActivationFunction(\n",
    "    x -> tanh.(x),\n",
    "    y -> 1 .- y .^ 2\n",
    ")\n",
    "\n",
    "relu = ActivationFunction(\n",
    "    x -> max.(0, x),\n",
    "    y -> y .> 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの構造を定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNet end\n",
    "(f::NeuralNet)(x) = forward!(f, x)\n",
    "\n",
    "@kwdef struct MLP <: NeuralNet\n",
    "    L::Int # num. of layers\n",
    "    W::Vector{Array}; b::Vector{Array};     # weights and bias\n",
    "    ∇W::Vector{Array}; ∇b::Vector{Array}; # gradient of params\n",
    "    z::Vector{Array}; δ::Vector{Array};   # state of forward/backward activity\n",
    "    f::ActivationFunction # activation functions of hidden layers\n",
    "\n",
    "    function MLP(n_units::Vector{Int}; activation=sigmoid)\n",
    "        L = length(n_units) - 1\n",
    "        # initialization of parameters\n",
    "        W = [2 * (rand(n_units[l], n_units[l+1]) .- 0.5) / sqrt(n_units[l]) for l in 1:L]\n",
    "        b = [zeros(1, n_units[l+1]) for l in 1:L]\n",
    "        \n",
    "        # initialization of gradients\n",
    "        ∇W, ∇b = [[zero(param[l]) for l in 1:L] for param in [W, b]]\n",
    "        \n",
    "        # initialization of forward / backward states\n",
    "        z, δ = Vector{Array}(undef, L+1), Vector{Array}(undef, L+1)\n",
    "        new(L, W, b, ∇W, ∇b, z, δ, activation)\n",
    "    end;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mutable struct `MLP`を用意し，**重みの初期化(weight initialization)** を行う同名の関数`MLP`を用意する．重みの初期化の手法は複数あるが，ここでは重みを$W$として，$W_{ij} \\sim U\\left(-1/\\sqrt{n}, 1/\\sqrt{n}\\right)$とする (Xavier initialization) \\citep{Glorot2010-iu}．ただし，$n$は入力ユニット数である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 順伝播 (forward propagation)\n",
    "$f(\\cdot)$を活性化関数とする．順伝播(feedforward propagation)は以下のようになる．$(\\ell=1,\\ldots,L)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{入力層 : }&\\mathbf{z}_1=\\mathbf{x}\\\\\n",
    "\\text{隠れ層 : }&\\mathbf{a}_\\ell=W_\\ell \\mathbf{z}_\\ell +\\mathbf{b}_\\ell\\\\\n",
    "&\\mathbf{z}_{\\ell+1}=f_\\ell\\left(\\mathbf{a}_\\ell\\right)\\\\\n",
    "\\text{出力層 : }&\\hat{\\mathbf{y}}=\\mathbf{z}_{L+1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward computation of MLP\n",
    "function forward!(mlp::MLP, x::Array)\n",
    "    (;L, f, W, b, z) = mlp\n",
    "    z[1] = x # input (n_batch x n_neurons)\n",
    "    for l in 1:L-1\n",
    "        z[l+1] = f(z[l] * W[l] .+ b[l]) # hidden layers\n",
    "    end\n",
    "    z[L+1] = z[L] * W[L] .+ b[L]\n",
    "    return z[L+1] # output\n",
    "end\n",
    "\n",
    "function backward!(mlp::MLP)\n",
    "    (;L, W, ∇W, ∇b, z, δ, f) = mlp\n",
    "    n_batch = size(z[1])[1]\n",
    "    # backprop\n",
    "    for l in L:-1:1\n",
    "        δ[l] = δ[l+1] * W[l]' .* f.backward(z[l])\n",
    "        ∇W[l] = z[l]' * δ[l] / n_batch\n",
    "        ∇b[l] = sum(δ[l], dims=1) / n_batch\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softmax(x; dims=2)\n",
    "    expx = exp.(x .- maximum(x, dims=dims))\n",
    "    return expx ./ sum(expx, dims=dims)\n",
    "end\n",
    "\n",
    "# t: labels (1 dims)\n",
    "function cross_entropy_loss!(nn::NeuralNet, t::Vector)\n",
    "    (;z, δ) = nn\n",
    "    ŷ = softmax(z[end])\n",
    "    n_batch = length(t)\n",
    "    idx = CartesianIndex.([(i, t[i]) for i in 1:n_batch])\n",
    "    loss = -sum(log.(ŷ[idx])) / n_batch\n",
    "    grad = copy(ŷ)\n",
    "    grad[idx] .-= 1\n",
    "    δ[end] = grad / n_batch\n",
    "    return loss, ŷ\n",
    "end\n",
    "\n",
    "# t: probability (2 dims)\n",
    "function cross_entropy_loss!(nn::NeuralNet, t::Array)\n",
    "    (;z, δ) = nn\n",
    "    ŷ = softmax(z[end])\n",
    "    n_batch = length(t)\n",
    "    loss = -sum(t .* log.(ŷ)) / n_batch\n",
    "    δ[end] = (ŷ - t) / n_batch\n",
    "    return loss, ŷ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(2, Array[[-0.05377522344968438 -0.1697085319757896 … -0.3063756919328347 -0.22680540679654446; -0.08425906210757184 0.011362400892143989 … 0.31130591756197895 -0.25057002173112697; … ; 0.05053397735112597 -0.17995373030293096 … -0.16793357318455113 0.16653369487534697; -0.11382979602942686 -0.03311546446280612 … 0.11869330332694222 -0.10789770170139958], [0.10144599572692346 -0.03876392278718581 … 0.10852910446203604 0.14893736872093857; -0.055784104023506406 -0.04781011849389655 … -0.13659030234619643 0.17340341035327156; … ; 0.1634880626136854 -0.16316363864515174 … 0.12878625584154144 -0.11655579396864492; 0.2020065631302359 -0.06678648360439421 … -0.016639918342358746 0.08581261109778514]], Array[[0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0]], Array[[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]], Array[[0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0]], Array[#undef, #undef, #undef], Array[#undef, #undef, #undef], ActivationFunction(var\"#5#7\"(), var\"#6#8\"()))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP([10, 20, 5], activation=tan_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(3, 10)\n",
    "z = mlp(x)\n",
    "t = [1, 3, 4]\n",
    "loss, y = cross_entropy_loss!(mlp, t)\n",
    "backward!(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆伝播 (backward propagation)\n",
    "\n",
    "$i$ 番目のサンプルに対する目的関数 (objective function) あるいは損失関数 (loss function)を $\\mathcal{L}^{(i)}$とする．例えば2乗誤差の場合は\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(i)}=\\frac{1}{2}\\left\\|\\hat{\\mathbf{y}}^{(i)}-\\mathbf{y}^{(i)}\\right\\|^{2}\\\\\n",
    "$$\n",
    "\n",
    "となる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの学習とは，目的関数 (objective function) あるいは損失関数 (loss function) と呼ばれる評価指標を可能な限り小さく (場合によっては大きく) するようなパラメータ集合 $\\Theta = \\{W_\\ell, b_\\ell\\}_{\\ell=1}^{L}$ を求める過程のことである．学習に用いられる，パラメータを探索する最適化アルゴリズムを **optimizer** という．最も単純なoptimizerである **勾配降下法** (gradient descent; GD) では全データを用いてパラメータ $\\theta \\in \\Theta$ の更新量 $\\Delta \\theta$ を \n",
    "\n",
    "$$\n",
    "\\Delta \\theta = -\\eta \\frac{\\partial \\mathcal{L}_{\\textrm{GD}}}{\\partial \\theta} = -\\eta \\sum_{i=1}^N \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "として計算する．ただし，$\\mathcal{L}_{\\textrm{GD}}=\\sum_{i=1}^N \\mathcal{L}^{(i)}$ であり，$N$は全データのサンプル数を意味する．なお，パラメータは$\\theta\\leftarrow \\theta + \\Delta \\theta$により更新される．勾配降下法は$N$が大きい場合，あるいは1つのサンプルのデータサイズが大きい場合は非効率であるので，ニューラルネットワークの学習においては，データの部分集合であるミニバッチ (mini-bacth) を用いた **確率的勾配降下法** (stochastic gradient descent; SGD) が用いられる．Optimizerは複数種類があるが，一般的には $\\dfrac{\\partial \\mathcal{L}}{\\partial \\theta}$ の計算を必要とする．この計算を効率よく行う手法が**誤差逆伝播法** (backpropagation) である．誤差逆伝播法は連鎖律 (chain rule; 合成関数の微分に関わる関係式) を用いて導くことができる．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}=\\hat{\\mathbf{y}}-\\mathbf{y}\\\\\n",
    "\\delta_L&:=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_L}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}} \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_L}=\\left(\\hat{\\mathbf{y}}-\\mathbf{y}\\right) \\odot f_L^{\\prime}\\left(\\mathbf{a}_L\\right)\\\\\n",
    "\\mathbf{\\delta}_\\ell&:=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell}}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{\\ell+1}} \\frac{\\partial \\mathbf{z}_{\\ell+1}}{\\partial \\mathbf{a}_\\ell}\\\\\n",
    "&=\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell+1}}\\frac{\\partial \\mathbf{a}_{\\ell+1}}{\\partial \\mathbf{z}_{\\ell+1}}\\right)\\frac{\\partial \\mathbf{z}_{\\ell+1}}{\\partial \\mathbf{a}_{\\ell}}\\\\\n",
    "&={W_{\\ell+1}}^\\top \\delta_{\\ell+1} \\odot f_\\ell^{\\prime}\\left(\\mathbf{a}_{\\ell}\\right)\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_\\ell}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_\\ell} \\frac{\\partial \\mathbf{z}_\\ell}{\\partial \\mathbf{a}_\\ell} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial W_\\ell}=\\delta_\\ell \\mathbf{z}_\\ell^\\top\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_\\ell}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_\\ell} \\frac{\\partial \\mathbf{z}_\\ell}{\\partial \\mathbf{a}_\\ell} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial \\mathbf{b}_\\ell}=\\delta_\\ell\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ処理を考慮すると，行列を乗ずる順番が変わる．以下では$z=f(a), g(z)=f'(a)$として膜電位を使わず，発火率情報のみを使うようにしている．このようにできない関数もあるが，今回はこのように書き下せる活性化関数のみを扱う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h\\left(\\mathbf{z}_{\\ell+1}\\right)=f^{\\prime}\\left(\\mathbf{a}_{\\ell}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "クロスエントロピー損失関数の勾配は、ソフトマックス出力と正解ラベルの差を計算することで求められます。この過程を以下で詳しく説明します。\n",
    "\n",
    "---\n",
    "\n",
    "### **前提**\n",
    "- **ソフトマックス出力**:  \n",
    "  \\( \\mathbf{y} = \\text{softmax}(\\mathbf{z}) \\)  \n",
    "  各成分は以下のように定義されます:\n",
    "  \\[\n",
    "  y_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "  \\]\n",
    "  ここで \\( z_i \\) はモデルのロジット（未正規化スコア）です。\n",
    "\n",
    "- **正解ラベル**:  \n",
    "  \\( \\mathbf{t} \\) は one-hot エンコーディングで表現される正解ラベルベクトル。  \n",
    "  \\( t_i = 1 \\) （正解クラスの場合）で、それ以外は \\( t_j = 0 \\)（非正解クラスの場合）。\n",
    "\n",
    "- **クロスエントロピー損失**:  \n",
    "  損失関数 \\( L \\) は次のように定義されます:\n",
    "  \\[\n",
    "  L = - \\sum_{i} t_i \\log(y_i)\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **勾配の導出**\n",
    "\n",
    "クロスエントロピー損失 \\( L \\) を \\( z_i \\) で微分します。\n",
    "\n",
    "#### **1. ソフトマックスの性質を使う**\n",
    "ソフトマックスの出力 \\( y_i \\) に対して、ロジット \\( z_k \\) の微分は以下のように表されます:\n",
    "\\[\n",
    "\\frac{\\partial y_i}{\\partial z_k} =\n",
    "\\begin{cases}\n",
    "y_i (1 - y_i) & \\text{if } i = k, \\\\\n",
    "-y_i y_k & \\text{if } i \\neq k.\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "#### **2. クロスエントロピー損失の微分**\n",
    "損失 \\( L \\) を \\( z_k \\) で微分すると、チェーンルールを使って次の式が得られます:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial z_k} = \\sum_i \\frac{\\partial L}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial z_k}.\n",
    "\\]\n",
    "\n",
    "クロスエントロピー損失の \\( y_i \\) に関する微分は:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial y_i} = -\\frac{t_i}{y_i}.\n",
    "\\]\n",
    "\n",
    "これを代入して整理すると:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial z_k} = y_k - t_k.\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **結果**\n",
    "クロスエントロピー損失におけるロジット \\( z_k \\) の勾配は次のように単純化されます:\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial z_k} = y_k - t_k.\n",
    "\\]\n",
    "\n",
    "つまり、ソフトマックス出力 \\( \\mathbf{y} \\) と正解ラベル \\( \\mathbf{t} \\) の差そのものです。\n",
    "\n",
    "---\n",
    "\n",
    "### **直感的な解釈**\n",
    "- 勾配 \\( y_k - t_k \\) はnp.sum(np.exp(z))\n",
    "\n",
    "# 勾配\n",
    "grad = y - t\n",
    "print(\"Softmax出力:\", y)\n",
    "print(\"勾配:\", grad)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clog(x) = max(log(x), -1e2) # clamped log\n",
    "\n",
    "function binary_crossentropy!(nn::NeuralNet, ŷ::Array, y::Array)\n",
    "    (; L, states, δ) = nn\n",
    "    error = ŷ - y\n",
    "    loss = sum(-y .* clog.(ŷ) + (1 .- y) .* clog.(1 .- ŷ))\n",
    "    δ[L] = error\n",
    "    return loss\n",
    "end\n",
    "\n",
    "function squared_error!(nn::NeuralNet, ŷ::Array, y::Array)\n",
    "    (;L, states, ∇f, δ) = mlp\n",
    "    error = ŷ - y\n",
    "    loss = sum(error .^ 2)\n",
    "    δ[L] = error .* ∇f.(ŷ)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizerの作成\n",
    "abstract typeとして`Optimizer`タイプを作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Optimizer end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**確率的勾配降下法(stochastic gradient descent; SGD)** を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "@kwdef struct SGD{FT} <: Optimizer\n",
    "    η::FT = 1e-2\n",
    "end\n",
    "\n",
    "function optimizer_update!(param, grad, optimizer::SGD)\n",
    "    @unpack η = optimizer\n",
    "    param[:, :] -= η * grad\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に**Adam** {cite:p}`Kingma2014-fm` を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "@kwdef mutable struct Adam{FT} <: Optimizer\n",
    "    α::FT  = 1e-4; β1::FT = 0.9; β2::FT = 0.999; ϵ::FT = 1e-8\n",
    "    ms = Dict(); vs = Dict();\n",
    "end\n",
    "\n",
    "# Adam optimizer\n",
    "function optimizer_update!(param, grad, optimizer::Adam)\n",
    "    @unpack α, β1, β2, ϵ, ms, vs = optimizer\n",
    "    key = objectid(param)\n",
    "    if !haskey(ms, key) \n",
    "        ms[key], vs[key] = zeros(size(param)), zeros(size(param))\n",
    "    end    \n",
    "    m, v = ms[key], vs[key]\n",
    "    m += (1 - β1) * (grad - m)\n",
    "    v += (1 - β2) * (grad .* grad - v)\n",
    "    param[:, :] -= α * m ./ (sqrt.(v) .+ ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optim_step!(nn::NeuralNet, optimizer::Optimizer)\n",
    "    @unpack L, W, b, ∇W, ∇b = nn\n",
    "    params = [W, b]\n",
    "    grads = [∇W, ∇b]\n",
    "    for (param, grad) in zip(params, grads)\n",
    "        for l in 1:L\n",
    "            optimizer_update!(param[l], grad[l], optimizer)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_step!(nn::NeuralNet, x::Array, y::Array, loss_fun::Function, optimizer::Optimizer=SGD())\n",
    "    #ŷ = forward!(nn, x)\n",
    "    ŷ = nn(x)\n",
    "    loss = loss_fun(nn, ŷ, y)\n",
    "    backward!(nn)\n",
    "    optim_step!(nn, optimizer) # update params\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipser-Andersenモデル\n",
    "Zipser-Andersenモデル {cite:p}`Zipser1988-nc` は頭頂葉の7a野のモデルであり，網膜座標系における物体の位置と眼球位置を入力として，頭部中心座標(head centered coordinate)に変換する．隠れ層はPPC(Posterior parietal cortex)の細胞のモデルになっている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの生成\n",
    "物体位置の表現にはGaussian形式とmonotonic形式があるが，簡単のために，Gaussian形式を用いる．なお，monotonic形式については末尾の補足を参照してほしい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian 2d\n",
    "function Gaussian2d(pos, sizex=8, sizey=8, σ=1)\n",
    "    x, y = 0:sizex-1, 0:sizey-1\n",
    "    X, Y = [i for i in x, j in 1:length(y)], [j for i in 1:length(x), j in y]\n",
    "    x0, y0 = pos\n",
    "    return exp.(-((X .- x0) .^2 + (Y .- y0) .^2) / 2σ^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力は64(網膜座標系での位置)+2(眼球位置信号)=66とする．眼球位置信号は原著ではmonotonic形式による32(=8ユニット×2(x, y方向)×2 (傾き正負))ユニットで構成されるが，簡単のために眼球位置信号も$x, y$の2次元とする．視覚刺激は-40度から40度までの範囲であり，10度で離散化する．よって，網膜座標系での位置は$8\\times 8$の行列で表現される．位置は2次元のGaussianで表現する．ただし，1/e幅（ピークから1/eに減弱する幅）は15度である．$1/e$の代わりに$1/2$とすれば半値全幅(FWHM)となる．スポットサイズを$W$，Gaussianを$G(x)$とすると．$G(x+w/2)=G/e$より，$\\sigma=\\frac{\\sqrt{2}w}{4}$と求まる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset θeter\n",
    "θmax = 40.0 # degree, θ∈[-θmax, θmax]\n",
    "Δθ = 10.0 # degree\n",
    "stimuli_size = Int(2θmax / Δθ)\n",
    "w = 15.0 # degree; 1/e width\n",
    "σ = √2w/(4Δθ);\n",
    "\n",
    "# training θeter\n",
    "n_data = 10000\n",
    "n_traindata = Int(n_data*0.95)\n",
    "n_batch = 100 # batch size\n",
    "n_iter_per_epoch = Int(n_traindata/n_batch)\n",
    "n_epoch = 2000; # number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate positions\n",
    "Random.seed!(0)\n",
    "retinal_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]\n",
    "head_centered_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]\n",
    "#retinal_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]\n",
    "#head_centered_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]\n",
    "eye_pos = head_centered_pos - retinal_pos; # ∈ [-80, 80]\n",
    "\n",
    "# convert\n",
    "input_retina = [hcat(Gaussian2d((retinal_pos[i, :] .+ θmax)/Δθ, stimuli_size, stimuli_size, σ)...) for i in 1:n_data];\n",
    "input_retina = vcat(input_retina...)\n",
    "eye_pos /= 2θmax;\n",
    "\n",
    "# concat\n",
    "x_data = hcat(input_retina, eye_pos) #_encoded)\n",
    "y_data = vcat([hcat(Gaussian2d((head_centered_pos[i, :] .+ θmax)/Δθ, stimuli_size, stimuli_size, σ)...) for i in 1:n_data]...);\n",
    "\n",
    "# split\n",
    "x_traindata, y_traindata = x_data[1:n_traindata, :], y_data[1:n_traindata, :]\n",
    "x_testdata, y_testdata = x_data[n_traindata+1:end, :], y_data[n_traindata+1:end, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product(sets...) = hcat([collect(x) for x in Iterators.product(sets...)]...)' # Array of Cartesian product of sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの定義を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model θeter\n",
    "n_in = stimuli_size^2 + 2 # number of inputs\n",
    "n_hid = 16   # number of hidden units\n",
    "n_out = stimuli_size^2   # number of outputs\n",
    "η = 1e-2  # learning rate\n",
    "losstype = \"binary_crossentropy\" # \"squared_error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP([n_in, n_hid, n_out])#, bias=false)\n",
    "optimizer = SGD(η=η);\n",
    "loss_fun = binary_crossentropy!\n",
    "#optimizer = Adam();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_hid, n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 1\n",
    "iter = 1\n",
    "idx = (iter-1)*n_batch+1:iter*n_batch\n",
    "x, y = x_traindata[idx, :], y_traindata[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = forward!(nn, x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_step!(nn, x, y, loss_fun, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_arr[e] += loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "error_arr = zeros(n_epoch); # memory array of each epoch error\n",
    "\n",
    "@showprogress \"Training...\" for e in 1:n_epoch\n",
    "    for iter in 1:n_iter_per_epoch\n",
    "        idx = (iter-1)*n_batch+1:iter*n_batch\n",
    "        x, y = x_traindata[idx, :], y_traindata[idx, :]\n",
    "        loss = train_step!(nn, x, y, loss_fun, optimizer)\n",
    "        error_arr[e] += loss\n",
    "    end \n",
    "    error_arr[e] /= n_traindata\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失の変化を描画する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(4,2))\n",
    "#semilogy(error_arr)\n",
    "plot(error_arr)\n",
    "ylabel(\"Error\"); xlabel(\"Epoch\"); xlim(0, n_epoch)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータを用いて，出力を確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = x_testdata[1:2, :], y_testdata[1:2, :]\n",
    "ŷ = forward!(nn, x)\n",
    "\n",
    "id = 1\n",
    "figure(figsize=(6,2))\n",
    "ax1 = subplot(1,3,1); title(\"input\")\n",
    "ax1.imshow(reshape(x[id, 1:64], (stimuli_size, stimuli_size))', interpolation=\"gaussian\", extent=[-θmax, θmax, θmax, -θmax])\n",
    "ax1.add_patch(plt.Circle((x[id, 65:66])*2θmax, radius=2, color=\"tab:red\", fill=false))\n",
    "xlabel(\"x\"); ylabel(\"y\");\n",
    "\n",
    "ax2 = subplot(1,3,2); title(\"output\")\n",
    "ax2.imshow(reshape(ŷ[id, :], (stimuli_size, stimuli_size))', interpolation=\"gaussian\", extent=[-θmax, θmax, θmax, -θmax])\n",
    "ax2.add_patch(plt.Circle((x[id, 65:66])*2θmax, radius=2, color=\"tab:red\", fill=false))\n",
    "xlabel(\"x\");\n",
    "\n",
    "ax3 = subplot(1,3,3); title(\"target\")\n",
    "ax3.imshow(reshape(y[id, :], (stimuli_size, stimuli_size))', interpolation=\"gaussian\", extent=[-θmax, θmax, θmax, -θmax])\n",
    "ax3.add_patch(plt.Circle((x[id, 65:66])*2θmax, radius=2, color=\"tab:red\", fill=false))\n",
    "xlabel(\"x\");\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重み`W1`におけるゲインフィールドの描画を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gain fields\n",
    "figure(figsize=(3.2, 3))\n",
    "suptitle(\"Gain fields\", fontsize=12)\n",
    "subplots_adjust(hspace=0.1, wspace=0.1, top=0.925)\n",
    "for i in 1:n_hid\n",
    "    #subplot(3, 3, i)\n",
    "    subplot(4, 4, i)\n",
    "    imshow(reshape(nn.params[\"W\"][1][1:stimuli_size^2, i], (stimuli_size, stimuli_size)), cmap=\"hot\")\n",
    "    axis(\"off\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "補足としてMonotonic formatによる位置のエンコーディングに触れる．monotonic形式を入力の眼球位置と出力の頭部中心座標で用いるという仮定には，視覚刺激を中心窩で捉えた際，得られる眼球位置信号を頭部中心座標での位置の教師信号として使用できるという利点がある．\\citep{Andersen1983-zp} では Parietal visual neurons (PVNs)の活動を調べ，傾き正あるいは負．0度をピークとして減少あるいは上昇の4種類あることを示した．前者は一次関数（とReLU関数）で記述可能である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_line(p1, p2) = [(p2[2]-p1[2])/(p2[1]-p1[1]), (p2[1]*p1[2] - p1[1]*p2[2])/(p2[1]-p1[1])] # [slope, intercept]\n",
    "eye_pos_coding(x; linear_θ) = relu.(linear_θ[1, :] * x .+ linear_θ[2, :])\n",
    "\n",
    "x = -2θmax:1:2θmax\n",
    "slope_θ = hcat([get_line([80, 1], [-80, -2(i-1)/stimuli_size]) for i in 1:stimuli_size]...)\n",
    "y = hcat(eye_pos_coding.(x; linear_θ=slope_θ)...)\n",
    "eye_pos_encoded = eye_pos_coding(-10; linear_θ=slope_θ);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(5,3))\n",
    "subplot(2,1,1); plot(x, y'); xlabel(\"Eye position\"); ylabel(\"Firing rate\")\n",
    "subplot(2,1,2); imshow(eye_pos_encoded[:, :]'); title(L\"Eye position $=-10^\\circ$\"); xlabel(\"Units\") \n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
