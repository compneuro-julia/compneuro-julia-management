{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配法と誤差逆伝播法\n",
    "ニューラルネットワークにおいて，効率よく各重みの勾配を推定することで貢献度割り当て問題を解決する方法が**誤差逆伝播法** (backpropagation) である．本節では入力層，隠れ層，出力層からなる多層ニューラルネットワークを実装し，誤差逆伝播法による勾配推定を用いて学習を行う．\n",
    "\n",
    "本書では誤差逆伝播法を用いない学習法を実施することも考慮し，数式と対応するような実装を行う．そのため，Deep Learningライブラリ (PyTorch, Flux.jl等) のようにLayerを定義し，それを繋げてモデルを定義するということや計算グラフの構築は行わない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base: @kwdef\n",
    "#using Parameters: @unpack # or using UnPack\n",
    "using LinearAlgebra, Random, Statistics, PyPlot, ProgressMeter\n",
    "#using BenchmarkTools; @btime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数 (activation function) の構造体を`ActivationFunction` と定義する．この構造体には`forward` と `backward`の2種類の関数フィールド (field; 構造体の要素のこと) を持たせておく．2種類の関数はそれぞれ順伝播時と逆伝播時に使用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ActivationFunction\n",
    "    forward::Function   # function for forward propagation\n",
    "    backward::Function  # function for back-propagation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，次のコードにより，構造体のインスタンスを関数として使用できるようにしておく．これはcallable objectと呼ばれる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f::ActivationFunction)(x) = f.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代表的な活性化関数を紹介する．なお，`backward` における `y` は `forward` での出力に対応する．これは活性化関数を作用させる前の変数 ($x$であり，膜電位に対応する) を保持しておかなくても良いようにするためである．Sigmoid関数の場合，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textrm{forward: } y = \\frac{1}{1+e^{-x}}\\\\\n",
    "&\\textrm{backward: } \\frac{dy}{dx} =\\frac{e^{-x}}{(1+e^{-x})^2}= y\\cdot (1-y)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "tanh関数の場合，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textrm{forward: } y = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\\\\\n",
    "&\\textrm{backward: } \\frac{dy}{dx} =\\frac{(e^x+e^{-x})(e^x+e^{-x})-(e^x-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}= 1-y^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ReLU関数 (rectified linear unit, 正規化線形関数) の場合，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textrm{forward: } y = \\max(x, 0)\\\\\n",
    "&\\textrm{backward: } \\frac{dy}{dx} = \\mathbf{1}_{x > 0}(x) = \\mathbf{1}_{y > 0}(y)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ただし，$\\max(a, b)$は, $a, b$のうち，大きい値を返す関数である．また，$\\mathbf{1}_{A}(x)$ は指示関数 (indicator function)であり，$x\\in A$ ならば $\\mathbf{1}_A(x)=1$ であり，それ以外の場合は $\\mathbf{1}_A(x)=0$ となる関数である．ReLU関数は $x=0$ で折れ曲がるが，その他では線形であるため，区分線形関数 (piecewise linear function) の一種であると言える．\n",
    "\n",
    "これらの活性化関数を構造体 `ActivationFunction`を用いて実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigmoid = ActivationFunction(\n",
    "    x -> 1.0 ./ (1.0 .+ exp.(-x)),\n",
    "    y -> y .* (1 .- y)\n",
    ")\n",
    "\n",
    "Tanh = ActivationFunction(\n",
    "    x -> tanh.(x),\n",
    "    y -> 1 .- y .^ 2\n",
    ")\n",
    "\n",
    "Relu = ActivationFunction(\n",
    "    x -> max.(0, x),\n",
    "    y -> y .> 0\n",
    ")\n",
    "\n",
    "Identity = ActivationFunction(\n",
    "    x -> x, # or use `identity` function\n",
    "    y -> fill!(similar(y), 1)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークのパラメータを保持する構造体を定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNet end\n",
    "(f::NeuralNet)(x) = forward!(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワーク内の学習可能なパラメータ (learnable parameter) を保持する構造体を定義する．各パラメータはそれ自体の値 (value) と損失関数に対する勾配 (gradient) を持つ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Param\n",
    "    v::Array\n",
    "    grad::Array\n",
    "\n",
    "    Param(value) = new(value, zero(value))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MLP <: NeuralNet\n",
    "    L::Int # num. of layers\n",
    "    W::Vector{Param}; b::Vector{Param}   # weights and bias\n",
    "    z::Vector{Array}; δ::Vector{Array}   # state of forward/backward activity\n",
    "    f::Vector{ActivationFunction} # activation functions of layers\n",
    "\n",
    "    function MLP(n_units::Vector{Int}; f_hid::ActivationFunction=Sigmoid, f_out::ActivationFunction=Identity)\n",
    "        L = length(n_units) - 1\n",
    "        # initialization of parameters\n",
    "        W = [Param(2 * (rand(n_units[l], n_units[l+1]) .- 0.5) * sqrt(6/(n_units[l]+n_units[l+1]))) for l in 1:L] # Xavier\n",
    "        b = [Param(zeros(1, n_units[l+1])) for l in 1:L]\n",
    "        \n",
    "        # initialization of forward / backward states\n",
    "        z, δ = Vector{Array}(undef, L+1), Vector{Array}(undef, L)\n",
    "        f = vcat([repeat([f_hid], L-1)..., f_out])\n",
    "        new(L, W, b, z, δ, f)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "struct `MLP`を用意し，**重みの初期化(weight initialization)** を行う同名の関数`MLP`を用意する．重みの初期化に関しては，各層の出力および勾配の分散が一定となるような初期化をすることで学習が進行することが知られている．出力は活性化関数に依存するため，初期化についても活性化関数に応じて変更することが推奨され，sigmoid関数やtanh関数を用いる場合はXavierの初期化 \\citep{Glorot2010-iu}，ReLU関数を用いる場合はHeの初期化 \\citep{He2015-fs} が用いられる．入力ユニット数を $n_{\\textrm{in}}$, 出力ユニット数を $n_{\\textrm{out}}$ とすると，Xavierの初期化では重み $w$ の平均が0, 分散が $\\frac{2}{n_{\\textrm{in}}+n_{\\textrm{out}}}$ となるように一様分布 $U\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right)$ や正規分布 $\\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right)$ 等から重みをサンプリングする．Heの初期化ではReLUを用いる場合，重み $w$ の平均が0, 分散が$\\frac{2}{n_{\\textrm{in}}}$ あるいは $\\frac{2}{n_{\\textrm{out}}}$ となるようにし，前者の分散を使用する場合は一様分布 $U\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}}}\\right)$ や正規分布 $\\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}}}\\right)$ 等から重みをサンプリングする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 順伝播 (forward propagation)\n",
    "$f(\\cdot)$を活性化関数とする．順伝播(feedforward propagation)は以下のようになる．$(\\ell=1,\\ldots,L)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{入力層 : }&\\mathbf{z}_1=\\mathbf{x}\\\\\n",
    "\\text{隠れ層 : }&\\mathbf{a}_\\ell=W_\\ell \\mathbf{z}_\\ell +\\mathbf{b}_\\ell\\\\\n",
    "&\\mathbf{z}_{\\ell+1}=f_\\ell\\left(\\mathbf{a}_\\ell\\right)\\\\\n",
    "\\text{出力層 : }&\\hat{\\mathbf{y}}=\\mathbf{z}_{L+1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装における取り回しの都合上，以下では$f:=f_\\ell\\ (1, \\ldots, L-1)$ とし，$g:=f_{L}$ は損失関数において記述されることとする．以下では`z[l]`は$\\mathbf{z}_\\ell \\ (1, \\ldots, L)$に対応するが，`z[L+1]`は$\\mathbf{z}_{L+1}$ではなく$\\mathbf{a}_{L}$に対応することに注意．\n",
    "\n",
    "とするか？混乱をきたしかねないし，余計な計算が増える．softmaxをどうするか．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward computation of MLP\n",
    "function forward!(mlp::MLP, x::Array)\n",
    "    (;L, f, W, b, z) = mlp\n",
    "    z[1] = x # input (n_batch x n_neurons)\n",
    "    for l in 1:L\n",
    "        z[l+1] = f[l](z[l] * W[l].v .+ b[l].v) # hidden layers\n",
    "    end\n",
    "    return z[L+1] # output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W[l].v` は`l`番目の`Param`のインスタンスにおける`v`を取り出す操作である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆伝播 (backward propagation)\n",
    "ニューラルネットワークの学習 (learning) あるいは訓練 (training) とは，目的関数 (objective function) あるいは損失関数 (loss function) と呼ばれる評価指標を可能な限り小さく (場合によっては大きく) するようなパラメータ集合 $\\Theta = \\{W_\\ell, b_\\ell\\}_{\\ell=1}^{L}$ を求める過程のことである．学習においてパラメータを最適化するアルゴリズムを**オプティマイザ** (optimizer) という．オプティマイザは多数提案されており，代表的なものを後ほど紹介する．まず，最も単純なオプティマイザである **勾配降下法** (gradient descent; GD) を紹介する．勾配降下法では全データを用いてパラメータ $\\theta \\in \\Theta$ の更新量 $\\Delta \\theta$ を \n",
    "\n",
    "$$\n",
    "\\Delta \\theta = -\\eta \\frac{\\partial \\mathcal{L}_{\\textrm{GD}}}{\\partial \\theta} = -\\frac{\\eta}{N} \\sum_{i=1}^N \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "として計算する (パラメータは$\\theta\\leftarrow \\theta + \\Delta \\theta$により更新される)．ただし，$\\mathcal{L}_{\\textrm{GD}}:=\\frac{1}{N}\\sum_{i=1}^N \\mathcal{L}^{(i)}$ であり，$\\mathcal{L}^{(i)}$は$i$ 番目のサンプルに対する目的関数であり，$N$ は全データのサンプル数を意味する．$\\eta$ は学習率 (learning rate) である．オプティマイザは一般的には $\\dfrac{\\partial \\mathcal{L}}{\\partial \\theta}$ の計算を必要とする．この計算を効率よく行う手法が**誤差逆伝播法** (backpropagation) である．誤差逆伝播法は連鎖律 (chain rule; 合成関数の微分の関係式) を用いて導くことができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}\\\\\n",
    "\\delta_L&:=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_L}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}} \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_L}\\\\\n",
    "\\mathbf{\\delta}_\\ell&:=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell}}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{\\ell+1}} \\frac{\\partial \\mathbf{z}_{\\ell+1}}{\\partial \\mathbf{a}_\\ell}\\\\\n",
    "&=\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{\\ell+1}}\\frac{\\partial \\mathbf{a}_{\\ell+1}}{\\partial \\mathbf{z}_{\\ell+1}}\\right)\\frac{\\partial \\mathbf{z}_{\\ell+1}}{\\partial \\mathbf{a}_{\\ell}}\\\\\n",
    "&={W_{\\ell+1}}^\\top \\delta_{\\ell+1} \\odot f_\\ell^{\\prime}\\left(\\mathbf{a}_{\\ell}\\right)\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_\\ell}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_\\ell} \\frac{\\partial \\mathbf{z}_\\ell}{\\partial \\mathbf{a}_\\ell} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial W_\\ell}=\\delta_\\ell \\mathbf{z}_\\ell^\\top\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_\\ell}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_\\ell} \\frac{\\partial \\mathbf{z}_\\ell}{\\partial \\mathbf{a}_\\ell} \\frac{\\partial \\mathbf{a}_\\ell}{\\partial \\mathbf{b}_\\ell}=\\delta_\\ell\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ処理を考慮すると，行列を乗ずる順番が変わる．以下では$z=f(a), g(z)=f'(a)$として膜電位を使わず，発火率情報のみを使うようにしている．このようにできない関数もあるが，今回はこのように書き下せる活性化関数のみを扱う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward!(mlp::MLP)\n",
    "    (;L, W, b, z, δ, f) = mlp\n",
    "    n_batch = size(z[1])[1]\n",
    "    # backprop\n",
    "    for l in L:-1:1\n",
    "        if l < L\n",
    "            δ[l] = δ[l+1] * W[l+1].v' .* f[l+1].backward(z[l+1])\n",
    "        end\n",
    "        W[l].grad = z[l]' * δ[l] / n_batch\n",
    "        b[l].grad = sum(δ[l], dims=1) / n_batch\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "回帰問題において，平均二乗誤差 (mean squared error) である．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{y}} &= \\mathbf{z}_{L+1}\\\\\n",
    "\\mathcal{L}&:=\\frac{1}{2}\\left\\|\\hat{\\mathbf{y}}-\\mathbf{y}\\right\\|^{2}\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}}&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}}=\\hat{\\mathbf{y}}-\\mathbf{y}\\\\\n",
    "\\delta_L&=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_L}=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{L+1}} \\frac{\\partial \\mathbf{z}_{L+1}}{\\partial \\mathbf{a}_L}=\\left(\\hat{\\mathbf{y}}-\\mathbf{y}\\right) \\odot f_L^{\\prime}\\left(\\mathbf{a}_L\\right)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "squared_error! (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function squared_error!(nn::NeuralNet, y::Array)\n",
    "    (;z, δ, f) = nn\n",
    "    ŷ = z[end]\n",
    "    error = ŷ - y\n",
    "    loss = sum(error .^ 2)\n",
    "    δ[end] = error .* f[end].backward(ŷ)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binary_crossentropy! (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clog(x) = max(log(x), -1e2) # clamped log\n",
    "\n",
    "function binary_crossentropy!(nn::NeuralNet, y::Array)\n",
    "    (;z, δ, f) = nn\n",
    "    @assert f[end] == Sigmoid # f[end] must be sigmoid function\n",
    "    ŷ = z[end]\n",
    "    error = ŷ - y\n",
    "    loss = sum(-y .* clog.(ŷ) + (1 .- y) .* clog.(1 .- ŷ))\n",
    "    δ[end] = error\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多クラス分類課題で用いられるのが，softmaxおよびcross entropy lossである．\n",
    "softmax関数は $\\mathbf{y} = \\text{softmax}(\\mathbf{z})$ とすると，各成分を以下のように定義される．\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy_loss! (generic function with 2 methods)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(x; dims=2)\n",
    "    expx = exp.(x .- maximum(x, dims=dims))\n",
    "    return expx ./ sum(expx, dims=dims)\n",
    "end\n",
    "\n",
    "# t: labels (1 dims)\n",
    "function cross_entropy_loss!(nn::NeuralNet, t::Vector)\n",
    "    (;z, δ, f) = nn\n",
    "    @assert f[end] == Softmax # f[end] must be softmax function\n",
    "    ŷ = z[end]\n",
    "    n_batch = length(t)\n",
    "    idx = CartesianIndex.([(i, t[i]) for i in 1:n_batch])\n",
    "    loss = -sum(log.(ŷ[idx])) / n_batch\n",
    "    grad = copy(ŷ)\n",
    "    grad[idx] .-= 1\n",
    "    δ[end] = grad / n_batch\n",
    "    return loss\n",
    "end\n",
    "\n",
    "# t: probability (2 dims)\n",
    "function cross_entropy_loss!(nn::NeuralNet, t::Array)\n",
    "    (;z, δ, f) = nn\n",
    "    @assert f[end] == Softmax # f[end] must be softmax function\n",
    "    ŷ = z[end]\n",
    "    n_batch = length(t)\n",
    "    loss = -sum(t .* log.(ŷ)) / n_batch\n",
    "    δ[end] = (ŷ - t) / n_batch\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmaxのbackwardは基本的に使用しないため，適当に`identity`関数などを入れておく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Softmax = ActivationFunction(softmax, identity);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(2, Param[Param([0.27348110356798505 -0.09033486777923028 … -0.2730145650905053 -0.17690212045501486; -0.21808232772976824 -0.1151467611443998 … 0.28552587866536666 -0.12108017201208604; … ; -0.1654325391445399 0.30943539522473124 … -0.2507082940420157 -0.1318004417683606; 0.26358809787759846 -0.2785204651452566 … 0.2798145614440025 -0.0476109853928617], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]), Param([-0.033103085034701364 -0.21217752353721678 … -0.20346804340322314 -0.07689395972604034; 0.18299426733320803 0.02810237226895674 … -0.20191951156126825 -0.21955104661496402; … ; -0.043368476939381166 0.1565225277448134 … -0.16113933488537874 0.001588308241564527; -0.023644691122710216 0.05835296230813551 … -0.016387980603060914 -0.1248859722633745], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0])], Param[Param([0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0]), Param([0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0])], Array[#undef, #undef, #undef], Array[#undef, #undef], ActivationFunction[ActivationFunction(var\"#39#41\"(), var\"#40#42\"()), ActivationFunction(softmax, identity)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP([10, 20, 5], f_hid=Sigmoid, f_out=Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Int64}:\n",
       " 1\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rand(3, 10)\n",
    "z = mlp(x)\n",
    "t = [1, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5562923838174205"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = cross_entropy_loss!(mlp, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward!(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×20 Matrix{Float64}:\n",
       " -0.00203199  -0.00559096  -0.00198917   …  0.00298765  -0.000666\n",
       " -0.010464    -0.00967283  -0.00129624      0.00986524  -0.00399338\n",
       " -0.00508898  -0.0101427   -0.00442144      0.0107013   -0.00265334\n",
       " -0.0090633   -0.0126173   -0.00262835      0.00810504  -0.00299921\n",
       " -0.0059243   -0.0119502   -0.00191028      0.0010087   -0.00077351\n",
       " -0.0102574   -0.0137671   -0.00321541   …  0.0106966   -0.00374569\n",
       " -0.00836289  -0.00600306  -0.000661941     0.00889653  -0.00354638\n",
       " -0.00598769  -0.00532804  -0.00253649      0.0121643   -0.00361447\n",
       " -0.00983012  -0.00988697  -0.000233914     0.00477464  -0.0027777\n",
       "  0.0045094   -0.0013537   -0.00424051      0.0050462    0.000334339"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.W[1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オプティマイザ\n",
    "abstract typeとして`Optimizer`タイプを作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Optimizer end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配降下法は$N$が大きい場合，あるいは1つのサンプルのデータサイズが大きい場合は非効率であるので，ニューラルネットワークの学習においては，データの部分集合であるミニバッチ (mini-bacth) を用いた **確率的勾配降下法** (stochastic gradient descent; SGD) が用いられる．\n",
    "**確率的勾配降下法(stochastic gradient descent; SGD)** を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "@kwdef struct SGD{FT} <: Optimizer\n",
    "    η::FT=1e-2\n",
    "end\n",
    "\n",
    "function optimizer_update!(param::Param, optimizer::SGD)\n",
    "    @unpack η = optimizer\n",
    "    param.v -= η * param.grad\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に**Adam** {cite:p}`Kingma2014-fm` を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "@kwdef mutable struct Adam{FT} <: Optimizer\n",
    "    α::FT= 1e-4; β1::FT = 0.9; β2::FT = 0.999; ϵ::FT = 1e-8\n",
    "    ms = Dict(); vs = Dict();\n",
    "end\n",
    "\n",
    "# Adam optimizer\n",
    "function optimizer_update!(param::Param, optimizer::Adam)\n",
    "    @unpack α, β1, β2, ϵ, ms, vs = optimizer\n",
    "    key = objectid(param)\n",
    "    if !haskey(ms, key) \n",
    "        ms[key], vs[key] = zero(param.v), zero(param.v)\n",
    "    end    \n",
    "    m, v = ms[key], vs[key]\n",
    "    m += (1 - β1) * (param.grad - m)\n",
    "    v += (1 - β2) * (param.grad .* param.grad - v)\n",
    "    param.v -= α * m ./ (sqrt.(v) .+ ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optim_step!(nn::NeuralNet, optimizer::Optimizer)\n",
    "    (;L, W, b) = nn\n",
    "    for param in [W, b]\n",
    "        for l in 1:L\n",
    "            optimizer_update!(param[l], optimizer)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_step!(nn::NeuralNet, x::Array, y::Array, loss_fun::Function, optimizer::Optimizer=SGD())\n",
    "    #ŷ = forward!(nn, x)\n",
    "    _ = nn(x)\n",
    "    loss = loss_fun(nn, y)\n",
    "    backward!(nn)\n",
    "    optim_step!(nn, optimizer) # update params\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipser-Andersenモデル\n",
    "Zipser-Andersenモデル {cite:p}`Zipser1988-nc` は頭頂葉の7a野のモデルであり，網膜座標系における物体の位置と眼球位置を入力として，頭部中心座標(head centered coordinate)に変換する．隠れ層はPPC(Posterior parietal cortex)の細胞のモデルになっている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの生成\n",
    "物体位置の表現にはGaussian形式とmonotonic形式があるが，簡単のために，Gaussian形式を用いる．なお，monotonic形式については末尾の補足を参照してほしい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian 2d\n",
    "function Gaussian2d(pos, sizex=8, sizey=8, σ=1)\n",
    "    x, y = 0:sizex-1, 0:sizey-1\n",
    "    X, Y = [i for i in x, j in 1:length(y)], [j for i in 1:length(x), j in y]\n",
    "    x0, y0 = pos\n",
    "    return exp.(-((X .- x0) .^2 + (Y .- y0) .^2) / 2σ^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力は64(網膜座標系での位置)+2(眼球位置信号)=66とする．眼球位置信号は原著ではmonotonic形式による32(=8ユニット×2(x, y方向)×2 (傾き正負))ユニットで構成されるが，簡単のために眼球位置信号も$x, y$の2次元とする．視覚刺激は-40度から40度までの範囲であり，10度で離散化する．よって，網膜座標系での位置は$8\\times 8$の行列で表現される．位置は2次元のGaussianで表現する．ただし，1/e幅（ピークから1/eに減弱する幅）は15度である．$1/e$の代わりに$1/2$とすれば半値全幅(FWHM)となる．スポットサイズを$W$，Gaussianを$G(x)$とすると．$G(x+w/2)=G/e$より，$\\sigma=\\frac{\\sqrt{2}w}{4}$と求まる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset θeter\n",
    "θmax = 40.0 # degree, θ∈[-θmax, θmax]\n",
    "Δθ = 10.0 # degree\n",
    "stimuli_size = Int(2θmax / Δθ)\n",
    "w = 15.0 # degree; 1/e width\n",
    "σ = √2w/(4Δθ);\n",
    "\n",
    "# training θeter\n",
    "n_data = 10000\n",
    "n_traindata = Int(n_data*0.95)\n",
    "n_batch = 100 # batch size\n",
    "n_iter_per_epoch = Int(n_traindata/n_batch)\n",
    "n_epoch = 2000; # number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate positions\n",
    "Random.seed!(0)\n",
    "retinal_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]\n",
    "head_centered_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]\n",
    "#retinal_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]\n",
    "#head_centered_pos = (rand(n_data, 2) .- 0.5) * 2θmax # ∈ [-40, 40]\n",
    "eye_pos = head_centered_pos - retinal_pos; # ∈ [-80, 80]\n",
    "\n",
    "# convert\n",
    "input_retina = [hcat(Gaussian2d((retinal_pos[i, :] .+ θmax)/Δθ, stimuli_size, stimuli_size, σ)...) for i in 1:n_data];\n",
    "input_retina = vcat(input_retina...)\n",
    "eye_pos /= 2θmax;\n",
    "\n",
    "# concat\n",
    "x_data = hcat(input_retina, eye_pos) #_encoded)\n",
    "y_data = vcat([hcat(Gaussian2d((head_centered_pos[i, :] .+ θmax)/Δθ, stimuli_size, stimuli_size, σ)...) for i in 1:n_data]...);\n",
    "\n",
    "# split\n",
    "x_traindata, y_traindata = x_data[1:n_traindata, :], y_data[1:n_traindata, :]\n",
    "x_testdata, y_testdata = x_data[n_traindata+1:end, :], y_data[n_traindata+1:end, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product(sets...) = hcat([collect(x) for x in Iterators.product(sets...)]...)' # Array of Cartesian product of sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの定義を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model θeter\n",
    "n_in = stimuli_size^2 + 2 # number of inputs\n",
    "n_hid = 16   # number of hidden units\n",
    "n_out = stimuli_size^2   # number of outputs\n",
    "η = 1e-2  # learning rate\n",
    "losstype = \"binary_crossentropy\" # \"squared_error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP([n_in, n_hid, n_out])#, bias=false)\n",
    "optimizer = SGD(η=η);\n",
    "loss_fun = binary_crossentropy!\n",
    "#optimizer = Adam();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_hid, n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 1\n",
    "iter = 1\n",
    "idx = (iter-1)*n_batch+1:iter*n_batch\n",
    "x, y = x_traindata[idx, :], y_traindata[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = forward!(nn, x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_step!(nn, x, y, loss_fun, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_arr[e] += loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "error_arr = zeros(n_epoch); # memory array of each epoch error\n",
    "\n",
    "@showprogress \"Training...\" for e in 1:n_epoch\n",
    "    for iter in 1:n_iter_per_epoch\n",
    "        idx = (iter-1)*n_batch+1:iter*n_batch\n",
    "        x, y = x_traindata[idx, :], y_traindata[idx, :]\n",
    "        loss = train_step!(nn, x, y, loss_fun, optimizer)\n",
    "        error_arr[e] += loss\n",
    "    end \n",
    "    error_arr[e] /= n_traindata\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失の変化を描画する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(4,2))\n",
    "#semilogy(error_arr)\n",
    "plot(error_arr)\n",
    "ylabel(\"Error\"); xlabel(\"Epoch\"); xlim(0, n_epoch)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータを用いて，出力を確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = x_testdata[1:2, :], y_testdata[1:2, :]\n",
    "ŷ = forward!(nn, x)\n",
    "\n",
    "id = 1\n",
    "figure(figsize=(6,2))\n",
    "ax1 = subplot(1,3,1); title(\"input\")\n",
    "ax1.imshow(reshape(x[id, 1:64], (stimuli_size, stimuli_size))', interpolation=\"gaussian\", extent=[-θmax, θmax, θmax, -θmax])\n",
    "ax1.add_patch(plt.Circle((x[id, 65:66])*2θmax, radius=2, color=\"tab:red\", fill=false))\n",
    "xlabel(\"x\"); ylabel(\"y\");\n",
    "\n",
    "ax2 = subplot(1,3,2); title(\"output\")\n",
    "ax2.imshow(reshape(ŷ[id, :], (stimuli_size, stimuli_size))', interpolation=\"gaussian\", extent=[-θmax, θmax, θmax, -θmax])\n",
    "ax2.add_patch(plt.Circle((x[id, 65:66])*2θmax, radius=2, color=\"tab:red\", fill=false))\n",
    "xlabel(\"x\");\n",
    "\n",
    "ax3 = subplot(1,3,3); title(\"target\")\n",
    "ax3.imshow(reshape(y[id, :], (stimuli_size, stimuli_size))', interpolation=\"gaussian\", extent=[-θmax, θmax, θmax, -θmax])\n",
    "ax3.add_patch(plt.Circle((x[id, 65:66])*2θmax, radius=2, color=\"tab:red\", fill=false))\n",
    "xlabel(\"x\");\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重み`W1`におけるゲインフィールドの描画を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gain fields\n",
    "figure(figsize=(3.2, 3))\n",
    "suptitle(\"Gain fields\", fontsize=12)\n",
    "subplots_adjust(hspace=0.1, wspace=0.1, top=0.925)\n",
    "for i in 1:n_hid\n",
    "    #subplot(3, 3, i)\n",
    "    subplot(4, 4, i)\n",
    "    imshow(reshape(nn.params[\"W\"][1][1:stimuli_size^2, i], (stimuli_size, stimuli_size)), cmap=\"hot\")\n",
    "    axis(\"off\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "補足としてMonotonic formatによる位置のエンコーディングに触れる．monotonic形式を入力の眼球位置と出力の頭部中心座標で用いるという仮定には，視覚刺激を中心窩で捉えた際，得られる眼球位置信号を頭部中心座標での位置の教師信号として使用できるという利点がある．\\citep{Andersen1983-zp} では Parietal visual neurons (PVNs)の活動を調べ，傾き正あるいは負．0度をピークとして減少あるいは上昇の4種類あることを示した．前者は一次関数（とReLU関数）で記述可能である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_line(p1, p2) = [(p2[2]-p1[2])/(p2[1]-p1[1]), (p2[1]*p1[2] - p1[1]*p2[2])/(p2[1]-p1[1])] # [slope, intercept]\n",
    "eye_pos_coding(x; linear_θ) = relu.(linear_θ[1, :] * x .+ linear_θ[2, :])\n",
    "\n",
    "x = -2θmax:1:2θmax\n",
    "slope_θ = hcat([get_line([80, 1], [-80, -2(i-1)/stimuli_size]) for i in 1:stimuli_size]...)\n",
    "y = hcat(eye_pos_coding.(x; linear_θ=slope_θ)...)\n",
    "eye_pos_encoded = eye_pos_coding(-10; linear_θ=slope_θ);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(5,3))\n",
    "subplot(2,1,1); plot(x, y'); xlabel(\"Eye position\"); ylabel(\"Firing rate\")\n",
    "subplot(2,1,2); imshow(eye_pos_encoded[:, :]'); title(L\"Eye position $=-10^\\circ$\"); xlabel(\"Units\") \n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
