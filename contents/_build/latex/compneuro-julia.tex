%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}




\title{Juliaで学ぶ計算論的神経科学}
\date{Aug 14, 2020}
\release{}
\author{}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


このサイトは\sphinxstylestrong{計算論的神経科学 (Computational Neuroscience)} をプログラミング言語 \sphinxhref{https://julialang.org/}{\sphinxstylestrong{Julia}}を通して学習することを目標とします。

内容に関する指摘やコメントは各ページ末尾のコメント欄からしていただければ幸いです (GitHubアカウントが必要です)。

\begin{sphinxadmonition}{note}{記事で使用しているJuliaのバージョン}

Julia 1.5.0
\end{sphinxadmonition}


\chapter{目次}
\label{\detokenize{index:id1}}\begin{itemize}
\item {} 
\sphinxhref{https://compneuro-julia.github.io/intro.html}{まえがき}

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxhref{https://compneuro-julia.github.io/1\_intro.html}{はじめに}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
計算論的神経科学とは何か

\item {} 
\sphinxhref{https://compneuro-julia.github.io/notation.html}{記号の表記}

\end{enumerate}

\item {} 
\sphinxhref{https://compneuro-julia.github.io/2\_intro.html}{神経細胞のモデル}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
神経細胞の形態と膜電位変化

\item {} 
\sphinxhref{https://compneuro-julia.github.io/2-2\_hodgkinhuxley.html}{Hodgkin\sphinxhyphen{}Huxleyモデル}

\item {} 
FitzHugh\textendash{}Nagumoモデル

\item {} 
\sphinxhref{https://compneuro-julia.github.io/2-4\_lif.html}{Leaky integrate\sphinxhyphen{}and\sphinxhyphen{}fire モデル}

\item {} 
\sphinxhref{https://compneuro-julia.github.io/2-5\_iz.html}{Izhikevich モデル}

\item {} 
ケーブル理論

\item {} 
Multi\sphinxhyphen{}compartment モデル

\item {} 
\sphinxhref{https://compneuro-julia.github.io/2-8\_isi.html}{Inter\sphinxhyphen{}spike interval モデル}

\item {} 
確率的シナプス電流のノイズによる表現 (Langevin方程式 etc.)

\item {} 
確率的集団モデル (Fokker\textendash{}Planck 方程式)

\item {} 
発火率モデル

\end{enumerate}

\item {} 
\sphinxhref{https://compneuro-julia.github.io/3\_intro.html}{シナプス伝達のモデル}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxhref{https://compneuro-julia.github.io/3-1\_synapse.html}{シナプス伝達}

\item {} 
\sphinxhref{https://compneuro-julia.github.io/3-1\_current-conductance-synapse.html}{Current\sphinxhyphen{}based vs Conductance\sphinxhyphen{}based シナプス}

\item {} 
\sphinxhref{https://compneuro-julia.github.io/3-2\_expo-synapse.html}{指数関数型シナプスモデル}

\item {} 
\sphinxhref{https://compneuro-julia.github.io/3-3\_kinetic-synapse.html}{動力学モデル}

\item {} 
増強シナプスと減衰シナプス

\item {} 
\sphinxhref{https://compneuro-julia.github.io/3-6\_synaptic-weighted.html}{シナプス入力の重みづけ}

\item {} 
電気シナプス

\end{enumerate}

\item {} 
神経回路網の構築 (発火率モデル)

\item {} 
神経回路網の構築 (Spikingモデル)

\item {} 
神経回路網の演算処理

\item {} 
神経回路網の学習則
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
学習則と貢献度分配問題 (credit assignment problem)

\item {} 
Hebb則

\item {} 
STDP則

\item {} 
Burst発火と可塑性

\item {} 
競合学習 (competitive learning)

\item {} 
勾配法と誤差逆伝播法 (backpropagation)

\item {} 
誤差逆伝播法の近似手法

\item {} 
経時的貢献度分配問題 (temporal credit assignment problem)

\item {} 
RTRLとBPTT

\item {} 
適格度トレース (eligibility trace) とRTRLの近似手法

\item {} 
Reservoir computing (FORCE etc.)

\end{enumerate}

\item {} 
神経系の非線形ダイナミクス

\item {} 
連想記憶モデル
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
Ising モデル

\item {} 
Amari\sphinxhyphen{}Hopfield モデル

\item {} 
Boltzmann machine

\end{enumerate}

\item {} 
情報理論と最適化原理

\item {} 
\sphinxhref{https://compneuro-julia.github.io/11\_intro.html}{ベイズ脳理論と生成モデル}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxhref{https://compneuro-julia.github.io/11-1\_bayes\_statistics.html}{ベイズ統計の基礎}

\item {} 
\sphinxhref{https://compneuro-julia.github.io/11-2\_sparse-coding.html}{Sparse coding (Olshausen \& Field, 1996) モデル}

\item {} 
\sphinxhref{https://compneuro-julia.github.io/11-3\_predictive-coding-rao.html}{Predictive coding (Rao \& Ballard, 1999) モデル}

\end{enumerate}

\item {} 
強化学習

\item {} 
運動制御

\item {} 
時空間の符号化

\item {} 
神経細胞の形態と数理モデル

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxhref{https://compneuro-julia.github.io/appendix\_intro.html}{付録}
\begin{itemize}
\item {} 
\sphinxhref{https://compneuro-julia.github.io/tips.html}{JuliaのTips集}

\item {} 
\sphinxhref{https://compneuro-julia.github.io/useful\_links.html}{有用なリンク集}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip



\chapter{『ゼロから作るSpiking Neural Networks』について}
\label{\detokenize{index:spiking-neural-networks}}
『\sphinxstylestrong{ゼロから作るSpiking Neural Networks}』 (通称：SNN本) は\sphinxstylestrong{Python}でSpiking neural networksの構築と学習を実装することを目標とした技術同人誌です。本サイトはこの本をベースとして作成しています。技術書典7で頒布し、BOOTHで有料で販売してきましたが、無料で公開することとしました。それでも購入していただける方はBOOTHから購入いただければと思います。なお、物理本は完売し、再販の予定はありません。

\begin{sphinxadmonition}{note}{『ゼロから作るSpiking Neural Networks』Links}
\begin{itemize}
\item {} 
\sphinxhref{https://compneuro-julia.github.io/\_static/pdf/SNN\_from\_scratch\_with\_python\_ver2\_05.pdf}{pdf} (Ver. 2.05)

\item {} 
\sphinxhref{https://github.com/takyamamoto/SNN-from-scratch-with-Python}{GitHub}

\item {} 
\sphinxhref{https://booth.pm/ja/items/1585421}{BOOTH}

\end{itemize}
\end{sphinxadmonition}


\section{まえがき}
\label{\detokenize{preface:id1}}\label{\detokenize{preface::doc}}
何故このサイトを作ろうと思ったか、というと鬱憤が溜まっていたためである。\sphinxstylestrong{物理シミュレーションの本はあるのに神経科学のシミュレーションの本は何故無いのか？} もちろん、洋書を探せば何冊かあるが、ブラックボックス的なライブラリを用いたり、幅広い分野の内容を扱っていなかったりする。数理モデルを扱う洋書・和書の名著は多数あるが、実装まで記述してくれない本がほとんどである。そのくせ、関連分野のニューラルネットワークの本は腐るほど出版されている (というと著者の方々に大変失礼だが)。有名な論文でも再現するためのパラメータの記載が不足していたり、著者実装がなかったり、あっても古いMATLAB実装は動かない。シミュレーションを実施する講義を受けたいと思っても医学部のカリキュラムで試験と被る。潜りも難しい。所属している研究室は理論や数値計算がメインではないので研究室の「秘伝のタレ」があるわけでもない。Twitterで流れてくる講義の情報を見ても「どうして資料を公開してくれないんだ」と嘆きながら枕を濡らすしかない。

「本が無いなら本を書けばいいじゃない」という言葉は…今作った言葉だが、一先ずこれまでに自分が書いてきたコードやノート (\sphinxhref{https://salad-bowl-of-knowledge.github.io/hp/}{ブログ}、\sphinxhref{https://compneuro-julia.github.io/\#spiking-neural-networks}{同人誌 (SNN本)}などを含む)をまとめるだけでも良い資料になるのではないか、という中々思い上がった考えからこのサイトを始めた (目次を見るとかなり無謀なことが察せられる。何事も勢いで初めてはいけない)。このサイトの元となったSNN本が「SNNを如何に学習させるか」ということに重きを置いた同人誌であったため、このサイトでも「学習」に重きを置く予定である。

目標とするのは\sphinxstylestrong{幅広い分野を取り扱い、数式や理論も書いていて、実装を掲載しているサイト}である。実装も線形代数の演算は許容するが、できるだけブラックボックスな演算を無くし、「手を動かして学ぶ」を基本とする。また、可読性が高く、高速に実行出来て研究にも使える実装が望ましい (もちろん、現時点でこれが実現できているとは思わない)。

問題はサイトで使用するプログラミング言語であった。一体何を使えばいいだろうか？神経科学ではMATLABがよく使われているし、関連分野の機械学習ではPythonがよく使われる。ただ、MATLABはライセンスが必要だし、Pythonが数値計算に向いているとは思えない。高速に実行したいし、これらのユーザーが簡単に移行できる言語を考えたとき、\sphinxhref{https://julialang.org/}{Julia}が望ましいという結論に至った。

\sphinxhref{https://www.geidai.ac.jp/~marui/julialang/why\_we\_created\_julia/index.html}{欲張りな言語であるJulia}は欲張りなこのサイトの理念に合致している。Juliaはまだ発展途上ではあるが、ユーザーも増えており、このサイトが一通り完成するころ (3～5年後?) にはかなり良い環境が整っているはずである。

サイトの作成に関しては\sphinxhref{https://jupyterbook.org/intro.html}{Jupyter book}を用いている。Jupyter bookはJupyter notebookやmarkdownをちゃんとしたサイトの形式に変換してくれ、大変便利である。

内容に関しての疑問点や指摘に関しては、各記事末尾のコメント欄からお願いしたい (\sphinxhref{https://utteranc.es/}{utterances.es}を用いているためGitHubアカウントが必要である)。他に、もしこのサイトに協力してくれるという方がいれば、Twitter(\sphinxhref{https://twitter.com/tak\_yamm}{@tak\_yamm})でDMをしていただければ幸いである。

なお、知り合いの方向けに言っておくと、このサイトはあくまで趣味であって、勉強や研究や仕事の方を優先するので安心してほしい。などという言葉を試験3日前に書いている私であった。

2020年7月24日山本 拓都


\section{1. はじめに}
\label{\detokenize{1_intro:id1}}\label{\detokenize{1_intro::doc}}

\subsection{まえがき}
\label{\detokenize{preface:id1}}\label{\detokenize{preface::doc}}
何故このサイトを作ろうと思ったか、というと鬱憤が溜まっていたためである。\sphinxstylestrong{物理シミュレーションの本はあるのに神経科学のシミュレーションの本は何故無いのか？} もちろん、洋書を探せば何冊かあるが、ブラックボックス的なライブラリを用いたり、幅広い分野の内容を扱っていなかったりする。数理モデルを扱う洋書・和書の名著は多数あるが、実装まで記述してくれない本がほとんどである。そのくせ、関連分野のニューラルネットワークの本は腐るほど出版されている (というと著者の方々に大変失礼だが)。有名な論文でも再現するためのパラメータの記載が不足していたり、著者実装がなかったり、あっても古いMATLAB実装は動かない。シミュレーションを実施する講義を受けたいと思っても医学部のカリキュラムで試験と被る。潜りも難しい。所属している研究室は理論や数値計算がメインではないので研究室の「秘伝のタレ」があるわけでもない。Twitterで流れてくる講義の情報を見ても「どうして資料を公開してくれないんだ」と嘆きながら枕を濡らすしかない。

「本が無いなら本を書けばいいじゃない」という言葉は…今作った言葉だが、一先ずこれまでに自分が書いてきたコードやノート (\sphinxhref{https://salad-bowl-of-knowledge.github.io/hp/}{ブログ}、\sphinxhref{https://compneuro-julia.github.io/\#spiking-neural-networks}{同人誌 (SNN本)}などを含む)をまとめるだけでも良い資料になるのではないか、という中々思い上がった考えからこのサイトを始めた (目次を見るとかなり無謀なことが察せられる。何事も勢いで初めてはいけない)。このサイトの元となったSNN本が「SNNを如何に学習させるか」ということに重きを置いた同人誌であったため、このサイトでも「学習」に重きを置く予定である。

目標とするのは\sphinxstylestrong{幅広い分野を取り扱い、数式や理論も書いていて、実装を掲載しているサイト}である。実装も線形代数の演算は許容するが、できるだけブラックボックスな演算を無くし、「手を動かして学ぶ」を基本とする。また、可読性が高く、高速に実行出来て研究にも使える実装が望ましい (もちろん、現時点でこれが実現できているとは思わない)。

問題はサイトで使用するプログラミング言語であった。一体何を使えばいいだろうか？神経科学ではMATLABがよく使われているし、関連分野の機械学習ではPythonがよく使われる。ただ、MATLABはライセンスが必要だし、Pythonが数値計算に向いているとは思えない。高速に実行したいし、これらのユーザーが簡単に移行できる言語を考えたとき、\sphinxhref{https://julialang.org/}{Julia}が望ましいという結論に至った。

\sphinxhref{https://www.geidai.ac.jp/~marui/julialang/why\_we\_created\_julia/index.html}{欲張りな言語であるJulia}は欲張りなこのサイトの理念に合致している。Juliaはまだ発展途上ではあるが、ユーザーも増えており、このサイトが一通り完成するころ (3～5年後?) にはかなり良い環境が整っているはずである。

サイトの作成に関しては\sphinxhref{https://jupyterbook.org/intro.html}{Jupyter book}を用いている。Jupyter bookはJupyter notebookやmarkdownをちゃんとしたサイトの形式に変換してくれ、大変便利である。

内容に関しての疑問点や指摘に関しては、各記事末尾のコメント欄からお願いしたい (\sphinxhref{https://utteranc.es/}{utterances.es}を用いているためGitHubアカウントが必要である)。他に、もしこのサイトに協力してくれるという方がいれば、Twitter(\sphinxhref{https://twitter.com/tak\_yamm}{@tak\_yamm})でDMをしていただければ幸いである。

なお、知り合いの方向けに言っておくと、このサイトはあくまで趣味であって、勉強や研究や仕事の方を優先するので安心してほしい。などという言葉を試験3日前に書いている私であった。

2020年7月24日山本 拓都


\subsection{1.2 記号の表記}
\label{\detokenize{notation:id1}}\label{\detokenize{notation::doc}}
このサイトは次のような記号表記を用いている。
\begin{itemize}
\item {} 
実数全体を\(\mathbb{R}\), 複素数全体は\(\mathbb{C}\)と表記する。

\item {} 
スカラーは小文字・斜体で \(x\) のように表記する。

\item {} 
ベクトルは小文字・立体・太字で \(\mathbf{x}\) のように表記し、列ベクトル (縦ベクトル) として扱う。

\item {} 
行列やテンソルは大文字・立体・太字で \(\mathbf{X}\) のように表記する。

\item {} 
\(n\times 1\)の実ベクトルの集合を \(\mathbb{R}^n\), \(n\times m\) の実行列の集合を \(\mathbb{R}^{n\times m}\)と表記する。

\item {} 
行列 \(\mathbf{X}\) の置換は \(\mathbf{X}^T\)と表記する。ベクトルの要素を表す場合は \(\mathbf{x} = (x_1, x_2,\cdots, x_n)^T\)のように表記する。

\item {} 
単位行列を \(\mathbf{I}\) と表記する。

\item {} 
ゼロベクトルは \(\mathbf{0}\) , 要素が全て1のベクトルは \(\mathbf{1}\) と表記する。

\item {} 
\(e\)を自然対数の底とし、指数関数を \(e^x=\exp(x)\)と表記する。また、自然対数を \(\ln(x)\)と表記する。

\item {} 
平均 \(\mu\), 標準偏差 \(\sigma\) の正規分布を \(\mathcal{N}(\mu, \sigma^2)\) と表記する。

\end{itemize}


\subsection{2.2 Hodgkin\sphinxhyphen{}Huxleyモデル}
\label{\detokenize{2-2_hodgkinhuxley:hodgkin-huxley}}\label{\detokenize{2-2_hodgkinhuxley::doc}}

\subsubsection{2.2.1 Hodgkin\sphinxhyphen{}Huxleyモデルにおける膜の等価回路モデル}
\label{\detokenize{2-2_hodgkinhuxley:id1}}
\sphinxstylestrong{Hodgkin\sphinxhyphen{}Huxleyモデル} (HH モデル)は, A.L. HodgkinとA.F. Huxleyによって1952年に考案されたニューロンの膜興奮を表すモデルである (\sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/}{Hodgkin \& Huxley, 1952})。Hodgkinらはヤリイカの巨大神経軸索に対する\sphinxstylestrong{電位固定法}(voltage\sphinxhyphen{}clamp)を用いた実験を行い, 実験から得られた観測結果を元にモデルを構築した。

HHモデルには等価な電気回路モデルがあり, \sphinxstylestrong{膜の並列等価回路モデル} (parallel conductance model)と呼ばれている。膜の並列等価回路モデルでは, ニューロンの細胞膜をコンデンサ, 細胞膜に埋まっているイオンチャネルを可変抵抗 (動的に変化する抵抗) として置き換える。

\sphinxstylestrong{イオンチャネル} (ion channel)は特定のイオン(例えばナトリウムイオンやカリウムイオンなど)を選択的に通す膜輸送体の一種である。それぞれのイオンの種類において, 異なるイオンチャネルがある (同じイオンでも複数の種類のイオンチャネルがある)。また, イオンチャネルにはイオンの種類に応じて異なる\sphinxstylestrong{コンダクタンス}(抵抗の逆数で電流の「流れやすさ」を意味する)と\sphinxstylestrong{平衡電位}(equilibrium potential)がある。HHモデルでは, ナトリウム(Na\(^{+}\))チャネル, カリウム(K\(^{+}\))チャネル, 漏れ電流(leak current)のイオンチャネルを仮定する。漏れ電流のイオンチャネルは当時特定できなかったチャネルで, 膜から電流が漏れ出すチャネルを意味する。なお, 現在では漏れ電流の多くはCl\(^{-}\)イオン(chloride ion)によることが分かっている。

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{parallel_conductance_model}.JPG}
\caption{Hodgkin\sphinxhyphen{}Huxleyモデルの膜の等価回路モデル}\label{\detokenize{2-2_hodgkinhuxley:parallel-conductance-model}}\end{figure}

それでは, 等価回路モデルを用いて電位変化の式を立ててみよう。上図において, \(C_m\)は細胞膜のキャパシタンス(膜容量), \(I_{m}(t)\)は細胞膜を流れる電流(外部からの入力電流), \(I_\text{Cap}(t)\)は膜のコンデンサを流れる電流, \(I_\text{Na}(t)\)及び \(I_K(t)\)はそれぞれナトリウムチャネルとカリウムチャネルを通って膜から流出する電流, \(I_\text{L}(t)\)は漏れ電流である。このとき,
\begin{equation*}
\begin{split}
I_{m}(t)=I_\text{Cap}(t)+I_\text{Na}(t)+I_\text{K}(t)+I_\text{L}(t)    
\end{split}
\end{equation*}
という仮定をしている。

膜電位を\(V(t)\)とすると, Kirchhoffの第二法則 (Kirchhoff’s Voltage Law)より,
\begin{equation*}
\begin{split}
\underbrace{C_m\frac {dV(t)}{dt}}_{I_\text{Cap} (t)}=I_{m}(t)-I_\text{Na}(t)-I_\text{K}(t)-I_\text{L}(t)
\end{split}
\end{equation*}
となる。Hodgkinらはチャネル電流\(I_\text{Na}, I_K, I_\text{L}\)が従う式を実験的に求めた。
\begin{equation*}
\begin{split}
\begin{aligned}
I_\text{Na}(t) &= g_{\text{Na}}\cdot m^{3}h(V-E_{\text{Na}})\\
I_\text{K}(t) &= g_{\text{K}}\cdot n^{4}(V-E_{\text{K}})\\
I_\text{L}(t) &= g_{\text{L}}(V-E_{\text{L}})
\end{aligned}
\end{split}
\end{equation*}
ただし, \(g_{\text{Na}}, g_{\text{K}}\)はそれぞれNa\(^+\), K\(^+\)の最大コンダクタンスである。\(g_{\text{L}}\)はオームの法則に従うコンダクタンスで, Lコンダクタンスは時間的に変化はしないと仮定する。また, \(m\)はNa\(^+\)コンダクタンスの活性化パラメータ, \(h\)はNa\(^+\)コンダクタンスの不活性化パラメータ, \(n\)はK\(^+\)コンダクタンスの活性化パラメータであり, ゲートの開閉確率を表している。よって, HHモデルの状態は\(V, m, h, n\)の4変数で表される。これらの変数は以下の\(x\)を\(m, n, h\)に置き換えた3つの微分方程式に従う。
\begin{equation*}
\begin{split}
\frac{dx}{dt}=\alpha_{x}(V)(1-x)-\beta_{x}(V)x
\end{split}
\end{equation*}
ただし, \(V\)の関数である\(\alpha_{x}(V),\ \beta_{x}(V)\)は\(m, h, n\)によって異なり, 次の6つの式に従う。
\begin{equation*}
\begin{split}
\begin{array}{ll}
\alpha_{m}(V)=\dfrac {0.1(25-V)}{\exp \left[(25-V)/10\right]-1}, &\beta_{m}(V)=4\exp {(-V/18)}\\
\alpha_{h}(V)=0.07\exp {(-V/20)}, & \beta_{h}(V)={\dfrac{1}{\exp {\left[(30-V)/10 \right]}+1}}\\
\alpha_{n}(V)={\dfrac {0.01(10-V)}{\exp {\left[(10-V)/10\right]}-1}},& \beta_{n}(V)=0.125\exp {(-V/80)} 
\end{array}
\end{split}
\end{equation*}
なお、この式は6.3℃の条件下においてイカの巨大軸索の活動から得たデータを用いて導かれたものであることに注意しよう。


\subsubsection{2.2.2 Hodgkin\sphinxhyphen{}Huxley モデルの定義}
\label{\detokenize{2-2_hodgkinhuxley:id2}}
これまでに説明した式を用いてHHモデルを実装する。まず必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\end{sphinxVerbatim}

変更しない定数を保持する \sphinxcode{\sphinxupquote{struct}} の \sphinxcode{\sphinxupquote{HHParameter}} と, 変数を保持する \sphinxcode{\sphinxupquote{mutable struct}} の \sphinxcode{\sphinxupquote{HH}} を作成する。\sphinxcode{\sphinxupquote{v, m, h, n}} はHH modelの4変数だが, \sphinxcode{\sphinxupquote{r}} はpre\sphinxhyphen{}synaptic dynamicsを表す変数である。詳細は3章で解説する。 定数は次のように設定する。

\textbackslash{}begin\{align*\}
C\_m=1.0, g\_\{\textbackslash{}text\{Na\}\}=120, g\_\{\textbackslash{}text\{K\}\}=36, g\_\{\textbackslash{}text\{L\}\}=0.3\textbackslash{}
E\_\{\textbackslash{}text\{Na\}\}=50.0, E\_\{\textbackslash{}text\{K\}\}=\sphinxhyphen{}77, E\_\{\textbackslash{}text\{L\}\}=\sphinxhyphen{}54.387
\textbackslash{}end\{align*\}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{Cm}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{c+c1}{\PYGZsh{} 膜容量(uF/cm\PYGZca{}2)}
    \PYG{n}{gNa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{120.0} \PYG{c+c1}{\PYGZsh{} Na+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{36.0} \PYG{c+c1}{\PYGZsh{} K+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.3} \PYG{c+c1}{\PYGZsh{} 漏れイオンの最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{ENa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{50.0} \PYG{c+c1}{\PYGZsh{} Na+ の平衡電位(mV)}
    \PYG{n}{EK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{77.0} \PYG{c+c1}{\PYGZsh{} K+ の平衡電位(mV)}
    \PYG{n}{EL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{54.387} \PYG{c+c1}{\PYGZsh{}漏れイオンの平衡電位(mV)}
    \PYG{n}{tr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.5} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{td}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{8.0} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{invtr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{tr}
    \PYG{n}{invtd}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{td}
    \PYG{n}{v0}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{20.0} \PYG{c+c1}{\PYGZsh{} mV}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{HH}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{HHParameter} \PYG{o}{=} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{N}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16}
    \PYG{n}{v}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{65.0}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{m}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{h}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{n}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.32}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{r}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

次に変数を更新する関数\sphinxcode{\sphinxupquote{update!}}を書く。ソルバーとしては陽的Euler法または4次のRunge\sphinxhyphen{}Kutta法を用いる。以下ではEuler法を用いている。Juliaではforループを用いて1つのニューロンごとにパラメータを更新する方がベクトルを用いるよりも高速である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function update!(variable::HH, param::HHParameter, I::Vector, dt)
    @unpack N, v, m, h, n, r = variable
    @unpack Cm, gNa, gK, gL, ENa, EK, EL, tr, td, invtr, invtd, v0 = param
    @inbounds for i = 1:N
        m[i] += dt * ((0.1(v[i]+40.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+40.0))))*(1.0 \PYGZhy{} m[i]) \PYGZhy{} 4.0exp(\PYGZhy{}(v[i]+65.0) / 18.0)*m[i])
        h[i] += dt * ((0.07exp(\PYGZhy{}0.05(v[i]+65.0)))*(1.0 \PYGZhy{} h[i]) \PYGZhy{} 1.0/(1.0 + exp(\PYGZhy{}0.1(v[i]+35.0)))*h[i])
        n[i] += dt * ((0.01(v[i]+55.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+55.0))))*(1.0 \PYGZhy{} n[i]) \PYGZhy{} (0.125exp(\PYGZhy{}0.0125(v[i]+65)))*n[i])
        v[i] += dt / Cm * (I[i] \PYGZhy{} gNa * m[i]\PYGZca{}3 * h[i] * (v[i] \PYGZhy{} ENa) \PYGZhy{} gK * n[i]\PYGZca{}4 * (v[i] \PYGZhy{} EK) \PYGZhy{} gL * (v[i] \PYGZhy{} EL))
        r[i] += dt * ((invtr \PYGZhy{} invtd) * (1.0 \PYGZhy{} r[i])/(1.0 + exp(\PYGZhy{}v[i] + v0)) \PYGZhy{} r[i] * invtd)
    end
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
update! (generic function with 1 method)
\end{sphinxVerbatim}


\subsubsection{2.2.3 Hodgkin\sphinxhyphen{}Huxleyモデルのシミュレーションの実行}
\label{\detokenize{2-2_hodgkinhuxley:id3}}
いくつかの定数を設定してシミュレーションを実行する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * ((t .\PYGZgt{} 50) \PYGZhy{} (t .\PYGZgt{} 200)) + 35f0 * ((t .\PYGZgt{} 250) \PYGZhy{} (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} 記録用
varr = zeros(Float32, nt, N)
gatearr = zeros(Float32, nt, 3, N)

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr[i, :] = neurons.v
    gatearr[i, 1, :] = neurons.m
    gatearr[i, 2, :] = neurons.h
    gatearr[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.217901 seconds (1.36 M allocations: 46.979 MiB, 3.69\PYGZpc{} gc time)
\end{sphinxVerbatim}

結果を表示するために \sphinxcode{\sphinxupquote{Plots}}を読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Plots}
\end{sphinxVerbatim}

ニューロンの膜電位 \sphinxcode{\sphinxupquote{v}}, ゲート変数 \sphinxcode{\sphinxupquote{m, h, n}}, 刺激電流 \sphinxcode{\sphinxupquote{I}}の描画をする。入力電流の単位は \(\mu\text{A/cm}^2\)である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

次項で用いるために発火回数を求める。\sphinxcode{\sphinxupquote{bitwise and}}を用いると楽である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Num. of spikes : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}spikes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Num. of spikes : 27
\end{sphinxVerbatim}

50msから200msまでで11回, 250msから400msまでで16回発火しているので発火回数は計27回であり、この結果は正しい。


\subsubsection{2.2.4 Frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve}
\label{\detokenize{2-2_hodgkinhuxley:frequency-current-f-i-curve}}
この項ではHodgkin\sphinxhyphen{}Huxleyモデルにおいて、入力電流に対する発火率がどのように変化するかを調べる。次のコードのように入力電流を徐々に増加させたときの発火率を見てみよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 1000 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps

N = 100 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
maxcurrent = 30
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = Array\PYGZob{}Float32\PYGZcb{}(range(1,maxcurrent,length=N)) \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr\PYGZus{}fi = zeros(Float32, nt, N)

\PYGZsh{} simulation
for i = 1:nt
    update!(neurons, neurons.param, I[:], dt)
    varr\PYGZus{}fi[i, :] = neurons.v
end
\end{sphinxVerbatim}

発火率を計算して結果を描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{rate} \PYG{o}{=} \PYG{n}{num\PYGZus{}spikes}\PYG{o}{/}\PYG{n}{T}\PYG{o}{*}\PYG{l+m+mf}{1e3}

\PYG{n}{plot}\PYG{p}{(}\PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{rate}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Input current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Firing rate (Hz)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{400}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

このような曲線を\sphinxstylestrong{frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve} (または neuronal input/output (I/O) curve)と呼ぶ。


\subsubsection{2.2.5  抑制後リバウンド (Postinhibitory rebound; PIR)}
\label{\detokenize{2-2_hodgkinhuxley:postinhibitory-rebound-pir}}
ニューロンは電流が流入することで膜電位が変化し, 膜電位がある一定の閾値を超えると発火が起こる, というのはニューロンの活動電位発生についての典型的な説明である。それではHHモデルの膜電位閾値はどのくらいの値になるのだろうか。答えは「\sphinxstylestrong{膜電位閾値は一定ではない}」である。それを示す現象として \sphinxstylestrong{抑制後リバウンド} (Postinhibitory rebound; PIR)がある。この時生じる発火を\sphinxstylestrong{リバウンド発火} (rebound spikes)
と呼ぶ。抑制後リバウンドは過分極性の電流の印加を止めた際に膜電位が静止膜電位に回復するのみならず, さらに脱分極をして発火をするという現象である。この現象が生じる要因として
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{アノーダルブレイク} (anodal break, またはanode break excitation; ABE)

\item {} 
遅いT型カルシウム電流 (slow T\sphinxhyphen{}type calcium current)

\end{enumerate}

がある (\sphinxhref{https://pubmed.ncbi.nlm.nih.gov/15324089/}{Chik et al., 2004})。HH モデルはこのうちアノーダルブレイクを再現できるため, シミュレーションによりどのような現象か確認してみよう。これは入力電流を変更するだけで行える。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * (\PYGZhy{}(t .\PYGZgt{} 50) + (t .\PYGZgt{} 200)) + 20f0 * (\PYGZhy{}(t .\PYGZgt{} 250) + (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr2 = zeros(Float32, nt, N)
gatearr2 = zeros(Float32, nt, 3, N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr2[i, :] = neurons.v
    gatearr2[i, 1, :] = neurons.m
    gatearr2[i, 2, :] = neurons.h
    gatearr2[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.074358 seconds (848.89 k allocations: 19.134 MiB)
\end{sphinxVerbatim}

結果は次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Injection}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

なぜこのようなことが起こるか, というと過分極の状態から静止膜電位へと戻る際にNa\(^+\)チャネルが活性化 (Na\(^+\)チャネルの活性化パラメータ\(m\)が増加し, 不活性化パラメータ\(h\)が減少)し, 膜電位が脱分極することで再度Na\(^+\)チャネルが活性化する, というポジティブフィードバック過程(\sphinxstylestrong{自己再生的過程})に突入するためである (もちろん, この過程は通常の活動電位発生のメカニズムである)。 この際, 発火に必要な閾値が膜電位の低下に応じて下がった, ということもできる。

このように膜電位閾値は一定ではない。しかし, この後の節で紹介するLIFモデルなどでは簡略化のためにif文を用い, 膜電位閾値を超えたから発火, というものもある。実際には違うということを頭の片隅に残しながら読み進めることを推奨する。

\begin{sphinxadmonition}{note}{Note:}
PIRに関連する現象として抑制後促通 (Postinhibitory facilitation; PIF)がある。これは抑制入力の後に興奮入力がある一定の時間内で入ると発火が起こるという現象である (\sphinxhref{http://www.brain.riken.jp/en/summer/prev/2006/files/j\_rinzel04.pdf}{Dolda et al., 2006}, \sphinxhref{https://link.springer.com/referenceworkentry/10.1007\%2F978-1-4614-7320-6\_152-1}{Dodla, 2014})。
\end{sphinxadmonition}


\subsection{3.1 シナプス伝達}
\label{\detokenize{3-1_synapse:id1}}\label{\detokenize{3-1_synapse::doc}}
\begin{sphinxadmonition}{note}{Note:}
内容は大幅に追記が必要。
\end{sphinxadmonition}

スパイクが生じたことによる膜電位変化は軸索を伝播し, \sphinxstylestrong{シナプス}という構造により, 次のニューロンへと興奮が伝わる. このときの伝達の仕組みとして, シナプスには\sphinxstylestrong{化学シナプス}(chemical synapse)とGap junctionによる\sphinxstylestrong{電気シナプス}(electrical synapse)がある。

化学シナプスの場合, シナプス前膜からの\sphinxstylestrong{神経伝達物質}の放出, シナプス後膜の受容体への神経伝達物質の結合, イオンチャネル開口による\sphinxstylestrong{シナプス後電流}(postsynaptic current; PSC)の発生, という過程が起こる。かなり簡略化して書いたが, 実際にはかなりの過程を含くむ。しかし, これらの過程を全てモデル化するのは計算量がかなり大きくなるので, 基本的には簡易的な現象論的なモデルを用いる。

このように, シナプス前細胞のスパイク列(spike train)は次のニューロンにそのまま伝わるのではなく, ある種の時間的フィルターをかけられて伝わる。このフィルターを\sphinxstylestrong{シナプスフィルター}(synaptic filter)と呼ぶ。3章では, このようにシナプス前細胞で生じた発火が, シナプス後細胞の膜電位に与える過程のモデルについて説明する。


\subsection{11.1 ベイズ統計の基礎}
\label{\detokenize{11-1_bayes_statistics:id1}}\label{\detokenize{11-1_bayes_statistics::doc}}
この節では本章で用いるベイズ統計の基礎的概念の説明を行う。

\begin{sphinxadmonition}{note}{Note:}
悪いこと言わないので\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/index-j.html}{渡辺澄夫先生のHP}の講義録、特に\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/joho-gakushu6.html}{ベイズ統計入門}を読もう。
\end{sphinxadmonition}


\subsection{JuliaのTips集}
\label{\detokenize{tips:juliatips}}\label{\detokenize{tips::doc}}
このページはJuliaでの実装におけるTips (詰まったところの解決策)をまとめたものである。体系的にまとまってはいない。


\subsubsection{1. 関数名の!記号}
\label{\detokenize{tips:id1}}
単なる\sphinxstylestrong{慣習}として関数への入力を変更する場合に!を付ける。

関数内で配列を変更する場合には注意が必要である。以下に入力された配列を同じサイズの要素1の配列で置き換える、ということを目的として書かれた2つの関数がある。違いは\sphinxcode{\sphinxupquote{v}}の後に\sphinxcode{\sphinxupquote{{[}:{]}}}としているかどうかである。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function wrong!(A::Array)
    a = ones(size(a))
end

function right!(a::Array)
    a[:] = ones(size(a))
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
right! (generic function with 2 methods)
\end{sphinxVerbatim}

実行すると\sphinxcode{\sphinxupquote{wrong!}}の場合には入力された配列が変更されていないことがわかる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
using Random
v = rand(2, 2)
print(\PYGZdq{}v : \PYGZdq{}, v)

wrong!(v)
print(\PYGZdq{}\PYGZbs{}nwrong : \PYGZdq{}, v)

right!(v)
print(\PYGZdq{}\PYGZbs{}nright : \PYGZdq{}, v)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
v : [0.42692589476552056 0.34730711983824536; 0.9208034639794132 0.41699915079158134]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+ne}{UndefVarError}: wrong! not defined

\PYG{n+ne}{Stacktrace}:
 \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{n}{top}\PYG{o}{\PYGZhy{}}\PYG{n}{level} \PYG{n}{scope} \PYG{n}{at} \PYG{n}{In}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}\PYG{l+m+mi}{5}
 \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{n}{include\PYGZus{}string}\PYG{p}{(}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Function}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{Module}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{)} \PYG{n}{at} \PYG{o}{.}\PYGZbs{}\PYG{n}{loading}\PYG{o}{.}\PYG{n}{jl}\PYG{p}{:}\PYG{l+m+mi}{1091}
\end{sphinxVerbatim}


\subsubsection{2. 配列の1次元化}
\label{\detokenize{tips:id2}}
配列を一次元化(flatten)する方法。まずは3次元配列を作成する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2×2 Array\PYGZob{}Float64,3\PYGZcb{}:
[:, :, 1] =
 0.997107  0.0827965
 0.394606  0.847174

[:, :, 2] =
 0.0305417  0.448929
 0.270553   0.601419
\end{sphinxVerbatim}

用意されている\sphinxcode{\sphinxupquote{flatten}}を素直に用いると次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{Base}\PYG{n+nn}{.}\PYG{n+nn}{Iterators}\PYG{p}{:} \PYG{n}{flatten}
\PYG{n}{collect}\PYG{p}{(}\PYG{n}{flatten}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}

ただし、単に\sphinxcode{\sphinxupquote{B{[}:{]}}}とするだけでもよい。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}


\subsubsection{3. 行列の行・列ごとの正規化}
\label{\detokenize{tips:id3}}
シミュレーションにおいてニューロン間の重み行列を行あるいは列ごとに正規化 (weight normalization)する場合がある。これは各ニューロンへの入力の大きさを同じにする働きや重みの発散を防ぐ役割がある。以下では行ごとの和を1にする。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.166781  0.0552745  0.973622
 0.100336  0.753766   0.370005
 0.378222  0.165159   0.471515
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Wnormed} \PYG{o}{=} \PYG{n}{W} \PYG{o}{.}\PYG{o}{/} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{W}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.258439  0.0567384  0.536389
 0.155479  0.773729   0.203843
 0.586082  0.169533   0.259767
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{Wnormed}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1.0 1.0 1.0]
\end{sphinxVerbatim}


\subsubsection{4. 行列の結合 (concatenate)}
\label{\detokenize{tips:concatenate}}
行列の結合はMATLABに近い形式で行うことができる。まず、2つの行列A, Bを用意する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{l+m+mi}{3} \PYG{l+m+mi}{4}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{4} \PYG{l+m+mi}{5} \PYG{l+m+mi}{6}\PYG{p}{;} \PYG{l+m+mi}{7} \PYG{l+m+mi}{8} \PYG{l+m+mi}{9}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×3 Array\PYGZob{}Int64,2\PYGZcb{}:
 4  5  6
 7  8  9
\end{sphinxVerbatim}


\paragraph{4.1 水平結合 (Horizontal concatenation)}
\label{\detokenize{tips:horizontal-concatenation}}
\sphinxcode{\sphinxupquote{hcat}}を使うやり方と、\sphinxcode{\sphinxupquote{{[} {]}}}を使うやり方がある。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H1} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,}\PYG{n}{B}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

なお、MATLABのように次のようにすると正しく結合はされない。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H3} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{,} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2\PYGZhy{}element Array\PYGZob{}Array\PYGZob{}Int64,2\PYGZcb{},1\PYGZcb{}:
 [1 2; 3 4]
 [4 5 6; 7 8 9]
\end{sphinxVerbatim}


\paragraph{4.2 垂直結合 (Vertical concatenation)}
\label{\detokenize{tips:vertical-concatenation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V1} \PYG{o}{=} \PYG{n}{vcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{V2} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;}\PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×4 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  1  2
 3  4  3  4
 4  7  4  7
 5  8  5  8
 6  9  6  9
\end{sphinxVerbatim}


\subsubsection{5. 配列に新しい軸を追加}
\label{\detokenize{tips:id4}}
要はnumpyでの\sphinxcode{\sphinxupquote{A{[}None, :{]}}}や\sphinxcode{\sphinxupquote{A{[}np.newaxis, :{]}}}のようなことがしたい場合。やや面倒だが、\sphinxcode{\sphinxupquote{reshape}}を使うか、\sphinxcode{\sphinxupquote{{[}CartesianIndex(){]}}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.3487808667518093
 0.717776723811032
 0.3369916406749496
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{newaxis} \PYG{o}{=} \PYG{p}{[}\PYG{n}{CartesianIndex}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{v1} \PYG{o}{=} \PYG{n}{v}\PYG{p}{[}\PYG{n}{newaxis}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.348781  0.717777  0.336992
\end{sphinxVerbatim}


\subsubsection{6. Array\{Array\{Float64, x\},1\}をArray\{Float64, x+1\}に変換}
\label{\detokenize{tips:array-array-float64-x-1-array-float64-x-1}}
numpyでは\sphinxcode{\sphinxupquote{array({[}matrix for i in range(){]})}}などを用いると、1次元配列のリストを2次元配列に変換できた。Juliaでも同様にする場合は\sphinxcode{\sphinxupquote{hcat(...)}}や\sphinxcode{\sphinxupquote{cat(...)}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,1\PYGZcb{},1\PYGZcb{}
Size : (5,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A2} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : LinearAlgebra.Adjoint\PYGZob{}Float64,Array\PYGZob{}Float64,2\PYGZcb{}\PYGZcb{}
Size : (5, 3)
\end{sphinxVerbatim}

以下は多次元配列の場合。\sphinxcode{\sphinxupquote{cat(...)}}で配列を結合し、\sphinxcode{\sphinxupquote{permitedims}}で転置する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,3\PYGZcb{},1\PYGZcb{}
Size : (6,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B2} \PYG{o}{=} \PYG{n}{permutedims}\PYG{p}{(}\PYG{n}{cat}\PYG{p}{(}\PYG{n}{B1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Float64,4\PYGZcb{}
Size : (6, 3, 4, 5)
\end{sphinxVerbatim}


\subsection{1.2 記号の表記}
\label{\detokenize{notation:id1}}\label{\detokenize{notation::doc}}
このサイトは次のような記号表記を用いている。
\begin{itemize}
\item {} 
実数全体を\(\mathbb{R}\), 複素数全体は\(\mathbb{C}\)と表記する。

\item {} 
スカラーは小文字・斜体で \(x\) のように表記する。

\item {} 
ベクトルは小文字・立体・太字で \(\mathbf{x}\) のように表記し、列ベクトル (縦ベクトル) として扱う。

\item {} 
行列やテンソルは大文字・立体・太字で \(\mathbf{X}\) のように表記する。

\item {} 
\(n\times 1\)の実ベクトルの集合を \(\mathbb{R}^n\), \(n\times m\) の実行列の集合を \(\mathbb{R}^{n\times m}\)と表記する。

\item {} 
行列 \(\mathbf{X}\) の置換は \(\mathbf{X}^T\)と表記する。ベクトルの要素を表す場合は \(\mathbf{x} = (x_1, x_2,\cdots, x_n)^T\)のように表記する。

\item {} 
単位行列を \(\mathbf{I}\) と表記する。

\item {} 
ゼロベクトルは \(\mathbf{0}\) , 要素が全て1のベクトルは \(\mathbf{1}\) と表記する。

\item {} 
\(e\)を自然対数の底とし、指数関数を \(e^x=\exp(x)\)と表記する。また、自然対数を \(\ln(x)\)と表記する。

\item {} 
平均 \(\mu\), 標準偏差 \(\sigma\) の正規分布を \(\mathcal{N}(\mu, \sigma^2)\) と表記する。

\end{itemize}


\section{2. 神経細胞のモデル}
\label{\detokenize{2_intro:id1}}\label{\detokenize{2_intro::doc}}

\subsection{まえがき}
\label{\detokenize{preface:id1}}\label{\detokenize{preface::doc}}
何故このサイトを作ろうと思ったか、というと鬱憤が溜まっていたためである。\sphinxstylestrong{物理シミュレーションの本はあるのに神経科学のシミュレーションの本は何故無いのか？} もちろん、洋書を探せば何冊かあるが、ブラックボックス的なライブラリを用いたり、幅広い分野の内容を扱っていなかったりする。数理モデルを扱う洋書・和書の名著は多数あるが、実装まで記述してくれない本がほとんどである。そのくせ、関連分野のニューラルネットワークの本は腐るほど出版されている (というと著者の方々に大変失礼だが)。有名な論文でも再現するためのパラメータの記載が不足していたり、著者実装がなかったり、あっても古いMATLAB実装は動かない。シミュレーションを実施する講義を受けたいと思っても医学部のカリキュラムで試験と被る。潜りも難しい。所属している研究室は理論や数値計算がメインではないので研究室の「秘伝のタレ」があるわけでもない。Twitterで流れてくる講義の情報を見ても「どうして資料を公開してくれないんだ」と嘆きながら枕を濡らすしかない。

「本が無いなら本を書けばいいじゃない」という言葉は…今作った言葉だが、一先ずこれまでに自分が書いてきたコードやノート (\sphinxhref{https://salad-bowl-of-knowledge.github.io/hp/}{ブログ}、\sphinxhref{https://compneuro-julia.github.io/\#spiking-neural-networks}{同人誌 (SNN本)}などを含む)をまとめるだけでも良い資料になるのではないか、という中々思い上がった考えからこのサイトを始めた (目次を見るとかなり無謀なことが察せられる。何事も勢いで初めてはいけない)。このサイトの元となったSNN本が「SNNを如何に学習させるか」ということに重きを置いた同人誌であったため、このサイトでも「学習」に重きを置く予定である。

目標とするのは\sphinxstylestrong{幅広い分野を取り扱い、数式や理論も書いていて、実装を掲載しているサイト}である。実装も線形代数の演算は許容するが、できるだけブラックボックスな演算を無くし、「手を動かして学ぶ」を基本とする。また、可読性が高く、高速に実行出来て研究にも使える実装が望ましい (もちろん、現時点でこれが実現できているとは思わない)。

問題はサイトで使用するプログラミング言語であった。一体何を使えばいいだろうか？神経科学ではMATLABがよく使われているし、関連分野の機械学習ではPythonがよく使われる。ただ、MATLABはライセンスが必要だし、Pythonが数値計算に向いているとは思えない。高速に実行したいし、これらのユーザーが簡単に移行できる言語を考えたとき、\sphinxhref{https://julialang.org/}{Julia}が望ましいという結論に至った。

\sphinxhref{https://www.geidai.ac.jp/~marui/julialang/why\_we\_created\_julia/index.html}{欲張りな言語であるJulia}は欲張りなこのサイトの理念に合致している。Juliaはまだ発展途上ではあるが、ユーザーも増えており、このサイトが一通り完成するころ (3～5年後?) にはかなり良い環境が整っているはずである。

サイトの作成に関しては\sphinxhref{https://jupyterbook.org/intro.html}{Jupyter book}を用いている。Jupyter bookはJupyter notebookやmarkdownをちゃんとしたサイトの形式に変換してくれ、大変便利である。

内容に関しての疑問点や指摘に関しては、各記事末尾のコメント欄からお願いしたい (\sphinxhref{https://utteranc.es/}{utterances.es}を用いているためGitHubアカウントが必要である)。他に、もしこのサイトに協力してくれるという方がいれば、Twitter(\sphinxhref{https://twitter.com/tak\_yamm}{@tak\_yamm})でDMをしていただければ幸いである。

なお、知り合いの方向けに言っておくと、このサイトはあくまで趣味であって、勉強や研究や仕事の方を優先するので安心してほしい。などという言葉を試験3日前に書いている私であった。

2020年7月24日山本 拓都


\subsection{1.2 記号の表記}
\label{\detokenize{notation:id1}}\label{\detokenize{notation::doc}}
このサイトは次のような記号表記を用いている。
\begin{itemize}
\item {} 
実数全体を\(\mathbb{R}\), 複素数全体は\(\mathbb{C}\)と表記する。

\item {} 
スカラーは小文字・斜体で \(x\) のように表記する。

\item {} 
ベクトルは小文字・立体・太字で \(\mathbf{x}\) のように表記し、列ベクトル (縦ベクトル) として扱う。

\item {} 
行列やテンソルは大文字・立体・太字で \(\mathbf{X}\) のように表記する。

\item {} 
\(n\times 1\)の実ベクトルの集合を \(\mathbb{R}^n\), \(n\times m\) の実行列の集合を \(\mathbb{R}^{n\times m}\)と表記する。

\item {} 
行列 \(\mathbf{X}\) の置換は \(\mathbf{X}^T\)と表記する。ベクトルの要素を表す場合は \(\mathbf{x} = (x_1, x_2,\cdots, x_n)^T\)のように表記する。

\item {} 
単位行列を \(\mathbf{I}\) と表記する。

\item {} 
ゼロベクトルは \(\mathbf{0}\) , 要素が全て1のベクトルは \(\mathbf{1}\) と表記する。

\item {} 
\(e\)を自然対数の底とし、指数関数を \(e^x=\exp(x)\)と表記する。また、自然対数を \(\ln(x)\)と表記する。

\item {} 
平均 \(\mu\), 標準偏差 \(\sigma\) の正規分布を \(\mathcal{N}(\mu, \sigma^2)\) と表記する。

\end{itemize}


\subsection{2.2 Hodgkin\sphinxhyphen{}Huxleyモデル}
\label{\detokenize{2-2_hodgkinhuxley:hodgkin-huxley}}\label{\detokenize{2-2_hodgkinhuxley::doc}}

\subsubsection{2.2.1 Hodgkin\sphinxhyphen{}Huxleyモデルにおける膜の等価回路モデル}
\label{\detokenize{2-2_hodgkinhuxley:id1}}
\sphinxstylestrong{Hodgkin\sphinxhyphen{}Huxleyモデル} (HH モデル)は, A.L. HodgkinとA.F. Huxleyによって1952年に考案されたニューロンの膜興奮を表すモデルである (\sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/}{Hodgkin \& Huxley, 1952})。Hodgkinらはヤリイカの巨大神経軸索に対する\sphinxstylestrong{電位固定法}(voltage\sphinxhyphen{}clamp)を用いた実験を行い, 実験から得られた観測結果を元にモデルを構築した。

HHモデルには等価な電気回路モデルがあり, \sphinxstylestrong{膜の並列等価回路モデル} (parallel conductance model)と呼ばれている。膜の並列等価回路モデルでは, ニューロンの細胞膜をコンデンサ, 細胞膜に埋まっているイオンチャネルを可変抵抗 (動的に変化する抵抗) として置き換える。

\sphinxstylestrong{イオンチャネル} (ion channel)は特定のイオン(例えばナトリウムイオンやカリウムイオンなど)を選択的に通す膜輸送体の一種である。それぞれのイオンの種類において, 異なるイオンチャネルがある (同じイオンでも複数の種類のイオンチャネルがある)。また, イオンチャネルにはイオンの種類に応じて異なる\sphinxstylestrong{コンダクタンス}(抵抗の逆数で電流の「流れやすさ」を意味する)と\sphinxstylestrong{平衡電位}(equilibrium potential)がある。HHモデルでは, ナトリウム(Na\(^{+}\))チャネル, カリウム(K\(^{+}\))チャネル, 漏れ電流(leak current)のイオンチャネルを仮定する。漏れ電流のイオンチャネルは当時特定できなかったチャネルで, 膜から電流が漏れ出すチャネルを意味する。なお, 現在では漏れ電流の多くはCl\(^{-}\)イオン(chloride ion)によることが分かっている。

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{parallel_conductance_model}.JPG}
\caption{Hodgkin\sphinxhyphen{}Huxleyモデルの膜の等価回路モデル}\label{\detokenize{2-2_hodgkinhuxley:parallel-conductance-model}}\end{figure}

それでは, 等価回路モデルを用いて電位変化の式を立ててみよう。上図において, \(C_m\)は細胞膜のキャパシタンス(膜容量), \(I_{m}(t)\)は細胞膜を流れる電流(外部からの入力電流), \(I_\text{Cap}(t)\)は膜のコンデンサを流れる電流, \(I_\text{Na}(t)\)及び \(I_K(t)\)はそれぞれナトリウムチャネルとカリウムチャネルを通って膜から流出する電流, \(I_\text{L}(t)\)は漏れ電流である。このとき,
\begin{equation*}
\begin{split}
I_{m}(t)=I_\text{Cap}(t)+I_\text{Na}(t)+I_\text{K}(t)+I_\text{L}(t)    
\end{split}
\end{equation*}
という仮定をしている。

膜電位を\(V(t)\)とすると, Kirchhoffの第二法則 (Kirchhoff’s Voltage Law)より,
\begin{equation*}
\begin{split}
\underbrace{C_m\frac {dV(t)}{dt}}_{I_\text{Cap} (t)}=I_{m}(t)-I_\text{Na}(t)-I_\text{K}(t)-I_\text{L}(t)
\end{split}
\end{equation*}
となる。Hodgkinらはチャネル電流\(I_\text{Na}, I_K, I_\text{L}\)が従う式を実験的に求めた。
\begin{equation*}
\begin{split}
\begin{aligned}
I_\text{Na}(t) &= g_{\text{Na}}\cdot m^{3}h(V-E_{\text{Na}})\\
I_\text{K}(t) &= g_{\text{K}}\cdot n^{4}(V-E_{\text{K}})\\
I_\text{L}(t) &= g_{\text{L}}(V-E_{\text{L}})
\end{aligned}
\end{split}
\end{equation*}
ただし, \(g_{\text{Na}}, g_{\text{K}}\)はそれぞれNa\(^+\), K\(^+\)の最大コンダクタンスである。\(g_{\text{L}}\)はオームの法則に従うコンダクタンスで, Lコンダクタンスは時間的に変化はしないと仮定する。また, \(m\)はNa\(^+\)コンダクタンスの活性化パラメータ, \(h\)はNa\(^+\)コンダクタンスの不活性化パラメータ, \(n\)はK\(^+\)コンダクタンスの活性化パラメータであり, ゲートの開閉確率を表している。よって, HHモデルの状態は\(V, m, h, n\)の4変数で表される。これらの変数は以下の\(x\)を\(m, n, h\)に置き換えた3つの微分方程式に従う。
\begin{equation*}
\begin{split}
\frac{dx}{dt}=\alpha_{x}(V)(1-x)-\beta_{x}(V)x
\end{split}
\end{equation*}
ただし, \(V\)の関数である\(\alpha_{x}(V),\ \beta_{x}(V)\)は\(m, h, n\)によって異なり, 次の6つの式に従う。
\begin{equation*}
\begin{split}
\begin{array}{ll}
\alpha_{m}(V)=\dfrac {0.1(25-V)}{\exp \left[(25-V)/10\right]-1}, &\beta_{m}(V)=4\exp {(-V/18)}\\
\alpha_{h}(V)=0.07\exp {(-V/20)}, & \beta_{h}(V)={\dfrac{1}{\exp {\left[(30-V)/10 \right]}+1}}\\
\alpha_{n}(V)={\dfrac {0.01(10-V)}{\exp {\left[(10-V)/10\right]}-1}},& \beta_{n}(V)=0.125\exp {(-V/80)} 
\end{array}
\end{split}
\end{equation*}
なお、この式は6.3℃の条件下においてイカの巨大軸索の活動から得たデータを用いて導かれたものであることに注意しよう。


\subsubsection{2.2.2 Hodgkin\sphinxhyphen{}Huxley モデルの定義}
\label{\detokenize{2-2_hodgkinhuxley:id2}}
これまでに説明した式を用いてHHモデルを実装する。まず必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\end{sphinxVerbatim}

変更しない定数を保持する \sphinxcode{\sphinxupquote{struct}} の \sphinxcode{\sphinxupquote{HHParameter}} と, 変数を保持する \sphinxcode{\sphinxupquote{mutable struct}} の \sphinxcode{\sphinxupquote{HH}} を作成する。\sphinxcode{\sphinxupquote{v, m, h, n}} はHH modelの4変数だが, \sphinxcode{\sphinxupquote{r}} はpre\sphinxhyphen{}synaptic dynamicsを表す変数である。詳細は3章で解説する。 定数は次のように設定する。

\textbackslash{}begin\{align*\}
C\_m=1.0, g\_\{\textbackslash{}text\{Na\}\}=120, g\_\{\textbackslash{}text\{K\}\}=36, g\_\{\textbackslash{}text\{L\}\}=0.3\textbackslash{}
E\_\{\textbackslash{}text\{Na\}\}=50.0, E\_\{\textbackslash{}text\{K\}\}=\sphinxhyphen{}77, E\_\{\textbackslash{}text\{L\}\}=\sphinxhyphen{}54.387
\textbackslash{}end\{align*\}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{Cm}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{c+c1}{\PYGZsh{} 膜容量(uF/cm\PYGZca{}2)}
    \PYG{n}{gNa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{120.0} \PYG{c+c1}{\PYGZsh{} Na+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{36.0} \PYG{c+c1}{\PYGZsh{} K+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.3} \PYG{c+c1}{\PYGZsh{} 漏れイオンの最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{ENa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{50.0} \PYG{c+c1}{\PYGZsh{} Na+ の平衡電位(mV)}
    \PYG{n}{EK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{77.0} \PYG{c+c1}{\PYGZsh{} K+ の平衡電位(mV)}
    \PYG{n}{EL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{54.387} \PYG{c+c1}{\PYGZsh{}漏れイオンの平衡電位(mV)}
    \PYG{n}{tr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.5} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{td}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{8.0} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{invtr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{tr}
    \PYG{n}{invtd}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{td}
    \PYG{n}{v0}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{20.0} \PYG{c+c1}{\PYGZsh{} mV}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{HH}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{HHParameter} \PYG{o}{=} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{N}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16}
    \PYG{n}{v}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{65.0}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{m}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{h}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{n}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.32}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{r}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

次に変数を更新する関数\sphinxcode{\sphinxupquote{update!}}を書く。ソルバーとしては陽的Euler法または4次のRunge\sphinxhyphen{}Kutta法を用いる。以下ではEuler法を用いている。Juliaではforループを用いて1つのニューロンごとにパラメータを更新する方がベクトルを用いるよりも高速である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function update!(variable::HH, param::HHParameter, I::Vector, dt)
    @unpack N, v, m, h, n, r = variable
    @unpack Cm, gNa, gK, gL, ENa, EK, EL, tr, td, invtr, invtd, v0 = param
    @inbounds for i = 1:N
        m[i] += dt * ((0.1(v[i]+40.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+40.0))))*(1.0 \PYGZhy{} m[i]) \PYGZhy{} 4.0exp(\PYGZhy{}(v[i]+65.0) / 18.0)*m[i])
        h[i] += dt * ((0.07exp(\PYGZhy{}0.05(v[i]+65.0)))*(1.0 \PYGZhy{} h[i]) \PYGZhy{} 1.0/(1.0 + exp(\PYGZhy{}0.1(v[i]+35.0)))*h[i])
        n[i] += dt * ((0.01(v[i]+55.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+55.0))))*(1.0 \PYGZhy{} n[i]) \PYGZhy{} (0.125exp(\PYGZhy{}0.0125(v[i]+65)))*n[i])
        v[i] += dt / Cm * (I[i] \PYGZhy{} gNa * m[i]\PYGZca{}3 * h[i] * (v[i] \PYGZhy{} ENa) \PYGZhy{} gK * n[i]\PYGZca{}4 * (v[i] \PYGZhy{} EK) \PYGZhy{} gL * (v[i] \PYGZhy{} EL))
        r[i] += dt * ((invtr \PYGZhy{} invtd) * (1.0 \PYGZhy{} r[i])/(1.0 + exp(\PYGZhy{}v[i] + v0)) \PYGZhy{} r[i] * invtd)
    end
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
update! (generic function with 1 method)
\end{sphinxVerbatim}


\subsubsection{2.2.3 Hodgkin\sphinxhyphen{}Huxleyモデルのシミュレーションの実行}
\label{\detokenize{2-2_hodgkinhuxley:id3}}
いくつかの定数を設定してシミュレーションを実行する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * ((t .\PYGZgt{} 50) \PYGZhy{} (t .\PYGZgt{} 200)) + 35f0 * ((t .\PYGZgt{} 250) \PYGZhy{} (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} 記録用
varr = zeros(Float32, nt, N)
gatearr = zeros(Float32, nt, 3, N)

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr[i, :] = neurons.v
    gatearr[i, 1, :] = neurons.m
    gatearr[i, 2, :] = neurons.h
    gatearr[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.217901 seconds (1.36 M allocations: 46.979 MiB, 3.69\PYGZpc{} gc time)
\end{sphinxVerbatim}

結果を表示するために \sphinxcode{\sphinxupquote{Plots}}を読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Plots}
\end{sphinxVerbatim}

ニューロンの膜電位 \sphinxcode{\sphinxupquote{v}}, ゲート変数 \sphinxcode{\sphinxupquote{m, h, n}}, 刺激電流 \sphinxcode{\sphinxupquote{I}}の描画をする。入力電流の単位は \(\mu\text{A/cm}^2\)である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

次項で用いるために発火回数を求める。\sphinxcode{\sphinxupquote{bitwise and}}を用いると楽である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Num. of spikes : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}spikes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Num. of spikes : 27
\end{sphinxVerbatim}

50msから200msまでで11回, 250msから400msまでで16回発火しているので発火回数は計27回であり、この結果は正しい。


\subsubsection{2.2.4 Frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve}
\label{\detokenize{2-2_hodgkinhuxley:frequency-current-f-i-curve}}
この項ではHodgkin\sphinxhyphen{}Huxleyモデルにおいて、入力電流に対する発火率がどのように変化するかを調べる。次のコードのように入力電流を徐々に増加させたときの発火率を見てみよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 1000 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps

N = 100 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
maxcurrent = 30
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = Array\PYGZob{}Float32\PYGZcb{}(range(1,maxcurrent,length=N)) \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr\PYGZus{}fi = zeros(Float32, nt, N)

\PYGZsh{} simulation
for i = 1:nt
    update!(neurons, neurons.param, I[:], dt)
    varr\PYGZus{}fi[i, :] = neurons.v
end
\end{sphinxVerbatim}

発火率を計算して結果を描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{rate} \PYG{o}{=} \PYG{n}{num\PYGZus{}spikes}\PYG{o}{/}\PYG{n}{T}\PYG{o}{*}\PYG{l+m+mf}{1e3}

\PYG{n}{plot}\PYG{p}{(}\PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{rate}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Input current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Firing rate (Hz)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{400}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

このような曲線を\sphinxstylestrong{frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve} (または neuronal input/output (I/O) curve)と呼ぶ。


\subsubsection{2.2.5  抑制後リバウンド (Postinhibitory rebound; PIR)}
\label{\detokenize{2-2_hodgkinhuxley:postinhibitory-rebound-pir}}
ニューロンは電流が流入することで膜電位が変化し, 膜電位がある一定の閾値を超えると発火が起こる, というのはニューロンの活動電位発生についての典型的な説明である。それではHHモデルの膜電位閾値はどのくらいの値になるのだろうか。答えは「\sphinxstylestrong{膜電位閾値は一定ではない}」である。それを示す現象として \sphinxstylestrong{抑制後リバウンド} (Postinhibitory rebound; PIR)がある。この時生じる発火を\sphinxstylestrong{リバウンド発火} (rebound spikes)
と呼ぶ。抑制後リバウンドは過分極性の電流の印加を止めた際に膜電位が静止膜電位に回復するのみならず, さらに脱分極をして発火をするという現象である。この現象が生じる要因として
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{アノーダルブレイク} (anodal break, またはanode break excitation; ABE)

\item {} 
遅いT型カルシウム電流 (slow T\sphinxhyphen{}type calcium current)

\end{enumerate}

がある (\sphinxhref{https://pubmed.ncbi.nlm.nih.gov/15324089/}{Chik et al., 2004})。HH モデルはこのうちアノーダルブレイクを再現できるため, シミュレーションによりどのような現象か確認してみよう。これは入力電流を変更するだけで行える。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * (\PYGZhy{}(t .\PYGZgt{} 50) + (t .\PYGZgt{} 200)) + 20f0 * (\PYGZhy{}(t .\PYGZgt{} 250) + (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr2 = zeros(Float32, nt, N)
gatearr2 = zeros(Float32, nt, 3, N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr2[i, :] = neurons.v
    gatearr2[i, 1, :] = neurons.m
    gatearr2[i, 2, :] = neurons.h
    gatearr2[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.074358 seconds (848.89 k allocations: 19.134 MiB)
\end{sphinxVerbatim}

結果は次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Injection}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

なぜこのようなことが起こるか, というと過分極の状態から静止膜電位へと戻る際にNa\(^+\)チャネルが活性化 (Na\(^+\)チャネルの活性化パラメータ\(m\)が増加し, 不活性化パラメータ\(h\)が減少)し, 膜電位が脱分極することで再度Na\(^+\)チャネルが活性化する, というポジティブフィードバック過程(\sphinxstylestrong{自己再生的過程})に突入するためである (もちろん, この過程は通常の活動電位発生のメカニズムである)。 この際, 発火に必要な閾値が膜電位の低下に応じて下がった, ということもできる。

このように膜電位閾値は一定ではない。しかし, この後の節で紹介するLIFモデルなどでは簡略化のためにif文を用い, 膜電位閾値を超えたから発火, というものもある。実際には違うということを頭の片隅に残しながら読み進めることを推奨する。

\begin{sphinxadmonition}{note}{Note:}
PIRに関連する現象として抑制後促通 (Postinhibitory facilitation; PIF)がある。これは抑制入力の後に興奮入力がある一定の時間内で入ると発火が起こるという現象である (\sphinxhref{http://www.brain.riken.jp/en/summer/prev/2006/files/j\_rinzel04.pdf}{Dolda et al., 2006}, \sphinxhref{https://link.springer.com/referenceworkentry/10.1007\%2F978-1-4614-7320-6\_152-1}{Dodla, 2014})。
\end{sphinxadmonition}


\subsection{3.1 シナプス伝達}
\label{\detokenize{3-1_synapse:id1}}\label{\detokenize{3-1_synapse::doc}}
\begin{sphinxadmonition}{note}{Note:}
内容は大幅に追記が必要。
\end{sphinxadmonition}

スパイクが生じたことによる膜電位変化は軸索を伝播し, \sphinxstylestrong{シナプス}という構造により, 次のニューロンへと興奮が伝わる. このときの伝達の仕組みとして, シナプスには\sphinxstylestrong{化学シナプス}(chemical synapse)とGap junctionによる\sphinxstylestrong{電気シナプス}(electrical synapse)がある。

化学シナプスの場合, シナプス前膜からの\sphinxstylestrong{神経伝達物質}の放出, シナプス後膜の受容体への神経伝達物質の結合, イオンチャネル開口による\sphinxstylestrong{シナプス後電流}(postsynaptic current; PSC)の発生, という過程が起こる。かなり簡略化して書いたが, 実際にはかなりの過程を含くむ。しかし, これらの過程を全てモデル化するのは計算量がかなり大きくなるので, 基本的には簡易的な現象論的なモデルを用いる。

このように, シナプス前細胞のスパイク列(spike train)は次のニューロンにそのまま伝わるのではなく, ある種の時間的フィルターをかけられて伝わる。このフィルターを\sphinxstylestrong{シナプスフィルター}(synaptic filter)と呼ぶ。3章では, このようにシナプス前細胞で生じた発火が, シナプス後細胞の膜電位に与える過程のモデルについて説明する。


\subsection{11.1 ベイズ統計の基礎}
\label{\detokenize{11-1_bayes_statistics:id1}}\label{\detokenize{11-1_bayes_statistics::doc}}
この節では本章で用いるベイズ統計の基礎的概念の説明を行う。

\begin{sphinxadmonition}{note}{Note:}
悪いこと言わないので\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/index-j.html}{渡辺澄夫先生のHP}の講義録、特に\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/joho-gakushu6.html}{ベイズ統計入門}を読もう。
\end{sphinxadmonition}


\subsection{JuliaのTips集}
\label{\detokenize{tips:juliatips}}\label{\detokenize{tips::doc}}
このページはJuliaでの実装におけるTips (詰まったところの解決策)をまとめたものである。体系的にまとまってはいない。


\subsubsection{1. 関数名の!記号}
\label{\detokenize{tips:id1}}
単なる\sphinxstylestrong{慣習}として関数への入力を変更する場合に!を付ける。

関数内で配列を変更する場合には注意が必要である。以下に入力された配列を同じサイズの要素1の配列で置き換える、ということを目的として書かれた2つの関数がある。違いは\sphinxcode{\sphinxupquote{v}}の後に\sphinxcode{\sphinxupquote{{[}:{]}}}としているかどうかである。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function wrong!(A::Array)
    a = ones(size(a))
end

function right!(a::Array)
    a[:] = ones(size(a))
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
right! (generic function with 2 methods)
\end{sphinxVerbatim}

実行すると\sphinxcode{\sphinxupquote{wrong!}}の場合には入力された配列が変更されていないことがわかる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
using Random
v = rand(2, 2)
print(\PYGZdq{}v : \PYGZdq{}, v)

wrong!(v)
print(\PYGZdq{}\PYGZbs{}nwrong : \PYGZdq{}, v)

right!(v)
print(\PYGZdq{}\PYGZbs{}nright : \PYGZdq{}, v)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
v : [0.42692589476552056 0.34730711983824536; 0.9208034639794132 0.41699915079158134]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+ne}{UndefVarError}: wrong! not defined

\PYG{n+ne}{Stacktrace}:
 \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{n}{top}\PYG{o}{\PYGZhy{}}\PYG{n}{level} \PYG{n}{scope} \PYG{n}{at} \PYG{n}{In}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}\PYG{l+m+mi}{5}
 \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{n}{include\PYGZus{}string}\PYG{p}{(}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Function}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{Module}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{)} \PYG{n}{at} \PYG{o}{.}\PYGZbs{}\PYG{n}{loading}\PYG{o}{.}\PYG{n}{jl}\PYG{p}{:}\PYG{l+m+mi}{1091}
\end{sphinxVerbatim}


\subsubsection{2. 配列の1次元化}
\label{\detokenize{tips:id2}}
配列を一次元化(flatten)する方法。まずは3次元配列を作成する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2×2 Array\PYGZob{}Float64,3\PYGZcb{}:
[:, :, 1] =
 0.997107  0.0827965
 0.394606  0.847174

[:, :, 2] =
 0.0305417  0.448929
 0.270553   0.601419
\end{sphinxVerbatim}

用意されている\sphinxcode{\sphinxupquote{flatten}}を素直に用いると次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{Base}\PYG{n+nn}{.}\PYG{n+nn}{Iterators}\PYG{p}{:} \PYG{n}{flatten}
\PYG{n}{collect}\PYG{p}{(}\PYG{n}{flatten}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}

ただし、単に\sphinxcode{\sphinxupquote{B{[}:{]}}}とするだけでもよい。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}


\subsubsection{3. 行列の行・列ごとの正規化}
\label{\detokenize{tips:id3}}
シミュレーションにおいてニューロン間の重み行列を行あるいは列ごとに正規化 (weight normalization)する場合がある。これは各ニューロンへの入力の大きさを同じにする働きや重みの発散を防ぐ役割がある。以下では行ごとの和を1にする。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.166781  0.0552745  0.973622
 0.100336  0.753766   0.370005
 0.378222  0.165159   0.471515
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Wnormed} \PYG{o}{=} \PYG{n}{W} \PYG{o}{.}\PYG{o}{/} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{W}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.258439  0.0567384  0.536389
 0.155479  0.773729   0.203843
 0.586082  0.169533   0.259767
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{Wnormed}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1.0 1.0 1.0]
\end{sphinxVerbatim}


\subsubsection{4. 行列の結合 (concatenate)}
\label{\detokenize{tips:concatenate}}
行列の結合はMATLABに近い形式で行うことができる。まず、2つの行列A, Bを用意する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{l+m+mi}{3} \PYG{l+m+mi}{4}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{4} \PYG{l+m+mi}{5} \PYG{l+m+mi}{6}\PYG{p}{;} \PYG{l+m+mi}{7} \PYG{l+m+mi}{8} \PYG{l+m+mi}{9}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×3 Array\PYGZob{}Int64,2\PYGZcb{}:
 4  5  6
 7  8  9
\end{sphinxVerbatim}


\paragraph{4.1 水平結合 (Horizontal concatenation)}
\label{\detokenize{tips:horizontal-concatenation}}
\sphinxcode{\sphinxupquote{hcat}}を使うやり方と、\sphinxcode{\sphinxupquote{{[} {]}}}を使うやり方がある。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H1} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,}\PYG{n}{B}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

なお、MATLABのように次のようにすると正しく結合はされない。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H3} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{,} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2\PYGZhy{}element Array\PYGZob{}Array\PYGZob{}Int64,2\PYGZcb{},1\PYGZcb{}:
 [1 2; 3 4]
 [4 5 6; 7 8 9]
\end{sphinxVerbatim}


\paragraph{4.2 垂直結合 (Vertical concatenation)}
\label{\detokenize{tips:vertical-concatenation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V1} \PYG{o}{=} \PYG{n}{vcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{V2} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;}\PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×4 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  1  2
 3  4  3  4
 4  7  4  7
 5  8  5  8
 6  9  6  9
\end{sphinxVerbatim}


\subsubsection{5. 配列に新しい軸を追加}
\label{\detokenize{tips:id4}}
要はnumpyでの\sphinxcode{\sphinxupquote{A{[}None, :{]}}}や\sphinxcode{\sphinxupquote{A{[}np.newaxis, :{]}}}のようなことがしたい場合。やや面倒だが、\sphinxcode{\sphinxupquote{reshape}}を使うか、\sphinxcode{\sphinxupquote{{[}CartesianIndex(){]}}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.3487808667518093
 0.717776723811032
 0.3369916406749496
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{newaxis} \PYG{o}{=} \PYG{p}{[}\PYG{n}{CartesianIndex}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{v1} \PYG{o}{=} \PYG{n}{v}\PYG{p}{[}\PYG{n}{newaxis}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.348781  0.717777  0.336992
\end{sphinxVerbatim}


\subsubsection{6. Array\{Array\{Float64, x\},1\}をArray\{Float64, x+1\}に変換}
\label{\detokenize{tips:array-array-float64-x-1-array-float64-x-1}}
numpyでは\sphinxcode{\sphinxupquote{array({[}matrix for i in range(){]})}}などを用いると、1次元配列のリストを2次元配列に変換できた。Juliaでも同様にする場合は\sphinxcode{\sphinxupquote{hcat(...)}}や\sphinxcode{\sphinxupquote{cat(...)}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,1\PYGZcb{},1\PYGZcb{}
Size : (5,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A2} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : LinearAlgebra.Adjoint\PYGZob{}Float64,Array\PYGZob{}Float64,2\PYGZcb{}\PYGZcb{}
Size : (5, 3)
\end{sphinxVerbatim}

以下は多次元配列の場合。\sphinxcode{\sphinxupquote{cat(...)}}で配列を結合し、\sphinxcode{\sphinxupquote{permitedims}}で転置する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,3\PYGZcb{},1\PYGZcb{}
Size : (6,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B2} \PYG{o}{=} \PYG{n}{permutedims}\PYG{p}{(}\PYG{n}{cat}\PYG{p}{(}\PYG{n}{B1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Float64,4\PYGZcb{}
Size : (6, 3, 4, 5)
\end{sphinxVerbatim}


\subsection{2.2 Hodgkin\sphinxhyphen{}Huxleyモデル}
\label{\detokenize{2-2_hodgkinhuxley:hodgkin-huxley}}\label{\detokenize{2-2_hodgkinhuxley::doc}}

\subsubsection{2.2.1 Hodgkin\sphinxhyphen{}Huxleyモデルにおける膜の等価回路モデル}
\label{\detokenize{2-2_hodgkinhuxley:id1}}
\sphinxstylestrong{Hodgkin\sphinxhyphen{}Huxleyモデル} (HH モデル)は, A.L. HodgkinとA.F. Huxleyによって1952年に考案されたニューロンの膜興奮を表すモデルである (\sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/}{Hodgkin \& Huxley, 1952})。Hodgkinらはヤリイカの巨大神経軸索に対する\sphinxstylestrong{電位固定法}(voltage\sphinxhyphen{}clamp)を用いた実験を行い, 実験から得られた観測結果を元にモデルを構築した。

HHモデルには等価な電気回路モデルがあり, \sphinxstylestrong{膜の並列等価回路モデル} (parallel conductance model)と呼ばれている。膜の並列等価回路モデルでは, ニューロンの細胞膜をコンデンサ, 細胞膜に埋まっているイオンチャネルを可変抵抗 (動的に変化する抵抗) として置き換える。

\sphinxstylestrong{イオンチャネル} (ion channel)は特定のイオン(例えばナトリウムイオンやカリウムイオンなど)を選択的に通す膜輸送体の一種である。それぞれのイオンの種類において, 異なるイオンチャネルがある (同じイオンでも複数の種類のイオンチャネルがある)。また, イオンチャネルにはイオンの種類に応じて異なる\sphinxstylestrong{コンダクタンス}(抵抗の逆数で電流の「流れやすさ」を意味する)と\sphinxstylestrong{平衡電位}(equilibrium potential)がある。HHモデルでは, ナトリウム(Na\(^{+}\))チャネル, カリウム(K\(^{+}\))チャネル, 漏れ電流(leak current)のイオンチャネルを仮定する。漏れ電流のイオンチャネルは当時特定できなかったチャネルで, 膜から電流が漏れ出すチャネルを意味する。なお, 現在では漏れ電流の多くはCl\(^{-}\)イオン(chloride ion)によることが分かっている。

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{parallel_conductance_model}.JPG}
\caption{Hodgkin\sphinxhyphen{}Huxleyモデルの膜の等価回路モデル}\label{\detokenize{2-2_hodgkinhuxley:parallel-conductance-model}}\end{figure}

それでは, 等価回路モデルを用いて電位変化の式を立ててみよう。上図において, \(C_m\)は細胞膜のキャパシタンス(膜容量), \(I_{m}(t)\)は細胞膜を流れる電流(外部からの入力電流), \(I_\text{Cap}(t)\)は膜のコンデンサを流れる電流, \(I_\text{Na}(t)\)及び \(I_K(t)\)はそれぞれナトリウムチャネルとカリウムチャネルを通って膜から流出する電流, \(I_\text{L}(t)\)は漏れ電流である。このとき,
\begin{equation*}
\begin{split}
I_{m}(t)=I_\text{Cap}(t)+I_\text{Na}(t)+I_\text{K}(t)+I_\text{L}(t)    
\end{split}
\end{equation*}
という仮定をしている。

膜電位を\(V(t)\)とすると, Kirchhoffの第二法則 (Kirchhoff’s Voltage Law)より,
\begin{equation*}
\begin{split}
\underbrace{C_m\frac {dV(t)}{dt}}_{I_\text{Cap} (t)}=I_{m}(t)-I_\text{Na}(t)-I_\text{K}(t)-I_\text{L}(t)
\end{split}
\end{equation*}
となる。Hodgkinらはチャネル電流\(I_\text{Na}, I_K, I_\text{L}\)が従う式を実験的に求めた。
\begin{equation*}
\begin{split}
\begin{aligned}
I_\text{Na}(t) &= g_{\text{Na}}\cdot m^{3}h(V-E_{\text{Na}})\\
I_\text{K}(t) &= g_{\text{K}}\cdot n^{4}(V-E_{\text{K}})\\
I_\text{L}(t) &= g_{\text{L}}(V-E_{\text{L}})
\end{aligned}
\end{split}
\end{equation*}
ただし, \(g_{\text{Na}}, g_{\text{K}}\)はそれぞれNa\(^+\), K\(^+\)の最大コンダクタンスである。\(g_{\text{L}}\)はオームの法則に従うコンダクタンスで, Lコンダクタンスは時間的に変化はしないと仮定する。また, \(m\)はNa\(^+\)コンダクタンスの活性化パラメータ, \(h\)はNa\(^+\)コンダクタンスの不活性化パラメータ, \(n\)はK\(^+\)コンダクタンスの活性化パラメータであり, ゲートの開閉確率を表している。よって, HHモデルの状態は\(V, m, h, n\)の4変数で表される。これらの変数は以下の\(x\)を\(m, n, h\)に置き換えた3つの微分方程式に従う。
\begin{equation*}
\begin{split}
\frac{dx}{dt}=\alpha_{x}(V)(1-x)-\beta_{x}(V)x
\end{split}
\end{equation*}
ただし, \(V\)の関数である\(\alpha_{x}(V),\ \beta_{x}(V)\)は\(m, h, n\)によって異なり, 次の6つの式に従う。
\begin{equation*}
\begin{split}
\begin{array}{ll}
\alpha_{m}(V)=\dfrac {0.1(25-V)}{\exp \left[(25-V)/10\right]-1}, &\beta_{m}(V)=4\exp {(-V/18)}\\
\alpha_{h}(V)=0.07\exp {(-V/20)}, & \beta_{h}(V)={\dfrac{1}{\exp {\left[(30-V)/10 \right]}+1}}\\
\alpha_{n}(V)={\dfrac {0.01(10-V)}{\exp {\left[(10-V)/10\right]}-1}},& \beta_{n}(V)=0.125\exp {(-V/80)} 
\end{array}
\end{split}
\end{equation*}
なお、この式は6.3℃の条件下においてイカの巨大軸索の活動から得たデータを用いて導かれたものであることに注意しよう。


\subsubsection{2.2.2 Hodgkin\sphinxhyphen{}Huxley モデルの定義}
\label{\detokenize{2-2_hodgkinhuxley:id2}}
これまでに説明した式を用いてHHモデルを実装する。まず必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\end{sphinxVerbatim}

変更しない定数を保持する \sphinxcode{\sphinxupquote{struct}} の \sphinxcode{\sphinxupquote{HHParameter}} と, 変数を保持する \sphinxcode{\sphinxupquote{mutable struct}} の \sphinxcode{\sphinxupquote{HH}} を作成する。\sphinxcode{\sphinxupquote{v, m, h, n}} はHH modelの4変数だが, \sphinxcode{\sphinxupquote{r}} はpre\sphinxhyphen{}synaptic dynamicsを表す変数である。詳細は3章で解説する。 定数は次のように設定する。

\textbackslash{}begin\{align*\}
C\_m=1.0, g\_\{\textbackslash{}text\{Na\}\}=120, g\_\{\textbackslash{}text\{K\}\}=36, g\_\{\textbackslash{}text\{L\}\}=0.3\textbackslash{}
E\_\{\textbackslash{}text\{Na\}\}=50.0, E\_\{\textbackslash{}text\{K\}\}=\sphinxhyphen{}77, E\_\{\textbackslash{}text\{L\}\}=\sphinxhyphen{}54.387
\textbackslash{}end\{align*\}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{Cm}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{c+c1}{\PYGZsh{} 膜容量(uF/cm\PYGZca{}2)}
    \PYG{n}{gNa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{120.0} \PYG{c+c1}{\PYGZsh{} Na+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{36.0} \PYG{c+c1}{\PYGZsh{} K+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.3} \PYG{c+c1}{\PYGZsh{} 漏れイオンの最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{ENa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{50.0} \PYG{c+c1}{\PYGZsh{} Na+ の平衡電位(mV)}
    \PYG{n}{EK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{77.0} \PYG{c+c1}{\PYGZsh{} K+ の平衡電位(mV)}
    \PYG{n}{EL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{54.387} \PYG{c+c1}{\PYGZsh{}漏れイオンの平衡電位(mV)}
    \PYG{n}{tr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.5} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{td}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{8.0} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{invtr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{tr}
    \PYG{n}{invtd}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{td}
    \PYG{n}{v0}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{20.0} \PYG{c+c1}{\PYGZsh{} mV}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{HH}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{HHParameter} \PYG{o}{=} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{N}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16}
    \PYG{n}{v}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{65.0}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{m}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{h}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{n}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.32}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{r}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

次に変数を更新する関数\sphinxcode{\sphinxupquote{update!}}を書く。ソルバーとしては陽的Euler法または4次のRunge\sphinxhyphen{}Kutta法を用いる。以下ではEuler法を用いている。Juliaではforループを用いて1つのニューロンごとにパラメータを更新する方がベクトルを用いるよりも高速である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function update!(variable::HH, param::HHParameter, I::Vector, dt)
    @unpack N, v, m, h, n, r = variable
    @unpack Cm, gNa, gK, gL, ENa, EK, EL, tr, td, invtr, invtd, v0 = param
    @inbounds for i = 1:N
        m[i] += dt * ((0.1(v[i]+40.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+40.0))))*(1.0 \PYGZhy{} m[i]) \PYGZhy{} 4.0exp(\PYGZhy{}(v[i]+65.0) / 18.0)*m[i])
        h[i] += dt * ((0.07exp(\PYGZhy{}0.05(v[i]+65.0)))*(1.0 \PYGZhy{} h[i]) \PYGZhy{} 1.0/(1.0 + exp(\PYGZhy{}0.1(v[i]+35.0)))*h[i])
        n[i] += dt * ((0.01(v[i]+55.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+55.0))))*(1.0 \PYGZhy{} n[i]) \PYGZhy{} (0.125exp(\PYGZhy{}0.0125(v[i]+65)))*n[i])
        v[i] += dt / Cm * (I[i] \PYGZhy{} gNa * m[i]\PYGZca{}3 * h[i] * (v[i] \PYGZhy{} ENa) \PYGZhy{} gK * n[i]\PYGZca{}4 * (v[i] \PYGZhy{} EK) \PYGZhy{} gL * (v[i] \PYGZhy{} EL))
        r[i] += dt * ((invtr \PYGZhy{} invtd) * (1.0 \PYGZhy{} r[i])/(1.0 + exp(\PYGZhy{}v[i] + v0)) \PYGZhy{} r[i] * invtd)
    end
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
update! (generic function with 1 method)
\end{sphinxVerbatim}


\subsubsection{2.2.3 Hodgkin\sphinxhyphen{}Huxleyモデルのシミュレーションの実行}
\label{\detokenize{2-2_hodgkinhuxley:id3}}
いくつかの定数を設定してシミュレーションを実行する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * ((t .\PYGZgt{} 50) \PYGZhy{} (t .\PYGZgt{} 200)) + 35f0 * ((t .\PYGZgt{} 250) \PYGZhy{} (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} 記録用
varr = zeros(Float32, nt, N)
gatearr = zeros(Float32, nt, 3, N)

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr[i, :] = neurons.v
    gatearr[i, 1, :] = neurons.m
    gatearr[i, 2, :] = neurons.h
    gatearr[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.217901 seconds (1.36 M allocations: 46.979 MiB, 3.69\PYGZpc{} gc time)
\end{sphinxVerbatim}

結果を表示するために \sphinxcode{\sphinxupquote{Plots}}を読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Plots}
\end{sphinxVerbatim}

ニューロンの膜電位 \sphinxcode{\sphinxupquote{v}}, ゲート変数 \sphinxcode{\sphinxupquote{m, h, n}}, 刺激電流 \sphinxcode{\sphinxupquote{I}}の描画をする。入力電流の単位は \(\mu\text{A/cm}^2\)である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

次項で用いるために発火回数を求める。\sphinxcode{\sphinxupquote{bitwise and}}を用いると楽である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Num. of spikes : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}spikes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Num. of spikes : 27
\end{sphinxVerbatim}

50msから200msまでで11回, 250msから400msまでで16回発火しているので発火回数は計27回であり、この結果は正しい。


\subsubsection{2.2.4 Frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve}
\label{\detokenize{2-2_hodgkinhuxley:frequency-current-f-i-curve}}
この項ではHodgkin\sphinxhyphen{}Huxleyモデルにおいて、入力電流に対する発火率がどのように変化するかを調べる。次のコードのように入力電流を徐々に増加させたときの発火率を見てみよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 1000 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps

N = 100 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
maxcurrent = 30
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = Array\PYGZob{}Float32\PYGZcb{}(range(1,maxcurrent,length=N)) \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr\PYGZus{}fi = zeros(Float32, nt, N)

\PYGZsh{} simulation
for i = 1:nt
    update!(neurons, neurons.param, I[:], dt)
    varr\PYGZus{}fi[i, :] = neurons.v
end
\end{sphinxVerbatim}

発火率を計算して結果を描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{rate} \PYG{o}{=} \PYG{n}{num\PYGZus{}spikes}\PYG{o}{/}\PYG{n}{T}\PYG{o}{*}\PYG{l+m+mf}{1e3}

\PYG{n}{plot}\PYG{p}{(}\PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{rate}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Input current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Firing rate (Hz)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{400}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

このような曲線を\sphinxstylestrong{frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve} (または neuronal input/output (I/O) curve)と呼ぶ。


\subsubsection{2.2.5  抑制後リバウンド (Postinhibitory rebound; PIR)}
\label{\detokenize{2-2_hodgkinhuxley:postinhibitory-rebound-pir}}
ニューロンは電流が流入することで膜電位が変化し, 膜電位がある一定の閾値を超えると発火が起こる, というのはニューロンの活動電位発生についての典型的な説明である。それではHHモデルの膜電位閾値はどのくらいの値になるのだろうか。答えは「\sphinxstylestrong{膜電位閾値は一定ではない}」である。それを示す現象として \sphinxstylestrong{抑制後リバウンド} (Postinhibitory rebound; PIR)がある。この時生じる発火を\sphinxstylestrong{リバウンド発火} (rebound spikes)
と呼ぶ。抑制後リバウンドは過分極性の電流の印加を止めた際に膜電位が静止膜電位に回復するのみならず, さらに脱分極をして発火をするという現象である。この現象が生じる要因として
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{アノーダルブレイク} (anodal break, またはanode break excitation; ABE)

\item {} 
遅いT型カルシウム電流 (slow T\sphinxhyphen{}type calcium current)

\end{enumerate}

がある (\sphinxhref{https://pubmed.ncbi.nlm.nih.gov/15324089/}{Chik et al., 2004})。HH モデルはこのうちアノーダルブレイクを再現できるため, シミュレーションによりどのような現象か確認してみよう。これは入力電流を変更するだけで行える。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * (\PYGZhy{}(t .\PYGZgt{} 50) + (t .\PYGZgt{} 200)) + 20f0 * (\PYGZhy{}(t .\PYGZgt{} 250) + (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr2 = zeros(Float32, nt, N)
gatearr2 = zeros(Float32, nt, 3, N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr2[i, :] = neurons.v
    gatearr2[i, 1, :] = neurons.m
    gatearr2[i, 2, :] = neurons.h
    gatearr2[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.074358 seconds (848.89 k allocations: 19.134 MiB)
\end{sphinxVerbatim}

結果は次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Injection}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

なぜこのようなことが起こるか, というと過分極の状態から静止膜電位へと戻る際にNa\(^+\)チャネルが活性化 (Na\(^+\)チャネルの活性化パラメータ\(m\)が増加し, 不活性化パラメータ\(h\)が減少)し, 膜電位が脱分極することで再度Na\(^+\)チャネルが活性化する, というポジティブフィードバック過程(\sphinxstylestrong{自己再生的過程})に突入するためである (もちろん, この過程は通常の活動電位発生のメカニズムである)。 この際, 発火に必要な閾値が膜電位の低下に応じて下がった, ということもできる。

このように膜電位閾値は一定ではない。しかし, この後の節で紹介するLIFモデルなどでは簡略化のためにif文を用い, 膜電位閾値を超えたから発火, というものもある。実際には違うということを頭の片隅に残しながら読み進めることを推奨する。

\begin{sphinxadmonition}{note}{Note:}
PIRに関連する現象として抑制後促通 (Postinhibitory facilitation; PIF)がある。これは抑制入力の後に興奮入力がある一定の時間内で入ると発火が起こるという現象である (\sphinxhref{http://www.brain.riken.jp/en/summer/prev/2006/files/j\_rinzel04.pdf}{Dolda et al., 2006}, \sphinxhref{https://link.springer.com/referenceworkentry/10.1007\%2F978-1-4614-7320-6\_152-1}{Dodla, 2014})。
\end{sphinxadmonition}


\subsection{2.4 Leaky integrate\sphinxhyphen{}and\sphinxhyphen{}fire モデル}
\label{\detokenize{2-4_lif:leaky-integrate-and-fire}}\label{\detokenize{2-4_lif::doc}}

\subsubsection{2.4.1 LIFモデルの定義}
\label{\detokenize{2-4_lif:lif}}
生理学的なイオンチャネルの挙動は考慮せず, 入力電流を膜電位が閾値に達するまで時間的に積分するというモデルを\sphinxstylestrong{Integrate\sphinxhyphen{}and\sphinxhyphen{}fire (IF, 積分発火)モデル} という。さらに, IFモデルにおいて膜電位の漏れ(leak)%
\begin{footnote}[1]\sphinxAtStartFootnote
この漏れはイオンの拡散などによるもの。
%
\end{footnote}も考慮したモデルを \sphinxstylestrong{Leaky integrate\sphinxhyphen{}and\sphinxhyphen{}fire (LIF, 漏れ積分発火) モデル} と呼ぶ。ここではLIFモデルのみを取り扱う。

ニューロンの膜電位を\(V_m(t)\), 静止膜電位を\(V_\text{rest}\), 入力電流%
\begin{footnote}[2]\sphinxAtStartFootnote
シナプス入力による電流がどうなるかは、第三章「シナプス伝達のモデル」で扱う。
%
\end{footnote}を\(I(t)\), 膜抵抗を\(R_m\), 膜電位の時定数を\(\tau_m\ (=R_m \cdot C_m)\)とすると, 式は次のようになる%
\begin{footnote}[3]\sphinxAtStartFootnote
\((V_{m}(t)-V_\text{rest})\)の部分は膜電位の基準を静止膜電位としたことにして, 単に\(V_m(t)\)だけの場合もある。 また, 右辺の\(RI(t)\)の部分は単に\(I(t)\)とされることもある。 同じ表記だが, この場合の\(I(t)\)はシナプス電流に比例する量, となっている(単位はmV)。
%
\end{footnote}。
\begin{equation*}
\begin{split}
\begin{equation}
\tau_m \frac{dV_{m}(t)}{dt}=-(V_{m}(t)-V_\text{rest})+R_mI(t)
\end{equation}
\end{split}
\end{equation*}
ここで, \(V_m\)が閾値(threshold)%
\begin{footnote}[4]\sphinxAtStartFootnote
thから始まるので文字\(\theta\)が使われることもある。
%
\end{footnote}\(V_{\text{th}}\)を超えると, 脱分極が起こり, 膜電位はピーク電位 \(V_{\text{peak}}\)まで上昇する。発火後は再分極が起こり, 膜電位はリセット電位 \(V_{\text{reset}}\)まで低下すると仮定する%
\begin{footnote}[5]\sphinxAtStartFootnote
リセット電位は静止膜電位と同じ場合もあれば, 過分極を考慮して静止膜電位より低めに設定することもある。
%
\end{footnote}。発火後, 一定の期間\(\tau_{\text{ref}}\) の間は膜電位が変化しない%
\begin{footnote}[6]\sphinxAtStartFootnote
実装によっては不応期の間は膜電位の変化は許容するが発火は生じないようにすることもある。
%
\end{footnote}, とする。これを \sphinxstylestrong{不応期(refractory time period)} と呼ぶ。

以上を踏まえてLIFモデルを実装してみよう。まず必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\end{sphinxVerbatim}

HHモデルと同様に変更しない定数を保持する \sphinxcode{\sphinxupquote{struct}} の \sphinxcode{\sphinxupquote{LIFParameter}} と, 変数を保持する \sphinxcode{\sphinxupquote{mutable struct}} の \sphinxcode{\sphinxupquote{LIF}} を作成する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{LIFParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{tref}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT}   \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{c+c1}{\PYGZsh{} 不応期 (ms)}
    \PYG{n}{tc\PYGZus{}m}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT}   \PYG{o}{=} \PYG{l+m+mi}{10} \PYG{c+c1}{\PYGZsh{} 膜時定数 (ms)}
    \PYG{n}{vrest}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT}  \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{60} \PYG{c+c1}{\PYGZsh{} 静止膜電位 (mV) }
    \PYG{n}{vreset}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{65} \PYG{c+c1}{\PYGZsh{} リセット電位 (mV) }
    \PYG{n}{vthr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT}   \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{40} \PYG{c+c1}{\PYGZsh{} 閾値電位 (mV)}
    \PYG{n}{vpeak}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT}  \PYG{o}{=} \PYG{l+m+mi}{30} \PYG{c+c1}{\PYGZsh{}　ピーク電位 (mV)}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{LIF}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{LIFParameter} \PYG{o}{=} \PYG{n}{LIFParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{N}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt32} \PYG{c+c1}{\PYGZsh{}ニューロンの数}
    \PYG{n}{v}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{65.0}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} 膜電位 (mV)}
    \PYG{n}{v\PYGZus{}}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{65.0}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} 発火電位も記録する変数}
    \PYG{n}{fire}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{Bool}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{Bool}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} 発火}
    \PYG{n}{tlast}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} 最後の発火時刻 (ms)}
    \PYG{n}{tcount}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mi}{0} \PYG{c+c1}{\PYGZsh{} 時間カウント}
\PYG{n}{end}
\end{sphinxVerbatim}

次に変数を更新する関数\sphinxcode{\sphinxupquote{update!}}を書く。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function update!(variable::LIF, param::LIFParameter, I::Vector, dt)
    @unpack N, v, v\PYGZus{}, fire, tlast, tcount = variable
    @unpack tref, tc\PYGZus{}m, vrest, vreset, vthr, vpeak = param
    
    @inbounds for i = 1:N
        \PYGZsh{}v[i] += dt * ((vrest \PYGZhy{} v[i] + I[i]) / tc\PYGZus{}m) \PYGZsh{} 不応期を考慮しない場合の更新式
        v[i] += dt * ((dt*tcount) \PYGZgt{} (tlast[i] + tref))*((vrest \PYGZhy{} v[i] + I[i]) / tc\PYGZus{}m)
        \PYGZsh{}v[i] += dt * ifelse(dt*tcount[1] \PYGZgt{} tlast[i] + tref, (vrest \PYGZhy{} v[i] + I[i]) / tc\PYGZus{}m, 0)
    end
    @inbounds for i = 1:N
        fire[i] = v[i] \PYGZgt{}= vthr
        v\PYGZus{}[i] = ifelse(fire[i], vpeak, v[i]) \PYGZsh{}発火時の電位も含めて記録するための変数 (除いてもよい)
        v[i] = ifelse(fire[i], vreset, v[i])        
        tlast[i] = ifelse(fire[i], dt*tcount, tlast[i]) \PYGZsh{} 発火時刻の更新
    end
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
update! (generic function with 1 method)
\end{sphinxVerbatim}

いくつかの処理について解説しておく。まず、一番目のforループ内の\sphinxcode{\sphinxupquote{v{[}i{]}}}の\sphinxcode{\sphinxupquote{((dt*tcount) \textgreater{} (tlast{[}i{]} + tref))}}は最後にニューロンが発火した時刻\sphinxcode{\sphinxupquote{tlast{[}i{]}}}に不応期\sphinxcode{\sphinxupquote{tref}}を足した時刻よりも現在の時刻\sphinxcode{\sphinxupquote{dt*tcount{[}1{]}}}が大きければ膜電位の更新を許可し、小さければ更新しない。二番目のforループにおける\sphinxcode{\sphinxupquote{fire{[}i{]}}}はニューロンの膜電位が閾値電位\sphinxcode{\sphinxupquote{vthr}}を超えたら\sphinxcode{\sphinxupquote{True}}となる。\sphinxcode{\sphinxupquote{v{[}i{]}}}などの更新式にある\sphinxcode{\sphinxupquote{ifelse(a, b, c)}}はaが\sphinxcode{\sphinxupquote{True}}の時はbを返し、\sphinxcode{\sphinxupquote{False}}の時はcを返す関数であり、\sphinxcode{\sphinxupquote{v{[}i{]} = ifelse(fire{[}i{]}, vreset, v{[}i{]})}}は\sphinxcode{\sphinxupquote{fire{[}i{]}}}が\sphinxcode{\sphinxupquote{True}}なら\sphinxcode{\sphinxupquote{v{[}i{]}}}をリセット電位\sphinxcode{\sphinxupquote{vreset}}とし、そうでなければそのままの値を返すという処理である。同様にして\sphinxcode{\sphinxupquote{tlast{[}i{]}}}は発火したときにその時刻を記録する変数となっている。なお、\sphinxcode{\sphinxupquote{v\_{[}i{]} = ifelse(fire{[}i{]}, vpeak, v{[}i{]})}}は実際のシミュレーションにおいて意味をなさない。単に発火時の電位\sphinxcode{\sphinxupquote{vpeak}}を含めて記録すると描画時の見栄えが良いというだけである。

これらの\sphinxcode{\sphinxupquote{struct}}と関数を用いてシミュレーションを実行する。\sphinxcode{\sphinxupquote{I}} はHHモデルのときと同じように矩形波を入力する。実は\sphinxcode{\sphinxupquote{I}}は入力電流ではなく入力電流に比例する量となっているが、これは膜抵抗を乗じた後の値であると考えるとよい。


\subsubsection{2.4.2 LIFモデルのシミュレーションの実行}
\label{\detokenize{2-4_lif:id7}}
いくつかの定数を設定してシミュレーションを実行する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(25f0 * ((t .\PYGZgt{} 50) \PYGZhy{} (t .\PYGZgt{} 200)) + 50f0 * ((t .\PYGZgt{} 250) \PYGZhy{} (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} 記録用
varr = zeros(Float32, nt, N)

\PYGZsh{} modelの定義
neurons = LIF\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    neurons.tcount += 1
    varr[i, :] = neurons.v\PYGZus{}
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.121075 seconds (803.64 k allocations: 24.919 MiB, 6.18\PYGZpc{} gc time)
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{Plots}}を読み込み、発火時電位を含む膜電位\sphinxcode{\sphinxupquote{v\_}}と入力電流\sphinxcode{\sphinxupquote{I}}を描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Plots}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{2.4.3 LIFモデルのF\sphinxhyphen{}I curve}
\label{\detokenize{2-4_lif:liff-i-curve}}

\paragraph{数値的計算によるF\sphinxhyphen{}I curveの描画}
\label{\detokenize{2-4_lif:f-i-curve}}
この項目ではLIFモデルにおける入力電流に対する発火率の変化 (F\sphinxhyphen{}I curve)を描画する。方法はHHモデルの場合と同様だが、今回は発火したかどうかがモデル内の変数として明示的に記録されているので処理が少ない。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 1000 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps

N = 100 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
mincurrent = 15
maxcurrent = 40
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = Array\PYGZob{}Float32\PYGZcb{}(range(mincurrent,maxcurrent,length=N)) \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = LIF\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
firearr = zeros(Bool, nt, N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[:], dt)
    neurons.tcount += 1
    firearr[i, :] = neurons.fire
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.137166 seconds (1.08 M allocations: 66.640 MiB, 8.97\PYGZpc{} gc time)
\end{sphinxVerbatim}

発火率を計算し、描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{firearr}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{rate} \PYG{o}{=} \PYG{n}{num\PYGZus{}spikes}\PYG{o}{/}\PYG{n}{T}\PYG{o}{*}\PYG{l+m+mf}{1e3}

\PYG{n}{plot}\PYG{p}{(}\PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{rate}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Input current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Firing rate (Hz)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{400}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

さらに電流を強めると発火率は飽和(saturation)する。なお、不応期がない、すなわち0の場合は閾値付近以外はReLU関数のような挙動をする。


\paragraph{解析的計算によるF\sphinxhyphen{}I curveの描画}
\label{\detokenize{2-4_lif:id8}}
ここまでは数値的なシミュレーションによりF\sphinxhyphen{}I curveを求めた。以下では解析的にF\sphinxhyphen{}I curveの式を求めよう。具体的には、一定かつ持続的な入力電流を\(I\)としたときのLIFニューロンの発火率(firing rate)が
\begin{equation*}
\begin{split}
\begin{equation}
\text{rate}\approx \left(\tau_m \ln \frac{R_mI}{R_mI＋V_\text{rest}-V_{\text{th}}}\right)^{-1}
\end{equation}
\end{split}
\end{equation*}
と近似できることを示す。まず、\(t=t_1\)にスパイクが生じたとする。このとき, 膜電位はリセットされるので\(V_m(t_1)=V_\text{rest}\)である(リセット電位と静止膜電位が同じと仮定する)。\([t_1, t]\)における膜電位はLIFの式を積分することで得られる。
\begin{equation*}
\begin{split}
\begin{equation}
\tau_m \frac{dV_{m}(t)}{dt}=-(V_{m}(t)-V_\text{rest})+R_m I
\end{equation}
\end{split}
\end{equation*}
の式を積分すると,
\begin{equation*}
\begin{split}
\begin{aligned}
\int_{t_1}^{t} \frac{\tau_m dV_m}{R_mI＋V_\text{rest}-V_m}&=\int_{t_1}^{t} dt\\
\ln \left(1-\frac{V_m(t)-V_\text{rest}}{R_mI}\right)&=-\frac{t-t_1}{\tau_m} \quad (\because V_m(t_1)=V_\text{rest})\\
\therefore\ \ V_m(t) &=V_\text{rest} + R_mI\left[1-\exp\left(-\frac{t-t_1}{\tau_m}\right)\right] 
\end{aligned}
\end{split}
\end{equation*}
となる。\(t>t_1\)における初めのスパイクが\(t=t_2\)に生じたとすると, そのときの膜電位は\(V_m(t_2)=V_{\text{th}}\)である (実際には閾値以上となっている場合もあるますが近似する)。\(t=t_2\)を上の式に代入して
\begin{equation*}
\begin{split}
\begin{align}
V_{\text{th}}&=V_\text{rest} + R_mI\left[1-\exp\left(-\frac{t_2-t_1}{\tau}\right)\right] \\
\therefore\ \ T&= t_2-t_1 = \tau_m \ln \frac{R_mI}{R_mI＋V_\text{rest}-V_{\text{th}}}
\end{align}
\end{split}
\end{equation*}
となる。ここで\(T\)は2つのスパイクの時間間隔 (spike interval)である。\(t_1\leq t<t_2\)におけるスパイクは\(t=t_1\)時の1つなので, 発火率は\(1/T\)となる。よって
\begin{equation*}
\begin{split}
\text{rate}\approx \frac{1}{T}=\left(\tau_m \ln \frac{R_mI}{R_mI＋V_\text{rest}-V_{\text{th}}}\right)^{-1}
\end{split}
\end{equation*}
となる。不応期\(\tau_{\text{ref}}\)を考慮すると, 持続的に入力がある場合は単純に\(\tau_{\text{ref}}\)だけ発火が遅れるので発火率は\(1/(\tau_{\text{ref}}+T)\)となる。

それではこの式に基づいてF\sphinxhyphen{}I curveを描画してみよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tc\PYGZus{}m} \PYG{o}{=} \PYG{l+m+mi}{10} \PYG{c+c1}{\PYGZsh{} 膜時定数 (ms)}
\PYG{n}{tref} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{c+c1}{\PYGZsh{} 不応期 (ms)}

\PYG{n}{R} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{c+c1}{\PYGZsh{}膜抵抗 }
\PYG{n}{vrest} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{60.0} \PYG{c+c1}{\PYGZsh{} 静止膜電位 (mV) }
\PYG{n}{vthr} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{40.0} \PYG{c+c1}{\PYGZsh{} 閾値電位 (mV)}
\PYG{n}{rate} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{N}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{R}\PYG{o}{*}\PYG{n}{I}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{/} \PYG{p}{(}\PYG{n}{R}\PYG{o}{*}\PYG{n}{I}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{+} \PYG{n}{vrest} \PYG{o}{\PYGZhy{}} \PYG{n}{vthr}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{z} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}
        \PYG{n}{rate}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{/} \PYG{p}{(}\PYG{n}{tref} \PYG{o}{+} \PYG{n}{tc\PYGZus{}m} \PYG{o}{*} \PYG{n}{log}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{1e3}
    \PYG{k}{else}
        \PYG{n}{rate}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{end}
\PYG{n}{end} 
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{log}}の中身が0になるとErrorが生じるのでif文で場合分けをしている。なお、\sphinxcode{\sphinxupquote{1e3}}を乗じているのは1/msからHzに変換するためである。結果は次のようになる。数値的な計算結果とほぼ一致していることがわかる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{rate}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Input current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Firing rate (Hz)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{400}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\bigskip\hrule\bigskip



\subsection{2.5 Izhikevich モデル}
\label{\detokenize{2-5_iz:izhikevich}}\label{\detokenize{2-5_iz::doc}}

\subsubsection{2.5.1 Izhikevich モデルの定義}
\label{\detokenize{2-5_iz:id1}}
\sphinxstylestrong{Izhikevich モデル} (または\sphinxstylestrong{Simple model})は(\sphinxhref{https://www.izhikevich.org/publications/spikes.htm}{Izhikevich, 2003})で考案されたモデルである。HHモデルのような生理学的な知見に基づいたモデルは実際のニューロンの発火特性をよく再現できるが、式が複雑化するため、数学的な解析が難しく、計算量が増えるために大規模なシミュレーションも困難となる%
\begin{footnote}[1]\sphinxAtStartFootnote
これに関しては必ずしも正しくない。計算機の発達によりHHモデルで大きなモデルをシミュレーションすることも可能である。
%
\end{footnote}。そこで、生理学的な正しさには目をつぶり、生体内でのニューロンの発火特性を再現するモデルが求められた。その特徴を持つのがIzhikevich モデルである (以下ではIzモデルと表記する)。Izモデルは 2変数しかない%
\begin{footnote}[2]\sphinxAtStartFootnote
数値計算をする上では簡易的だが、if文が入るために解析をするのは難しくなる。(\sphinxhref{https://www.springer.com/gp/book/9781846280399}{Bernardo, et al., 2008})を読むといいらしい。
%
\end{footnote}簡素な微分方程式だが, 様々なニューロンの活動を模倣することができる。定式化には主に2種類ある。まず、(\sphinxhref{https://www.izhikevich.org/publications/spikes.htm}{Izhikevich, 2003})で提案されたのが次式である。
\begin{equation*}
\begin{split}
\begin{align}
\frac{dv(t)}{dt}&=0.04v(t)^2 + 5v(t)+140-u(t)+I(t) \\
\frac{du(t)}{dt}&=a(bv(t)-u(t))
\end{align} 
\end{split}
\end{equation*}
ここで、\(v\)と\(u\)が変数であり, \(v\)は膜電位(membrane potential;単位はmV), \(u\)は回復電流(recovery current; 単位はpA)%
\begin{footnote}[3]\sphinxAtStartFootnote
ここでの「回復」というのは脱分極した後の膜電位が静止膜電位へと戻る、という意味である (対義語はactivationで膜電位の上昇を意味する)。
\(u\)は\(v\)の導関数において\(v\)の上昇を抑制するように\(-u\)で入っているため、\(u\)としてはK\(^+\)チャネル電流やNa\(^+\)チャネルの不活性化動態などが考えられる。
%
\end{footnote}である。また、\(a\)は回復時定数(recovery time constant; 単位はms\(^{-1}\))の逆数 (これが大きいと\(u\)が元に戻る時間が短くなる), \(b\)は\(u\)の\(v\)に対する感受性(共鳴度合い,  resonance; 単位はpA/mV)である。

この式は簡便だが、生理学的な意味づけが分かりにくい。改善された式として\sphinxhref{https://mitpress.mit.edu/books/dynamical-systems-neuroscience}{”Dynamical Systems in Neuroscience” (Izhikevich, 2007)}のChapter 8で紹介されているのが次式である。
\begin{equation*}
\begin{split}
\begin{align}
C\frac{dv(t)}{dt}&=k\left(v(t)-v_r\right)\left(v(t)-v_t\right)-u(t)+I(t) \\
\frac{du(t)}{dt}&=a\left\{b\left(v(t)-v_{r}\right)-u(t)\right\}
\end{align} 
\end{split}
\end{equation*}
ここで、\(C\)は膜容量(membrane capacitance; 単位はpF), \(v_r\)は静止膜電位(resting membrane potential; 単位はmV), \(v_t\)は閾値電位(instantaneous threshold potential; 単位はmV), \(k\)はニューロンのゲインに関わる定数で、小さいと発火しやすくなる (単位はpA/mV)。以後はこちらの式を用いる。

Izモデルの\sphinxstylestrong{閾値の取り扱い}はLIFモデルと異なり、HHモデルに近い。LIFモデルでは閾値を超えた時に膜電位をピーク電位まで上昇させ (この過程は無くてもよい)、続いて膜電位をリセットする。Izモデルの閾値は\(v_t\)だが, 膜電位のリセットは閾値を超えたかで判断せず、膜電位\(v\)がピーク電位\(v_{\text{peak}}\)になったとき（または超えた時）に行う。そのためIzモデルの実際の閾値は膜電位の挙動が変化する(発火状態に移行する)、つまり分岐(bifurcation) が生じる点であり、パラメータの閾値\(v_t\)との間には差異がある。

さて、膜電位がピーク電位\(v_{\text{peak}}\)に達したとき (すなわち \sphinxcode{\sphinxupquote{if}} \(v \geq v_{\text{peak}}\))、\(u, v\)を次のようにリセットする%
\begin{footnote}[4]\sphinxAtStartFootnote
バースト発火(bursting)の挙動を表現するためには、速い回復変数(fast recovery variable)と遅い回復変数(slow recovery variable)の2つが必要となる(従って膜電位も合わせて全部で3変数必要)。一方で、IzモデルではLIFモデルのようなif文によるリセットを用いているため、速い回復変数が必要なく、遅い回復変数\(u\)のみでバースト発火を表現できる。
%
\end{footnote}。
\begin{equation*}
\begin{split}
\begin{align} 
u&\leftarrow u+d\\
v&\leftarrow v_{\text{reset}}
\end{align}
\end{split}
\end{equation*}
とする。ただし, \(v_{\text{reset}}\)は過分極を考慮して静止膜電位\(v_r\)よりも小さい値とする。また、\(d\)はスパイク発火中に活性化される正味の外向き電流の合計を表し、発火後の膜電位の挙動に影響する (単位はpA)。

以上を踏まえて, シミュレーションを行う。まず、必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\end{sphinxVerbatim}

変更しない定数を保持する\sphinxcode{\sphinxupquote{struct}}の\sphinxcode{\sphinxupquote{IZParameter}}と、変数を保持する\sphinxcode{\sphinxupquote{mutable struct}}の\sphinxcode{\sphinxupquote{IZ}}を作成する。2つの定式化でパラメータの値が異なるので注意すること。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{IZParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{C}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mi}{100}  \PYG{c+c1}{\PYGZsh{} 膜容量 (pF)}
    \PYG{n}{a}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.03} \PYG{c+c1}{\PYGZsh{} 回復時定数の逆数 (1/ms)}
    \PYG{n}{b}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2} \PYG{c+c1}{\PYGZsh{} u の v に対する共鳴度合い (pA/mV)}
    \PYG{n}{d}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mi}{100} \PYG{c+c1}{\PYGZsh{} 発火で活性化される正味の外向き電流 (pA)}
    \PYG{n}{k}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.7} \PYG{c+c1}{\PYGZsh{} ゲイン (pA/mV)}
    \PYG{n}{vthr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{40} \PYG{c+c1}{\PYGZsh{} 閾値電位 (mV)}
    \PYG{n}{vrest}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{60} \PYG{c+c1}{\PYGZsh{} 静止膜電位 (mV)}
    \PYG{n}{vreset}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{50} \PYG{c+c1}{\PYGZsh{} リセット電位 (mV)}
    \PYG{n}{vpeak}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mi}{35} \PYG{c+c1}{\PYGZsh{}　ピーク電位 (mV)}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{IZ}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{IZParameter} \PYG{o}{=} \PYG{n}{IZParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{N}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt32}
    \PYG{n}{v}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{n}{param}\PYG{o}{.}\PYG{n}{vrest}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{u}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
    \PYG{n}{fire}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{Bool}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{Bool}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

次に変数を更新する関数\sphinxcode{\sphinxupquote{update!}}を書く。LIFの場合と異なり、\sphinxcode{\sphinxupquote{v{[}i{]} \textgreater{}= vpeak}}であることに注意する (\sphinxcode{\sphinxupquote{v{[}i{]} \textgreater{}= vthr}}ではない)。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function update!(variable::IZ, param::IZParameter, I::Vector, dt)
    @unpack N, v, u, fire = variable
    @unpack C, a, b, d, k, vthr, vrest, vreset, vpeak = param
    @inbounds for i = 1:N
        v[i] += dt/C * (k*(v[i]\PYGZhy{}vrest)*(v[i]\PYGZhy{}vthr) \PYGZhy{} u[i] + I[i])
        u[i] += dt * (a * (b * (v[i]\PYGZhy{}vrest) \PYGZhy{} u[i]))
    end
    @inbounds for i = 1:N
        fire[i] = v[i] \PYGZgt{}= vpeak
        v[i] = ifelse(fire[i], vreset, v[i])
        u[i] += ifelse(fire[i], d, 0)
    end
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
update! (generic function with 1 method)
\end{sphinxVerbatim}


\subsubsection{2.5.2 Izhikevich モデルのシミュレーションの実行}
\label{\detokenize{2-5_iz:id6}}
いくつかの定数を設定してシミュレーションを実行する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(150f0 * ((t .\PYGZgt{} 50) \PYGZhy{} (t .\PYGZgt{} 200)) + 300f0 * ((t .\PYGZgt{} 250) \PYGZhy{} (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} 記録用
varr = zeros(Float32, nt, N)
uarr = zeros(Float32, nt, N)

\PYGZsh{} modelの定義
neurons = IZ\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr[i, :] = neurons.v
    uarr[i, :] = neurons.u
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.116870 seconds (797.86 k allocations: 24.585 MiB, 6.62\PYGZpc{} gc time)
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{Plots}}を読み込み、膜電位\sphinxcode{\sphinxupquote{v}}, 回復変数\sphinxcode{\sphinxupquote{u}}, 入力電流\sphinxcode{\sphinxupquote{I}}を描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Plots}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{uarr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{title}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Regular Spiking (RS) Neurons}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Membrane}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ potential (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Recovery}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current (pA)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Injection}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current (pA)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{legend} \PYG{o}{=} \PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{l+m+mi}{400}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{2.5.3 様々な発火パターンのシミュレーション}
\label{\detokenize{2-5_iz:id7}}
次に様々な発火パターンを模倣するようにIzモデルの定数を変化させてみよう。Intrinsically Bursting (IB)ニューロンとChattering (CH) ニューロン(または fast rhythmic bursting (FRB) ニューロン)のシミュレーションを行う。基本的には定数を変えるだけである。

\begin{sphinxadmonition}{note}{Note:}
本書で用いている式における発火パターンに対するパラメータは(\sphinxhref{https://www.izhikevich.org/publications/spikes.htm}{Izhikevich, 2003})では得られないが、\sphinxhref{https://mitpress.mit.edu/books/dynamical-systems-neuroscience}{”Dynamical Systems in Neuroscience” (Izhikevich, 2007)}には記載がある。他の発火パターンに関してはこの本を参照のこと。
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} 記録用
varr\PYGZus{}ib = zeros(Float32, nt, N)
varr\PYGZus{}ch = zeros(Float32, nt, N)

I = repeat(500f0 * ((t .\PYGZgt{} 50) \PYGZhy{} (t .\PYGZgt{} 200)) + 700f0 * ((t .\PYGZgt{} 250) \PYGZhy{} (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} IB neurons
neurons\PYGZus{}ib = IZ\PYGZob{}Float32\PYGZcb{}(N=N, 
    param=IZParameter\PYGZob{}Float32\PYGZcb{}(C = 150, a = 0.01, b = 5, k =1.2, d = 130, vrest = \PYGZhy{}75, vreset = \PYGZhy{}56, vthr = \PYGZhy{}45, vpeak = 50))

\PYGZsh{} CH neurons
neurons\PYGZus{}ch = IZ\PYGZob{}Float32\PYGZcb{}(N=N, 
    param=IZParameter\PYGZob{}Float32\PYGZcb{}(C = 50, a = 0.03, b = 1, k =1.5, d = 150, vrest = \PYGZhy{}60, vreset = \PYGZhy{}40, vthr = \PYGZhy{}40, vpeak = 35))

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons\PYGZus{}ib, neurons\PYGZus{}ib.param, I[i, :], dt)
    update!(neurons\PYGZus{}ch, neurons\PYGZus{}ch.param, I[i, :], dt)
    varr\PYGZus{}ib[i, :] = neurons\PYGZus{}ib.v
    varr\PYGZus{}ch[i, :] = neurons\PYGZus{}ch.v
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.052278 seconds (759.91 k allocations: 19.149 MiB)
\end{sphinxVerbatim}

これまでと異なり、モデルの定義時に\sphinxcode{\sphinxupquote{param}}を設定していることに注意しよう。最後に膜電位変化を描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr\PYGZus{}ib}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr\PYGZus{}ch}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p4} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p4}\PYG{p}{,}
    \PYG{n}{title} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IB Neurons}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{CH neurons}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Membrane}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ potential (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Injection}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current (pA)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{widths}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{legend} \PYG{o}{=} \PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,} \PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{2.5.4 ランダムネットワークのシミュレーション}
\label{\detokenize{2-5_iz:id8}}
1000個のIzニューロン(興奮性800個, 抑制性200個)によるランダムネットワークのシミュレーションを行う。これは(\sphinxhref{https://www.izhikevich.org/publications/spikes.htm}{Izhikevich, 2003})においてMATLABコードが示されており、それをJuliaに移植したものである。このシミュレーションではRS(regular spiking)ニューロンを興奮性細胞、FS(fast spiking)ニューロンを抑制性細胞のモデルとして用いている。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} Excitatory neurons    Inhibitory neurons
Ne = 800;               Ni = 200
re = rand(Ne,1);        ri = rand(Ni,1)
a = [0.02*ones(Ne,1);   0.02 .+ 0.08*ri]
b = [0.2*ones(Ne,1);    0.25 .\PYGZhy{} 0.05*ri]
c = [\PYGZhy{}65 .+ 15*re.\PYGZca{}2;   \PYGZhy{}65*ones(Ni,1)]
d = [8 .\PYGZhy{} 6*re.\PYGZca{}2;      2*ones(Ni,1)]
S = [0.5*rand(Ne+Ni,Ne) \PYGZhy{}rand(Ne+Ni,Ni)] \PYGZsh{} synaptic weight
v = \PYGZhy{}65*ones(Ne+Ni,1)   \PYGZsh{} Initial values of v
u = b .* v              \PYGZsh{} Initial values of u
firings = []            \PYGZsh{} spike timings

for t=1:1000 \PYGZsh{} simulation of 1000 ms
    I=[5*randn(Ne,1); 2*randn(Ni,1)] \PYGZsh{} thalamic input
    fired = findall(v[:, 1] .\PYGZgt{}= 30) \PYGZsh{} indices of spikes
    firings = t==1 ? [t .+ 0*fired fired] : [firings; [t .+ 0*fired fired]]
    v[fired]=c[fired]
    u[fired]=u[fired]+d[fired]
    I = I + sum(S[:,fired], dims=2)
    v = v .+0.5*(0.04*v.\PYGZca{}2+5*v .+140 \PYGZhy{}u+I) \PYGZsh{} step 0.5 ms for numerical stability
    v = v .+0.5*(0.04*v.\PYGZca{}2+5*v .+140 \PYGZhy{}u+I) 
    u = u+a.*(b.*v\PYGZhy{}u)
end
\end{sphinxVerbatim}

膜電位の更新の際、\sphinxcode{\sphinxupquote{v}}を2回に分けて更新しているが、これは数値的な安定性を高めるためである。計算量は上がるが、前述したモデルにおいても同様の処理を行う実装もある。

シミュレーションの実行後、ネットワークを構成するニューロンの発火を描画する。これを\sphinxstylestrong{ラスタープロット} (raster plot)という。この図は横軸が時間、縦軸がニューロンの番号となっており、各ニューロンが発火したことを点で表している。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{firings}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{firings}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{markersize}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{markercolor}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} 
    \PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Time (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{} neuron}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlim}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{)}
\end{sphinxVerbatim}

初めの400msぐらいまでは100msごとに10Hzの\(\alpha\)波が


\bigskip\hrule\bigskip



\subsection{2.8 Inter\sphinxhyphen{}spike interval モデル}
\label{\detokenize{2-8_isi:inter-spike-interval}}\label{\detokenize{2-8_isi::doc}}
これまで紹介したモデルでは、入力に対する膜電位などの時間変化に基づき発火が起こるかどうか、ということを考えてきた。この節では、発火が生じるまでの過程を考慮せず、発火の時間間隔(\sphinxstylestrong{inter\sphinxhyphen{}spike interval, ISI})の統計による現象論的モデルを考える。これを\sphinxstylestrong{Inter\sphinxhyphen{}spike interval (ISI)} モデルと呼ぶ。ISIモデルは\sphinxstylestrong{点過程(point process)} という統計的モデルに基づいており、各モデルにはISIが従う分布の名称がついている。

この節では、使用頻度の高い \sphinxstylestrong{ポアソン過程 (Poisson process) モデル}、ポアソン過程モデルにおいて不応期を考慮した \sphinxstylestrong{死時間付きポアソン過程 (Poisson process with dead time, PPD) モデル}、皮質の定常発火においてポアソン過程モデルよりも当てはまりがよいとされる \sphinxstylestrong{ガンマ過程 (Gamma process) モデル}について説明する。

なお、SNNにおいて、ISIモデルは主に画像入力の際に\sphinxstylestrong{連続値からスパイク列へのエンコード}に用いられる。これに限らず入力として用いられることが多い。


\subsubsection{2.8.1 ポアソン過程モデル}
\label{\detokenize{2-8_isi:id1}}

\paragraph{点過程とポアソン過程}
\label{\detokenize{2-8_isi:id2}}
時間に応じて変化する確率変数のことを\sphinxstylestrong{確率過程(stochastic process)} という。さらに確率過程の中で、連続時間軸上において離散的に生起する点事象の系列を\sphinxstylestrong{点過程(point process)} という。スパイクは離散的に起こるので、点過程を用いてモデル化ができるという話である。

ポアソン過程 (Poisson process)は点過程の1つである。ポアソン過程モデルはスパイクの発生をポアソン過程でモデル化したもので、このモデルによって生じるスパイクをポアソンスパイク(Poisson spike)と呼ぶ。ポアソン過程では、時刻\(t\)までに起こった点の数\(N(t)\)はポアソン分布に従う。すなわち、点が起こる確率が強度\(\lambda\)のポアソン分布に従う場合, 時刻\(t\)までに事象が\(n\)回起こる確率は\(P[N(t)=n]=\dfrac{(\lambda t)^{n}}{n !} e^{-\lambda t}\)となる。

ポアソン過程において点が起こる回数がポアソン分布に従うことは、ポアソン過程という名称の由来となっている。これを定義とする場合もあれば、次の4条件を満たす点過程をポアソン過程とするという定義もある。
\begin{itemize}
\item {} 
時刻0における初期の点の数は0 : \(P[N(0)=0]=1\)

\item {} 
\([t, t+\Delta t)\)に点が1つ生じる確率 : \(P[N(t+\Delta t)-N(t)=1]=\lambda(t)\Delta t+o(\Delta t)\)

\item {} 
微小時間\(\Delta t\)の間に点は2つ以上生じない : \(P[N(t+\Delta t)-N(t)=2]=o(\Delta t)\)

\item {} 
任意の時点\(t_1 < t_2 < \cdots< t_n\)に対して，増分 \(N(t_2)-N(t_1), N(t_3)-N(t_2), \cdots, N(t_n)-N(t_{n−1})\)は互いに独立である．

\end{itemize}

ただし, \(o(\cdot)\)はLandauの記号(Landauのsmall o)であり, \(o(x)\)は\(x\to 0\)のとき、\(o(x)/x\to 0\)となる微小な量を表す。ポアソン過程に従ってスパイクが生じるとする場合、条件2の強度関数\(\lambda(t)\)は\sphinxstylestrong{発火率}を意味する (また実装において有用)。条件3は不応期より小さいタイムステップにおいては、1つのタイムステップにおいて1つしかスパイクは生じないということを表す。条件4はスパイクは独立に発生する、ということを意味する。また、これらの条件から\(N(t)\)の分布は強度母数\(\lambda(t)\)のポアソン分布に従うことが示せる。

強度関数(点がスパイクの場合、発火率)が\(\lambda(t)=\lambda\) (定数)となる場合は点の時間間隔(点がスパイクの場合、ISI)の確率変数\(T\)が強度母数\(\lambda\)の \sphinxstylestrong{指数分布}に従う。なお、指数分布の確率密度関数は確率変数を\(T\)とするとき、
\begin{equation*}
\begin{split}
f(t;\lambda )=\left\{{\begin{array}{ll}\lambda e^{-\lambda t}&(t\geq 0)\\0&(t<0)\end{array}}\right.
\end{split}
\end{equation*}
となる。このことは4条件とChapman\sphinxhyphen{}Kolmogorovの式により求められるが、ややこしいので, \(P[N(t)=n]=\dfrac{(\lambda t)^{n}}{n !} e^{-\lambda t}\)から導出できることを簡単に示す。指数分布の累積分布関数を\(F(t; \lambda)\)とすると、
\begin{equation*}
\begin{split}
F(t; \lambda) = P(T<t)=1-P(T>t)=1-P(N(t)=0)=1-e^{-\lambda t}
\end{split}
\end{equation*}
となる。よって
\begin{equation*}
\begin{split}
f(t; \lambda)=\frac{dF(t; \lambda)}{dt}=\lambda e^{-\lambda t}
\end{split}
\end{equation*}
が成り立つ。


\paragraph{定常ポアソン過程}
\label{\detokenize{2-8_isi:id3}}
ここからポアソン過程によるスパイクのシミュレーションを実装する。実装方法にはISIが指数分布に従うことを利用したものと、ポアソン過程の条件2を利用したものの2通りがある。実装は後者が楽で計算量も少ないが、後のガンマ過程のために前者の実装を先に行う。


\subparagraph{ISIの累積により発火時刻を求める手法}
\label{\detokenize{2-8_isi:isi}}
ISIが指数分布に従うことを利用してポアソン過程モデルの実装を行う。まずISIを指数分布に従う乱数とする。次にISIを累積することで発火時刻を得る。最後に発火時間を整数値に丸めてindexとすることで\(\{0, 1\}\)のスパイク列が得られる。ISIの取得には\sphinxcode{\sphinxupquote{Random.randexp()}}を用いる。この関数は scale 1の指数分布に従う乱数を返す。このscaleは指数分布の確率密度関数を\(f(t; \frac{1}{\beta}) = \frac{1}{\beta} e^{-t/\beta}\)とした際の\(\beta = 1/\lambda\)である(この時、平均は\(\beta\)となる)。よって発火率を\sphinxcode{\sphinxupquote{fr}}(1/s), 単位時間を\sphinxcode{\sphinxupquote{dt}}(s)としたときのISIは \sphinxcode{\sphinxupquote{isi = 1/(fr*dt) * randexp()}}として得ることができる。

まず必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Random}
\PYG{n}{using} \PYG{n}{Plots}
\end{sphinxVerbatim}

乱数のseed値を設定し、必要な定数を定義した後に\sphinxcode{\sphinxupquote{isi}}を計算する。\sphinxcode{\sphinxupquote{isi}}を累積することでスパイクの生じた時刻を記録する配列\sphinxcode{\sphinxupquote{spike\_time}}を作成する。作成後、\sphinxcode{\sphinxupquote{spike\_time}}を用いてラスタープロットを描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Random.seed!(0) \PYGZsh{} set random seed

T = 1000 \PYGZsh{} ms
dt = 1f0 \PYGZsh{} ms
nt = Int32(T/dt) \PYGZsh{} number of timesteps

n\PYGZus{}neurons = 10 \PYGZsh{} ニューロンの数
fr = 30 \PYGZsh{} ポアソンスパイクの発火率(Hz)

isi = 1/(fr*dt*1e\PYGZhy{}3) * randexp(Int32(nt*1.5/fr), n\PYGZus{}neurons)
spike\PYGZus{}time = cumsum(isi, dims=1) \PYGZsh{} ISIを累積

\PYGZsh{} raster plot
p = plot(xlabel =\PYGZdq{}Time (ms)\PYGZdq{}, ylabel=\PYGZdq{}Neuron index\PYGZdq{}, xlim =(0, T+10), legend=false, size=(500, 200))
for i=1:n\PYGZus{}neurons
    scatter!(p, spike\PYGZus{}time[:, i], i*ones(Int32(nt*1.5/fr)), shape=:vline, color=\PYGZdq{}black\PYGZdq{})
end
display(p)
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{spike\_time}}のように発火時刻で記録しておく方がメモリを節約できるが、シミュレーションにおいてはスパイク列\(S\)はタイムステップごとに発火しているかを表す\(\{0,1\}\)配列で保持しておくと楽に扱うことができる。そのため冗長ではあるが、発火時刻の配列を\(\{0,1\}\)配列\sphinxcode{\sphinxupquote{spikes}}に変換しスパイクの数と発火率を計算する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike\PYGZus{}time}\PYG{p}{[}\PYG{n}{spike\PYGZus{}time} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{n}{nt} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{.}\PYG{o}{=} \PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} ntを超える場合を1に}
\PYG{n}{spike\PYGZus{}time} \PYG{o}{=} \PYG{n+nb}{round}\PYG{o}{.}\PYG{p}{(}\PYG{n}{Int32}\PYG{p}{,} \PYG{n}{spike\PYGZus{}time}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} float to int}
\PYG{n}{spikes} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{Bool}\PYG{p}{,} \PYG{n}{nt}\PYG{p}{,} \PYG{n}{n\PYGZus{}neurons}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} スパイク記録変数}

\PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{n\PYGZus{}neurons}    
    \PYG{n}{spikes}\PYG{p}{[}\PYG{n}{spike\PYGZus{}time}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]} \PYG{o}{.}\PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{end}

\PYG{n}{spikes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0} \PYG{c+c1}{\PYGZsh{} (spike\PYGZus{}time=1)の発火を削除}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Num. of spikes : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spikes}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Firing rate : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spikes}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{n}{n\PYGZus{}neurons}\PYG{o}{*}\PYG{n}{T}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mf}{1e3}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Hz}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Num. of spikes : 293
Firing rate : 29.3Hz
\end{sphinxVerbatim}


\subparagraph{\protect\(\Delta t\protect\) 間の発火確率が \protect\(\lambda\Delta t\protect\) であることを利用する方法}
\label{\detokenize{2-8_isi:delta-t-lambda-delta-t}}
次に2番目のポアソン過程モデルの実装を行う。こちらは\(\lambda\)を発火率とした場合, 区間\([t, t+\Delta t)\)の間にポアソンスパイクが発生する確率は\(\lambda \Delta t\)となることを利用する。これはポアソン過程の条件だが、ポアソン分布から導けることを簡単に示しておく。事象が起こる確率が強度\(\lambda\)のポアソン分布に従う場合, 時刻\(t\)までに事象が\(n\)回起こる確率は\(P[N(t)=n]=\dfrac{(\lambda t)^{n}}{n !} e^{-\lambda t}\)となる。よって, 微小時間\(\Delta t\)において事象が\(1\)回起こる確率は
\begin{equation*}
\begin{split}
P[N(\Delta t)=1]=\dfrac{\lambda \Delta t}{1 !} e^{-\lambda \Delta t}\simeq \lambda \Delta t+o(\Delta t)
\end{split}
\end{equation*}
となる。ただし, \(e^{-\lambda \Delta t}\)についてはマクローリン展開による近似を行っている。このことから, 一様分布\(U(0,1)\)に従う乱数\(\xi\)を取得し, \(\xi<\lambda dt\)なら発火\((y=1)\), それ以外では\((y=0)\)となるようにすればポアソンスパイクを実装できる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Random.seed!(0) \PYGZsh{} set random seed

T = 1000 \PYGZsh{} ms
dt = 1f0 \PYGZsh{} ms
nt = Int32(T/dt) \PYGZsh{} number of timesteps

n\PYGZus{}neurons = Int32(10) \PYGZsh{} ニューロンの数
fr = 30 \PYGZsh{} ポアソンスパイクの発火率(Hz)

spikes = rand(nt, n\PYGZus{}neurons) .\PYGZlt{} fr*dt*1e\PYGZhy{}3

println(\PYGZdq{}Num. of spikes : \PYGZdq{}, sum(spikes))
println(\PYGZdq{}Firing rate : \PYGZdq{}, sum(spikes)/(n\PYGZus{}neurons*T)*1e3, \PYGZdq{}Hz\PYGZdq{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Num. of spikes : 304
Firing rate : 30.4Hz
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{function} \PYG{n}{rasterplot}\PYG{p}{(}\PYG{n}{spikes}\PYG{p}{,} \PYG{n}{plotsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} input spike \PYGZhy{}\PYGZgt{} time, \PYGZsh{}neurons}
    \PYG{n}{spike\PYGZus{}inds} \PYG{o}{=} \PYG{n}{Tuple}\PYG{o}{.}\PYG{p}{(}\PYG{n}{findall}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{spikes}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{spike\PYGZus{}time} \PYG{o}{=} \PYG{n}{first}\PYG{o}{.}\PYG{p}{(}\PYG{n}{spike\PYGZus{}inds}\PYG{p}{)}
    \PYG{n}{neuron\PYGZus{}inds} \PYG{o}{=} \PYG{n}{last}\PYG{o}{.}\PYG{p}{(}\PYG{n}{spike\PYGZus{}inds}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} raster plot}
    \PYG{n}{scatter}\PYG{p}{(}\PYG{n}{spike\PYGZus{}time}\PYG{p}{,} \PYG{n}{neuron\PYGZus{}inds}\PYG{p}{,}
        \PYG{n}{xlabel} \PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Time (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Neuron index}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{shape}\PYG{o}{=}\PYG{p}{:}\PYG{n}{vline}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{plotsize}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rasterplot (generic function with 2 methods)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rasterplot}\PYG{p}{(}\PYG{n}{spikes}\PYG{p}{)}
\end{sphinxVerbatim}

なお、ここでは全時間における発火をまとめて計算しているが、タイムステップごとに発火の有無を計算することもできる。前者は発火情報を保持するためのメモリが必要だが、計算時間は短くなる。後者はメモリの節約になるが、計算時間は長くなる。そのため、これら2つの方法はメモリと計算時間のトレードオフとなる。また、他には発火情報を疎行列(sparse matrix)の形式で保持しておくとメモリの節約になると思われる。


\paragraph{非定常ポアソン過程}
\label{\detokenize{2-8_isi:id4}}
これまでの実装は発火率\(\lambda\)が一定であるとする、定常ポアソン過程 (homogeneous poisson process)であったが、ここからは発火率\(\lambda(t)\)が時間変化するとする\sphinxstylestrong{非定常ポアソン過程} (inhomogeneous poisson process)について考える。とはいえ、定常ポアソン過程における発火率を、時間についての関数で置き換えるだけで実装できる。以下は\(\lambda(t)=\sin^2(\alpha t)\)(ただし\(\alpha\)は定数)とした場合の実装である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Random.seed!(0) \PYGZsh{} set random seed

T = 1000 \PYGZsh{} ms
dt = 1f0 \PYGZsh{} ms
nt = Int32(T/dt) \PYGZsh{} number of timesteps

n\PYGZus{}neurons = Int32(10) \PYGZsh{} ニューロンの数

t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
fr = 30(sin.(1e\PYGZhy{}2t)).\PYGZca{}2 \PYGZsh{} ポアソンスパイクの発火率(Hz)

spikes = rand(nt, n\PYGZus{}neurons) .\PYGZlt{} fr*dt*1e\PYGZhy{}3

p1 = plot(t, fr, ylabel =\PYGZdq{}Firing rate (Hz)\PYGZdq{}, legend=false)
p2 = rasterplot(spikes)
plot(p1, p2, xlim=(0, T+10), layout = grid(2, 1, heights=[0.5, 0.5]), size=(500,300))
\end{sphinxVerbatim}

上が発火率\(\lambda(t)\)の時間変化, 下がラスタープロットである。


\subsubsection{2.8.2 死時間付きポアソン過程モデル (Poisson process with dead time, PPD)}
\label{\detokenize{2-8_isi:poisson-process-with-dead-time-ppd}}
ポアソン過程は簡易的で有用だが、不応期を考慮していない。そのため、時には生理的範疇を超えたバースト発火が起こる場合もある%
\begin{footnote}[1]\sphinxAtStartFootnote
複数のニューロンからの発火の重ね合わせ(superposition)であると考えることもできる。
%
\end{footnote}。そこで、ポアソン過程において不応期のようなイベントの生起が起こらない \sphinxstylestrong{死時間(dead time)} %
\begin{footnote}[2]\sphinxAtStartFootnote
例えば、ガイガー・カウンター(Geiger counter)などの放射線の検出器には放射線の到達を機器の物理的特性として検出できない時間(つまり死時間)がある。そのため放射線の到達数がポアソン分布に従うとした場合、放射線測定装置のモデルとしてPPDが用いられる。
%
\end{footnote}を考慮した\sphinxstylestrong{死時間付きポアソン過程 (Poisson process with dead time, PPD)} (またはdead time modified Poisson process)というモデルを導入する。

実装においてはLIFニューロンの時と同じような不応期の処理をする。つまり、現在が不応期かどうかを判断し、不応期なら発火を許可しないようにする。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Random.seed!(0) \PYGZsh{} set random seed

T = 1000 \PYGZsh{} ms
dt = 1f0 \PYGZsh{} ms
nt = Int32(T/dt) \PYGZsh{} number of timesteps
tref = 5f0 \PYGZsh{} 不応期 (ms)

n\PYGZus{}neurons = Int32(10) \PYGZsh{} ニューロンの数
fr = 30 \PYGZsh{} ポアソンスパイクの発火率(Hz)

tlast = zeros(n\PYGZus{}neurons) \PYGZsh{} 発火時刻の記録変数
spikes = zeros(nt, n\PYGZus{}neurons)

\PYGZsh{} simulation
@time for i=1:nt
    fire = rand(n\PYGZus{}neurons) .\PYGZlt{} fr*dt*1e\PYGZhy{}3
    spikes[i, :] = ((dt*i) .\PYGZgt{} (tlast .+ tref)) .* fire
    tlast[:] = tlast .* (1 .\PYGZhy{} fire) + dt*i * fire \PYGZsh{} 発火時刻の更新
end

println(\PYGZdq{}Num. of spikes : \PYGZdq{}, sum(spikes))
println(\PYGZdq{}Firing rate : \PYGZdq{}, sum(spikes)/(n\PYGZus{}neurons*T)*1e3, \PYGZdq{}Hz\PYGZdq{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.428621 seconds (1.60 M allocations: 77.271 MiB, 3.58\PYGZpc{} gc time)
Num. of spikes : 267.0
Firing rate : 26.700000000000003Hz
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{struct}}や\sphinxcode{\sphinxupquote{function}}を定義しても良いが、ここではforループ内に直接処理を書いた。forループ内に関しては以下のようにニューロンごとに処理しても良い (速度に大きな差はない)。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c}{\PYGZsh{} simulation}
\PYG{n+nd}{@time} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{:}\PYG{n}{nt}
    \PYG{n}{fire} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{n}{n\PYGZus{}neurons}\PYG{p}{)} \PYG{o}{.\PYGZlt{}} \PYG{n}{fr}\PYG{o}{*}\PYG{n}{dt}\PYG{o}{*}\PYG{l+m+mf}{1e\PYGZhy{}3}
    \PYG{k}{for} \PYG{n}{j}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{:}\PYG{n}{n\PYGZus{}neurons}
        \PYG{n}{spikes}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{p}{]} \PYG{o}{=} \PYG{n}{ifelse}\PYG{p}{(}\PYG{n}{dt}\PYG{o}{*}\PYG{n}{i} \PYG{o}{\PYGZgt{}} \PYG{n}{tlast}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{o}{+} \PYG{n}{tref}\PYG{p}{,} \PYG{n}{fire}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{tlast}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{o}{=} \PYG{n}{ifelse}\PYG{p}{(}\PYG{n}{fire}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dt}\PYG{o}{*}\PYG{n}{i}\PYG{o}{*}\PYG{n}{fire}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,} \PYG{n}{tlast}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} \PYG{c}{\PYGZsh{} 発火時刻の更新}
    \PYG{k}{end}
\PYG{k}{end}
\end{sphinxVerbatim}

また、不応期があるために発火率は設定値の30Hzよりも低くなっていることが分かる。次にラスタープロットを描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rasterplot}\PYG{p}{(}\PYG{n}{spikes}\PYG{p}{)}
\end{sphinxVerbatim}

通常のPoisson spikeと差はあまり感じられないが、高頻度発火の場合に通常のモデルとの違いが明瞭となる。


\subsubsection{2.8.3 ガンマ過程モデル}
\label{\detokenize{2-8_isi:id7}}
ガンマ過程(gamma process)は点の時間間隔がガンマ分布に従うとするモデルである。ガンマ過程はポアソン過程よりも皮質における定常発火への当てはまりが良いとされている (\sphinxhref{https://pubmed.ncbi.nlm.nih.gov/14629869/}{Shinomoto, et al., 2003}; \sphinxhref{https://pubmed.ncbi.nlm.nih.gov/19447097/}{Maimon \& Assad,2009})。

時間間隔の確率変数を\(T\)とした場合、ガンマ分布の確率密度関数は
\begin{equation*}
\begin{split}
\begin{equation}
f(t;k,\theta) =  t^{k-1}\frac{e^{-t/\theta}}{\theta^k\Gamma(k)}
\end{equation}
\end{split}
\end{equation*}
と表される。ただし、\(t > 0\)であり、 2つの母数は\(k, \theta > 0\)である。また、\(\Gamma (\cdot)\)はガンマ関数であり、
\begin{equation*}
\begin{split}
\begin{equation}
\Gamma (k)=\int _{0}^{\infty }x^{k-1}e^{-x}\,dx
\end{equation}
\end{split}
\end{equation*}
と定義される。ガンマ分布の平均は\(k\theta\)だが、発火率はISIの平均の逆数なので、\(\lambda=1/k\theta\)となる。また、\(k=1\)のとき、ガンマ分布は指数分布となる。さらに\(k\)が正整数のとき、ガンマ分布はアーラン分布となる。

ガンマ過程モデルの実装はポアソン過程モデルのISIを累積する手法と同様に書くことができる。ただしこの時、\sphinxhref{https://github.com/JuliaStats/Distributions.jl}{Distributions.jl}を用いる。基本的には\sphinxcode{\sphinxupquote{randexp(shape)}}を\sphinxcode{\sphinxupquote{rand(Gamma(a,b), shape)}}に置き換えればよい (もちろん多少の修正は必要とする)。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Distributions} 
\end{sphinxVerbatim}

スパイク列を生成する関数を書く。少々冗長なのが気になる点である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{function} \PYG{n}{GammaSpike}\PYG{p}{(}\PYG{n}{T}\PYG{p}{,} \PYG{n}{dt}\PYG{p}{,} \PYG{n}{n\PYGZus{}neurons}\PYG{p}{,} \PYG{n}{fr}\PYG{p}{,} \PYG{n}{k}\PYG{p}{)}
    \PYG{n}{nt} \PYG{o}{=} \PYG{n}{Int32}\PYG{p}{(}\PYG{n}{T}\PYG{o}{/}\PYG{n}{dt}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} number of timesteps}
    \PYG{n}{theta} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{n}{k}\PYG{o}{*}\PYG{p}{(}\PYG{n}{fr}\PYG{o}{*}\PYG{n}{dt}\PYG{o}{*}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} fr = 1/(k*theta)}

    \PYG{n}{isi} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{n}{Gamma}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,} \PYG{n}{theta}\PYG{p}{)}\PYG{p}{,} \PYG{n}{Int32}\PYG{p}{(}\PYG{n+nb}{round}\PYG{p}{(}\PYG{n}{nt}\PYG{o}{*}\PYG{l+m+mf}{1.5}\PYG{o}{/}\PYG{n}{fr}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{n\PYGZus{}neurons}\PYG{p}{)}
    \PYG{n}{spike\PYGZus{}time} \PYG{o}{=} \PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{isi}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} ISIを累積}

    \PYG{n}{spike\PYGZus{}time}\PYG{p}{[}\PYG{n}{spike\PYGZus{}time} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{n}{nt} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{.}\PYG{o}{=} \PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} ntを超える場合を1に}
    \PYG{n}{spike\PYGZus{}time} \PYG{o}{=} \PYG{n+nb}{round}\PYG{o}{.}\PYG{p}{(}\PYG{n}{Int32}\PYG{p}{,} \PYG{n}{spike\PYGZus{}time}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} float to int}
    \PYG{n}{spikes} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{Bool}\PYG{p}{,} \PYG{n}{nt}\PYG{p}{,} \PYG{n}{n\PYGZus{}neurons}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} スパイク記録変数}

    \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{n\PYGZus{}neurons}    
        \PYG{n}{spikes}\PYG{p}{[}\PYG{n}{spike\PYGZus{}time}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]} \PYG{o}{.}\PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{n}{end}

    \PYG{n}{spikes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0} \PYG{c+c1}{\PYGZsh{} (spike\PYGZus{}time=1)の発火を削除}
    \PYG{k}{return} \PYG{n}{spikes}
\PYG{n}{end}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
GammaSpike (generic function with 1 method)
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{GammaSpike}} 関数を用いて \(k=1, 12\) の場合のシミュレーションを実行する。なお、\(k=1\)のときはポアソン過程に一致することに注意しよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Random.seed!(0) \PYGZsh{} set random seed

T = 1000 \PYGZsh{} ms
dt = 1f0 \PYGZsh{} ms
nt = Int32(T/dt) \PYGZsh{} number of timesteps

n\PYGZus{}neurons = 10 \PYGZsh{} ニューロンの数
fr = 30 \PYGZsh{} ガンマスパイクの発火率(Hz)

\PYGZsh{} k=1のときはポアソン過程に一致
spikes1 = GammaSpike(T, dt, n\PYGZus{}neurons, fr, 1)
spikes2 = GammaSpike(T, dt, n\PYGZus{}neurons, fr, 12)

println(\PYGZdq{}Num. of spikes : \PYGZdq{}, sum(spikes1), \PYGZdq{}, \PYGZdq{},sum(spikes2))
println(\PYGZdq{}Firing rate : \PYGZdq{}, sum(spikes1)/(n\PYGZus{}neurons*T)*1e3, \PYGZdq{}Hz, \PYGZdq{}, sum(spikes2)/(n\PYGZus{}neurons*T)*1e3, \PYGZdq{}Hz\PYGZdq{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Num. of spikes : 289, 301
Firing rate : 28.9Hz, 30.099999999999998Hz
\end{sphinxVerbatim}

ISIの分布を描画するための関数を定義する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function GammaISIplot(dt, fr, k, n=1000)
    theta = 1/(k*(fr*dt*1e\PYGZhy{}3)) \PYGZsh{} fr = 1/(k*theta)
    isi = rand(Gamma(k, theta), n)
    gamma\PYGZus{}pdf = pdf.(Gamma(k, theta), minimum(isi):maximum(isi))

    p = plot(legend=false, xlabel=\PYGZdq{}ISI (ms)\PYGZdq{}, ylabel=\PYGZdq{}Density\PYGZdq{})
    histogram!(p, isi, normed=true)
    plot!(p, minimum(isi):maximum(isi), gamma\PYGZus{}pdf, color=\PYGZdq{}black\PYGZdq{})
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
GammaISIplot (generic function with 2 methods)
\end{sphinxVerbatim}

結果を尿がする。上段はISIの分布、下段はラスタープロットである。左の\(k=1\)の場合をポアソン過程モデルのスパイク列と比較しよう (同じ外観になっていることが分かる)。右は\(k=12\)とした場合である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{GammaISIplot}\PYG{p}{(}\PYG{n}{dt}\PYG{p}{,} \PYG{n}{fr}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{GammaISIplot}\PYG{p}{(}\PYG{n}{dt}\PYG{p}{,} \PYG{n}{fr}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{rasterplot}\PYG{p}{(}\PYG{n}{spikes1}\PYG{p}{)}
\PYG{n}{p4} \PYG{o}{=} \PYG{n}{rasterplot}\PYG{p}{(}\PYG{n}{spikes2}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p4}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{widths}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{legend} \PYG{o}{=} \PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,} \PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

なお、前述したようにガンマ過程モデルの方がポアソン過程モデルよりも皮質ニューロンのモデルとしては優れているが、入力画像のエンコーディングをガンマ過程モデルにすることでSNNの認識精度が向上するかどうかはまだ十分に研究されていない。また、(\sphinxhref{https://pubmed.ncbi.nlm.nih.gov/21964584/}{Deger, et al., 2012})ではPPDやガンマ過程の重ね合わせによるスパイク列を生成するアルゴリズムを考案している。


\bigskip\hrule\bigskip



\section{3. シナプス伝達のモデル}
\label{\detokenize{3_intro:id1}}\label{\detokenize{3_intro::doc}}

\subsection{まえがき}
\label{\detokenize{preface:id1}}\label{\detokenize{preface::doc}}
何故このサイトを作ろうと思ったか、というと鬱憤が溜まっていたためである。\sphinxstylestrong{物理シミュレーションの本はあるのに神経科学のシミュレーションの本は何故無いのか？} もちろん、洋書を探せば何冊かあるが、ブラックボックス的なライブラリを用いたり、幅広い分野の内容を扱っていなかったりする。数理モデルを扱う洋書・和書の名著は多数あるが、実装まで記述してくれない本がほとんどである。そのくせ、関連分野のニューラルネットワークの本は腐るほど出版されている (というと著者の方々に大変失礼だが)。有名な論文でも再現するためのパラメータの記載が不足していたり、著者実装がなかったり、あっても古いMATLAB実装は動かない。シミュレーションを実施する講義を受けたいと思っても医学部のカリキュラムで試験と被る。潜りも難しい。所属している研究室は理論や数値計算がメインではないので研究室の「秘伝のタレ」があるわけでもない。Twitterで流れてくる講義の情報を見ても「どうして資料を公開してくれないんだ」と嘆きながら枕を濡らすしかない。

「本が無いなら本を書けばいいじゃない」という言葉は…今作った言葉だが、一先ずこれまでに自分が書いてきたコードやノート (\sphinxhref{https://salad-bowl-of-knowledge.github.io/hp/}{ブログ}、\sphinxhref{https://compneuro-julia.github.io/\#spiking-neural-networks}{同人誌 (SNN本)}などを含む)をまとめるだけでも良い資料になるのではないか、という中々思い上がった考えからこのサイトを始めた (目次を見るとかなり無謀なことが察せられる。何事も勢いで初めてはいけない)。このサイトの元となったSNN本が「SNNを如何に学習させるか」ということに重きを置いた同人誌であったため、このサイトでも「学習」に重きを置く予定である。

目標とするのは\sphinxstylestrong{幅広い分野を取り扱い、数式や理論も書いていて、実装を掲載しているサイト}である。実装も線形代数の演算は許容するが、できるだけブラックボックスな演算を無くし、「手を動かして学ぶ」を基本とする。また、可読性が高く、高速に実行出来て研究にも使える実装が望ましい (もちろん、現時点でこれが実現できているとは思わない)。

問題はサイトで使用するプログラミング言語であった。一体何を使えばいいだろうか？神経科学ではMATLABがよく使われているし、関連分野の機械学習ではPythonがよく使われる。ただ、MATLABはライセンスが必要だし、Pythonが数値計算に向いているとは思えない。高速に実行したいし、これらのユーザーが簡単に移行できる言語を考えたとき、\sphinxhref{https://julialang.org/}{Julia}が望ましいという結論に至った。

\sphinxhref{https://www.geidai.ac.jp/~marui/julialang/why\_we\_created\_julia/index.html}{欲張りな言語であるJulia}は欲張りなこのサイトの理念に合致している。Juliaはまだ発展途上ではあるが、ユーザーも増えており、このサイトが一通り完成するころ (3～5年後?) にはかなり良い環境が整っているはずである。

サイトの作成に関しては\sphinxhref{https://jupyterbook.org/intro.html}{Jupyter book}を用いている。Jupyter bookはJupyter notebookやmarkdownをちゃんとしたサイトの形式に変換してくれ、大変便利である。

内容に関しての疑問点や指摘に関しては、各記事末尾のコメント欄からお願いしたい (\sphinxhref{https://utteranc.es/}{utterances.es}を用いているためGitHubアカウントが必要である)。他に、もしこのサイトに協力してくれるという方がいれば、Twitter(\sphinxhref{https://twitter.com/tak\_yamm}{@tak\_yamm})でDMをしていただければ幸いである。

なお、知り合いの方向けに言っておくと、このサイトはあくまで趣味であって、勉強や研究や仕事の方を優先するので安心してほしい。などという言葉を試験3日前に書いている私であった。

2020年7月24日山本 拓都


\subsection{1.2 記号の表記}
\label{\detokenize{notation:id1}}\label{\detokenize{notation::doc}}
このサイトは次のような記号表記を用いている。
\begin{itemize}
\item {} 
実数全体を\(\mathbb{R}\), 複素数全体は\(\mathbb{C}\)と表記する。

\item {} 
スカラーは小文字・斜体で \(x\) のように表記する。

\item {} 
ベクトルは小文字・立体・太字で \(\mathbf{x}\) のように表記し、列ベクトル (縦ベクトル) として扱う。

\item {} 
行列やテンソルは大文字・立体・太字で \(\mathbf{X}\) のように表記する。

\item {} 
\(n\times 1\)の実ベクトルの集合を \(\mathbb{R}^n\), \(n\times m\) の実行列の集合を \(\mathbb{R}^{n\times m}\)と表記する。

\item {} 
行列 \(\mathbf{X}\) の置換は \(\mathbf{X}^T\)と表記する。ベクトルの要素を表す場合は \(\mathbf{x} = (x_1, x_2,\cdots, x_n)^T\)のように表記する。

\item {} 
単位行列を \(\mathbf{I}\) と表記する。

\item {} 
ゼロベクトルは \(\mathbf{0}\) , 要素が全て1のベクトルは \(\mathbf{1}\) と表記する。

\item {} 
\(e\)を自然対数の底とし、指数関数を \(e^x=\exp(x)\)と表記する。また、自然対数を \(\ln(x)\)と表記する。

\item {} 
平均 \(\mu\), 標準偏差 \(\sigma\) の正規分布を \(\mathcal{N}(\mu, \sigma^2)\) と表記する。

\end{itemize}


\subsection{2.2 Hodgkin\sphinxhyphen{}Huxleyモデル}
\label{\detokenize{2-2_hodgkinhuxley:hodgkin-huxley}}\label{\detokenize{2-2_hodgkinhuxley::doc}}

\subsubsection{2.2.1 Hodgkin\sphinxhyphen{}Huxleyモデルにおける膜の等価回路モデル}
\label{\detokenize{2-2_hodgkinhuxley:id1}}
\sphinxstylestrong{Hodgkin\sphinxhyphen{}Huxleyモデル} (HH モデル)は, A.L. HodgkinとA.F. Huxleyによって1952年に考案されたニューロンの膜興奮を表すモデルである (\sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/}{Hodgkin \& Huxley, 1952})。Hodgkinらはヤリイカの巨大神経軸索に対する\sphinxstylestrong{電位固定法}(voltage\sphinxhyphen{}clamp)を用いた実験を行い, 実験から得られた観測結果を元にモデルを構築した。

HHモデルには等価な電気回路モデルがあり, \sphinxstylestrong{膜の並列等価回路モデル} (parallel conductance model)と呼ばれている。膜の並列等価回路モデルでは, ニューロンの細胞膜をコンデンサ, 細胞膜に埋まっているイオンチャネルを可変抵抗 (動的に変化する抵抗) として置き換える。

\sphinxstylestrong{イオンチャネル} (ion channel)は特定のイオン(例えばナトリウムイオンやカリウムイオンなど)を選択的に通す膜輸送体の一種である。それぞれのイオンの種類において, 異なるイオンチャネルがある (同じイオンでも複数の種類のイオンチャネルがある)。また, イオンチャネルにはイオンの種類に応じて異なる\sphinxstylestrong{コンダクタンス}(抵抗の逆数で電流の「流れやすさ」を意味する)と\sphinxstylestrong{平衡電位}(equilibrium potential)がある。HHモデルでは, ナトリウム(Na\(^{+}\))チャネル, カリウム(K\(^{+}\))チャネル, 漏れ電流(leak current)のイオンチャネルを仮定する。漏れ電流のイオンチャネルは当時特定できなかったチャネルで, 膜から電流が漏れ出すチャネルを意味する。なお, 現在では漏れ電流の多くはCl\(^{-}\)イオン(chloride ion)によることが分かっている。

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{parallel_conductance_model}.JPG}
\caption{Hodgkin\sphinxhyphen{}Huxleyモデルの膜の等価回路モデル}\label{\detokenize{2-2_hodgkinhuxley:parallel-conductance-model}}\end{figure}

それでは, 等価回路モデルを用いて電位変化の式を立ててみよう。上図において, \(C_m\)は細胞膜のキャパシタンス(膜容量), \(I_{m}(t)\)は細胞膜を流れる電流(外部からの入力電流), \(I_\text{Cap}(t)\)は膜のコンデンサを流れる電流, \(I_\text{Na}(t)\)及び \(I_K(t)\)はそれぞれナトリウムチャネルとカリウムチャネルを通って膜から流出する電流, \(I_\text{L}(t)\)は漏れ電流である。このとき,
\begin{equation*}
\begin{split}
I_{m}(t)=I_\text{Cap}(t)+I_\text{Na}(t)+I_\text{K}(t)+I_\text{L}(t)    
\end{split}
\end{equation*}
という仮定をしている。

膜電位を\(V(t)\)とすると, Kirchhoffの第二法則 (Kirchhoff’s Voltage Law)より,
\begin{equation*}
\begin{split}
\underbrace{C_m\frac {dV(t)}{dt}}_{I_\text{Cap} (t)}=I_{m}(t)-I_\text{Na}(t)-I_\text{K}(t)-I_\text{L}(t)
\end{split}
\end{equation*}
となる。Hodgkinらはチャネル電流\(I_\text{Na}, I_K, I_\text{L}\)が従う式を実験的に求めた。
\begin{equation*}
\begin{split}
\begin{aligned}
I_\text{Na}(t) &= g_{\text{Na}}\cdot m^{3}h(V-E_{\text{Na}})\\
I_\text{K}(t) &= g_{\text{K}}\cdot n^{4}(V-E_{\text{K}})\\
I_\text{L}(t) &= g_{\text{L}}(V-E_{\text{L}})
\end{aligned}
\end{split}
\end{equation*}
ただし, \(g_{\text{Na}}, g_{\text{K}}\)はそれぞれNa\(^+\), K\(^+\)の最大コンダクタンスである。\(g_{\text{L}}\)はオームの法則に従うコンダクタンスで, Lコンダクタンスは時間的に変化はしないと仮定する。また, \(m\)はNa\(^+\)コンダクタンスの活性化パラメータ, \(h\)はNa\(^+\)コンダクタンスの不活性化パラメータ, \(n\)はK\(^+\)コンダクタンスの活性化パラメータであり, ゲートの開閉確率を表している。よって, HHモデルの状態は\(V, m, h, n\)の4変数で表される。これらの変数は以下の\(x\)を\(m, n, h\)に置き換えた3つの微分方程式に従う。
\begin{equation*}
\begin{split}
\frac{dx}{dt}=\alpha_{x}(V)(1-x)-\beta_{x}(V)x
\end{split}
\end{equation*}
ただし, \(V\)の関数である\(\alpha_{x}(V),\ \beta_{x}(V)\)は\(m, h, n\)によって異なり, 次の6つの式に従う。
\begin{equation*}
\begin{split}
\begin{array}{ll}
\alpha_{m}(V)=\dfrac {0.1(25-V)}{\exp \left[(25-V)/10\right]-1}, &\beta_{m}(V)=4\exp {(-V/18)}\\
\alpha_{h}(V)=0.07\exp {(-V/20)}, & \beta_{h}(V)={\dfrac{1}{\exp {\left[(30-V)/10 \right]}+1}}\\
\alpha_{n}(V)={\dfrac {0.01(10-V)}{\exp {\left[(10-V)/10\right]}-1}},& \beta_{n}(V)=0.125\exp {(-V/80)} 
\end{array}
\end{split}
\end{equation*}
なお、この式は6.3℃の条件下においてイカの巨大軸索の活動から得たデータを用いて導かれたものであることに注意しよう。


\subsubsection{2.2.2 Hodgkin\sphinxhyphen{}Huxley モデルの定義}
\label{\detokenize{2-2_hodgkinhuxley:id2}}
これまでに説明した式を用いてHHモデルを実装する。まず必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\end{sphinxVerbatim}

変更しない定数を保持する \sphinxcode{\sphinxupquote{struct}} の \sphinxcode{\sphinxupquote{HHParameter}} と, 変数を保持する \sphinxcode{\sphinxupquote{mutable struct}} の \sphinxcode{\sphinxupquote{HH}} を作成する。\sphinxcode{\sphinxupquote{v, m, h, n}} はHH modelの4変数だが, \sphinxcode{\sphinxupquote{r}} はpre\sphinxhyphen{}synaptic dynamicsを表す変数である。詳細は3章で解説する。 定数は次のように設定する。

\textbackslash{}begin\{align*\}
C\_m=1.0, g\_\{\textbackslash{}text\{Na\}\}=120, g\_\{\textbackslash{}text\{K\}\}=36, g\_\{\textbackslash{}text\{L\}\}=0.3\textbackslash{}
E\_\{\textbackslash{}text\{Na\}\}=50.0, E\_\{\textbackslash{}text\{K\}\}=\sphinxhyphen{}77, E\_\{\textbackslash{}text\{L\}\}=\sphinxhyphen{}54.387
\textbackslash{}end\{align*\}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{Cm}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{c+c1}{\PYGZsh{} 膜容量(uF/cm\PYGZca{}2)}
    \PYG{n}{gNa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{120.0} \PYG{c+c1}{\PYGZsh{} Na+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{36.0} \PYG{c+c1}{\PYGZsh{} K+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.3} \PYG{c+c1}{\PYGZsh{} 漏れイオンの最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{ENa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{50.0} \PYG{c+c1}{\PYGZsh{} Na+ の平衡電位(mV)}
    \PYG{n}{EK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{77.0} \PYG{c+c1}{\PYGZsh{} K+ の平衡電位(mV)}
    \PYG{n}{EL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{54.387} \PYG{c+c1}{\PYGZsh{}漏れイオンの平衡電位(mV)}
    \PYG{n}{tr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.5} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{td}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{8.0} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{invtr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{tr}
    \PYG{n}{invtd}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{td}
    \PYG{n}{v0}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{20.0} \PYG{c+c1}{\PYGZsh{} mV}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{HH}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{HHParameter} \PYG{o}{=} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{N}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16}
    \PYG{n}{v}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{65.0}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{m}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{h}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{n}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.32}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{r}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

次に変数を更新する関数\sphinxcode{\sphinxupquote{update!}}を書く。ソルバーとしては陽的Euler法または4次のRunge\sphinxhyphen{}Kutta法を用いる。以下ではEuler法を用いている。Juliaではforループを用いて1つのニューロンごとにパラメータを更新する方がベクトルを用いるよりも高速である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function update!(variable::HH, param::HHParameter, I::Vector, dt)
    @unpack N, v, m, h, n, r = variable
    @unpack Cm, gNa, gK, gL, ENa, EK, EL, tr, td, invtr, invtd, v0 = param
    @inbounds for i = 1:N
        m[i] += dt * ((0.1(v[i]+40.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+40.0))))*(1.0 \PYGZhy{} m[i]) \PYGZhy{} 4.0exp(\PYGZhy{}(v[i]+65.0) / 18.0)*m[i])
        h[i] += dt * ((0.07exp(\PYGZhy{}0.05(v[i]+65.0)))*(1.0 \PYGZhy{} h[i]) \PYGZhy{} 1.0/(1.0 + exp(\PYGZhy{}0.1(v[i]+35.0)))*h[i])
        n[i] += dt * ((0.01(v[i]+55.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+55.0))))*(1.0 \PYGZhy{} n[i]) \PYGZhy{} (0.125exp(\PYGZhy{}0.0125(v[i]+65)))*n[i])
        v[i] += dt / Cm * (I[i] \PYGZhy{} gNa * m[i]\PYGZca{}3 * h[i] * (v[i] \PYGZhy{} ENa) \PYGZhy{} gK * n[i]\PYGZca{}4 * (v[i] \PYGZhy{} EK) \PYGZhy{} gL * (v[i] \PYGZhy{} EL))
        r[i] += dt * ((invtr \PYGZhy{} invtd) * (1.0 \PYGZhy{} r[i])/(1.0 + exp(\PYGZhy{}v[i] + v0)) \PYGZhy{} r[i] * invtd)
    end
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
update! (generic function with 1 method)
\end{sphinxVerbatim}


\subsubsection{2.2.3 Hodgkin\sphinxhyphen{}Huxleyモデルのシミュレーションの実行}
\label{\detokenize{2-2_hodgkinhuxley:id3}}
いくつかの定数を設定してシミュレーションを実行する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * ((t .\PYGZgt{} 50) \PYGZhy{} (t .\PYGZgt{} 200)) + 35f0 * ((t .\PYGZgt{} 250) \PYGZhy{} (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} 記録用
varr = zeros(Float32, nt, N)
gatearr = zeros(Float32, nt, 3, N)

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr[i, :] = neurons.v
    gatearr[i, 1, :] = neurons.m
    gatearr[i, 2, :] = neurons.h
    gatearr[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.217901 seconds (1.36 M allocations: 46.979 MiB, 3.69\PYGZpc{} gc time)
\end{sphinxVerbatim}

結果を表示するために \sphinxcode{\sphinxupquote{Plots}}を読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Plots}
\end{sphinxVerbatim}

ニューロンの膜電位 \sphinxcode{\sphinxupquote{v}}, ゲート変数 \sphinxcode{\sphinxupquote{m, h, n}}, 刺激電流 \sphinxcode{\sphinxupquote{I}}の描画をする。入力電流の単位は \(\mu\text{A/cm}^2\)である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

次項で用いるために発火回数を求める。\sphinxcode{\sphinxupquote{bitwise and}}を用いると楽である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Num. of spikes : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}spikes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Num. of spikes : 27
\end{sphinxVerbatim}

50msから200msまでで11回, 250msから400msまでで16回発火しているので発火回数は計27回であり、この結果は正しい。


\subsubsection{2.2.4 Frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve}
\label{\detokenize{2-2_hodgkinhuxley:frequency-current-f-i-curve}}
この項ではHodgkin\sphinxhyphen{}Huxleyモデルにおいて、入力電流に対する発火率がどのように変化するかを調べる。次のコードのように入力電流を徐々に増加させたときの発火率を見てみよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 1000 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps

N = 100 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
maxcurrent = 30
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = Array\PYGZob{}Float32\PYGZcb{}(range(1,maxcurrent,length=N)) \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr\PYGZus{}fi = zeros(Float32, nt, N)

\PYGZsh{} simulation
for i = 1:nt
    update!(neurons, neurons.param, I[:], dt)
    varr\PYGZus{}fi[i, :] = neurons.v
end
\end{sphinxVerbatim}

発火率を計算して結果を描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{rate} \PYG{o}{=} \PYG{n}{num\PYGZus{}spikes}\PYG{o}{/}\PYG{n}{T}\PYG{o}{*}\PYG{l+m+mf}{1e3}

\PYG{n}{plot}\PYG{p}{(}\PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{rate}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Input current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Firing rate (Hz)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{400}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

このような曲線を\sphinxstylestrong{frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve} (または neuronal input/output (I/O) curve)と呼ぶ。


\subsubsection{2.2.5  抑制後リバウンド (Postinhibitory rebound; PIR)}
\label{\detokenize{2-2_hodgkinhuxley:postinhibitory-rebound-pir}}
ニューロンは電流が流入することで膜電位が変化し, 膜電位がある一定の閾値を超えると発火が起こる, というのはニューロンの活動電位発生についての典型的な説明である。それではHHモデルの膜電位閾値はどのくらいの値になるのだろうか。答えは「\sphinxstylestrong{膜電位閾値は一定ではない}」である。それを示す現象として \sphinxstylestrong{抑制後リバウンド} (Postinhibitory rebound; PIR)がある。この時生じる発火を\sphinxstylestrong{リバウンド発火} (rebound spikes)
と呼ぶ。抑制後リバウンドは過分極性の電流の印加を止めた際に膜電位が静止膜電位に回復するのみならず, さらに脱分極をして発火をするという現象である。この現象が生じる要因として
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{アノーダルブレイク} (anodal break, またはanode break excitation; ABE)

\item {} 
遅いT型カルシウム電流 (slow T\sphinxhyphen{}type calcium current)

\end{enumerate}

がある (\sphinxhref{https://pubmed.ncbi.nlm.nih.gov/15324089/}{Chik et al., 2004})。HH モデルはこのうちアノーダルブレイクを再現できるため, シミュレーションによりどのような現象か確認してみよう。これは入力電流を変更するだけで行える。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * (\PYGZhy{}(t .\PYGZgt{} 50) + (t .\PYGZgt{} 200)) + 20f0 * (\PYGZhy{}(t .\PYGZgt{} 250) + (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr2 = zeros(Float32, nt, N)
gatearr2 = zeros(Float32, nt, 3, N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr2[i, :] = neurons.v
    gatearr2[i, 1, :] = neurons.m
    gatearr2[i, 2, :] = neurons.h
    gatearr2[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.074358 seconds (848.89 k allocations: 19.134 MiB)
\end{sphinxVerbatim}

結果は次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Injection}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

なぜこのようなことが起こるか, というと過分極の状態から静止膜電位へと戻る際にNa\(^+\)チャネルが活性化 (Na\(^+\)チャネルの活性化パラメータ\(m\)が増加し, 不活性化パラメータ\(h\)が減少)し, 膜電位が脱分極することで再度Na\(^+\)チャネルが活性化する, というポジティブフィードバック過程(\sphinxstylestrong{自己再生的過程})に突入するためである (もちろん, この過程は通常の活動電位発生のメカニズムである)。 この際, 発火に必要な閾値が膜電位の低下に応じて下がった, ということもできる。

このように膜電位閾値は一定ではない。しかし, この後の節で紹介するLIFモデルなどでは簡略化のためにif文を用い, 膜電位閾値を超えたから発火, というものもある。実際には違うということを頭の片隅に残しながら読み進めることを推奨する。

\begin{sphinxadmonition}{note}{Note:}
PIRに関連する現象として抑制後促通 (Postinhibitory facilitation; PIF)がある。これは抑制入力の後に興奮入力がある一定の時間内で入ると発火が起こるという現象である (\sphinxhref{http://www.brain.riken.jp/en/summer/prev/2006/files/j\_rinzel04.pdf}{Dolda et al., 2006}, \sphinxhref{https://link.springer.com/referenceworkentry/10.1007\%2F978-1-4614-7320-6\_152-1}{Dodla, 2014})。
\end{sphinxadmonition}


\subsection{3.1 シナプス伝達}
\label{\detokenize{3-1_synapse:id1}}\label{\detokenize{3-1_synapse::doc}}
\begin{sphinxadmonition}{note}{Note:}
内容は大幅に追記が必要。
\end{sphinxadmonition}

スパイクが生じたことによる膜電位変化は軸索を伝播し, \sphinxstylestrong{シナプス}という構造により, 次のニューロンへと興奮が伝わる. このときの伝達の仕組みとして, シナプスには\sphinxstylestrong{化学シナプス}(chemical synapse)とGap junctionによる\sphinxstylestrong{電気シナプス}(electrical synapse)がある。

化学シナプスの場合, シナプス前膜からの\sphinxstylestrong{神経伝達物質}の放出, シナプス後膜の受容体への神経伝達物質の結合, イオンチャネル開口による\sphinxstylestrong{シナプス後電流}(postsynaptic current; PSC)の発生, という過程が起こる。かなり簡略化して書いたが, 実際にはかなりの過程を含くむ。しかし, これらの過程を全てモデル化するのは計算量がかなり大きくなるので, 基本的には簡易的な現象論的なモデルを用いる。

このように, シナプス前細胞のスパイク列(spike train)は次のニューロンにそのまま伝わるのではなく, ある種の時間的フィルターをかけられて伝わる。このフィルターを\sphinxstylestrong{シナプスフィルター}(synaptic filter)と呼ぶ。3章では, このようにシナプス前細胞で生じた発火が, シナプス後細胞の膜電位に与える過程のモデルについて説明する。


\subsection{11.1 ベイズ統計の基礎}
\label{\detokenize{11-1_bayes_statistics:id1}}\label{\detokenize{11-1_bayes_statistics::doc}}
この節では本章で用いるベイズ統計の基礎的概念の説明を行う。

\begin{sphinxadmonition}{note}{Note:}
悪いこと言わないので\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/index-j.html}{渡辺澄夫先生のHP}の講義録、特に\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/joho-gakushu6.html}{ベイズ統計入門}を読もう。
\end{sphinxadmonition}


\subsection{JuliaのTips集}
\label{\detokenize{tips:juliatips}}\label{\detokenize{tips::doc}}
このページはJuliaでの実装におけるTips (詰まったところの解決策)をまとめたものである。体系的にまとまってはいない。


\subsubsection{1. 関数名の!記号}
\label{\detokenize{tips:id1}}
単なる\sphinxstylestrong{慣習}として関数への入力を変更する場合に!を付ける。

関数内で配列を変更する場合には注意が必要である。以下に入力された配列を同じサイズの要素1の配列で置き換える、ということを目的として書かれた2つの関数がある。違いは\sphinxcode{\sphinxupquote{v}}の後に\sphinxcode{\sphinxupquote{{[}:{]}}}としているかどうかである。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function wrong!(A::Array)
    a = ones(size(a))
end

function right!(a::Array)
    a[:] = ones(size(a))
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
right! (generic function with 2 methods)
\end{sphinxVerbatim}

実行すると\sphinxcode{\sphinxupquote{wrong!}}の場合には入力された配列が変更されていないことがわかる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
using Random
v = rand(2, 2)
print(\PYGZdq{}v : \PYGZdq{}, v)

wrong!(v)
print(\PYGZdq{}\PYGZbs{}nwrong : \PYGZdq{}, v)

right!(v)
print(\PYGZdq{}\PYGZbs{}nright : \PYGZdq{}, v)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
v : [0.42692589476552056 0.34730711983824536; 0.9208034639794132 0.41699915079158134]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+ne}{UndefVarError}: wrong! not defined

\PYG{n+ne}{Stacktrace}:
 \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{n}{top}\PYG{o}{\PYGZhy{}}\PYG{n}{level} \PYG{n}{scope} \PYG{n}{at} \PYG{n}{In}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}\PYG{l+m+mi}{5}
 \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{n}{include\PYGZus{}string}\PYG{p}{(}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Function}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{Module}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{)} \PYG{n}{at} \PYG{o}{.}\PYGZbs{}\PYG{n}{loading}\PYG{o}{.}\PYG{n}{jl}\PYG{p}{:}\PYG{l+m+mi}{1091}
\end{sphinxVerbatim}


\subsubsection{2. 配列の1次元化}
\label{\detokenize{tips:id2}}
配列を一次元化(flatten)する方法。まずは3次元配列を作成する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2×2 Array\PYGZob{}Float64,3\PYGZcb{}:
[:, :, 1] =
 0.997107  0.0827965
 0.394606  0.847174

[:, :, 2] =
 0.0305417  0.448929
 0.270553   0.601419
\end{sphinxVerbatim}

用意されている\sphinxcode{\sphinxupquote{flatten}}を素直に用いると次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{Base}\PYG{n+nn}{.}\PYG{n+nn}{Iterators}\PYG{p}{:} \PYG{n}{flatten}
\PYG{n}{collect}\PYG{p}{(}\PYG{n}{flatten}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}

ただし、単に\sphinxcode{\sphinxupquote{B{[}:{]}}}とするだけでもよい。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}


\subsubsection{3. 行列の行・列ごとの正規化}
\label{\detokenize{tips:id3}}
シミュレーションにおいてニューロン間の重み行列を行あるいは列ごとに正規化 (weight normalization)する場合がある。これは各ニューロンへの入力の大きさを同じにする働きや重みの発散を防ぐ役割がある。以下では行ごとの和を1にする。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.166781  0.0552745  0.973622
 0.100336  0.753766   0.370005
 0.378222  0.165159   0.471515
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Wnormed} \PYG{o}{=} \PYG{n}{W} \PYG{o}{.}\PYG{o}{/} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{W}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.258439  0.0567384  0.536389
 0.155479  0.773729   0.203843
 0.586082  0.169533   0.259767
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{Wnormed}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1.0 1.0 1.0]
\end{sphinxVerbatim}


\subsubsection{4. 行列の結合 (concatenate)}
\label{\detokenize{tips:concatenate}}
行列の結合はMATLABに近い形式で行うことができる。まず、2つの行列A, Bを用意する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{l+m+mi}{3} \PYG{l+m+mi}{4}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{4} \PYG{l+m+mi}{5} \PYG{l+m+mi}{6}\PYG{p}{;} \PYG{l+m+mi}{7} \PYG{l+m+mi}{8} \PYG{l+m+mi}{9}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×3 Array\PYGZob{}Int64,2\PYGZcb{}:
 4  5  6
 7  8  9
\end{sphinxVerbatim}


\paragraph{4.1 水平結合 (Horizontal concatenation)}
\label{\detokenize{tips:horizontal-concatenation}}
\sphinxcode{\sphinxupquote{hcat}}を使うやり方と、\sphinxcode{\sphinxupquote{{[} {]}}}を使うやり方がある。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H1} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,}\PYG{n}{B}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

なお、MATLABのように次のようにすると正しく結合はされない。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H3} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{,} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2\PYGZhy{}element Array\PYGZob{}Array\PYGZob{}Int64,2\PYGZcb{},1\PYGZcb{}:
 [1 2; 3 4]
 [4 5 6; 7 8 9]
\end{sphinxVerbatim}


\paragraph{4.2 垂直結合 (Vertical concatenation)}
\label{\detokenize{tips:vertical-concatenation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V1} \PYG{o}{=} \PYG{n}{vcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{V2} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;}\PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×4 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  1  2
 3  4  3  4
 4  7  4  7
 5  8  5  8
 6  9  6  9
\end{sphinxVerbatim}


\subsubsection{5. 配列に新しい軸を追加}
\label{\detokenize{tips:id4}}
要はnumpyでの\sphinxcode{\sphinxupquote{A{[}None, :{]}}}や\sphinxcode{\sphinxupquote{A{[}np.newaxis, :{]}}}のようなことがしたい場合。やや面倒だが、\sphinxcode{\sphinxupquote{reshape}}を使うか、\sphinxcode{\sphinxupquote{{[}CartesianIndex(){]}}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.3487808667518093
 0.717776723811032
 0.3369916406749496
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{newaxis} \PYG{o}{=} \PYG{p}{[}\PYG{n}{CartesianIndex}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{v1} \PYG{o}{=} \PYG{n}{v}\PYG{p}{[}\PYG{n}{newaxis}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.348781  0.717777  0.336992
\end{sphinxVerbatim}


\subsubsection{6. Array\{Array\{Float64, x\},1\}をArray\{Float64, x+1\}に変換}
\label{\detokenize{tips:array-array-float64-x-1-array-float64-x-1}}
numpyでは\sphinxcode{\sphinxupquote{array({[}matrix for i in range(){]})}}などを用いると、1次元配列のリストを2次元配列に変換できた。Juliaでも同様にする場合は\sphinxcode{\sphinxupquote{hcat(...)}}や\sphinxcode{\sphinxupquote{cat(...)}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,1\PYGZcb{},1\PYGZcb{}
Size : (5,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A2} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : LinearAlgebra.Adjoint\PYGZob{}Float64,Array\PYGZob{}Float64,2\PYGZcb{}\PYGZcb{}
Size : (5, 3)
\end{sphinxVerbatim}

以下は多次元配列の場合。\sphinxcode{\sphinxupquote{cat(...)}}で配列を結合し、\sphinxcode{\sphinxupquote{permitedims}}で転置する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,3\PYGZcb{},1\PYGZcb{}
Size : (6,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B2} \PYG{o}{=} \PYG{n}{permutedims}\PYG{p}{(}\PYG{n}{cat}\PYG{p}{(}\PYG{n}{B1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Float64,4\PYGZcb{}
Size : (6, 3, 4, 5)
\end{sphinxVerbatim}


\subsection{3.1 シナプス伝達}
\label{\detokenize{3-1_synapse:id1}}\label{\detokenize{3-1_synapse::doc}}
\begin{sphinxadmonition}{note}{Note:}
内容は大幅に追記が必要。
\end{sphinxadmonition}

スパイクが生じたことによる膜電位変化は軸索を伝播し, \sphinxstylestrong{シナプス}という構造により, 次のニューロンへと興奮が伝わる. このときの伝達の仕組みとして, シナプスには\sphinxstylestrong{化学シナプス}(chemical synapse)とGap junctionによる\sphinxstylestrong{電気シナプス}(electrical synapse)がある。

化学シナプスの場合, シナプス前膜からの\sphinxstylestrong{神経伝達物質}の放出, シナプス後膜の受容体への神経伝達物質の結合, イオンチャネル開口による\sphinxstylestrong{シナプス後電流}(postsynaptic current; PSC)の発生, という過程が起こる。かなり簡略化して書いたが, 実際にはかなりの過程を含くむ。しかし, これらの過程を全てモデル化するのは計算量がかなり大きくなるので, 基本的には簡易的な現象論的なモデルを用いる。

このように, シナプス前細胞のスパイク列(spike train)は次のニューロンにそのまま伝わるのではなく, ある種の時間的フィルターをかけられて伝わる。このフィルターを\sphinxstylestrong{シナプスフィルター}(synaptic filter)と呼ぶ。3章では, このようにシナプス前細胞で生じた発火が, シナプス後細胞の膜電位に与える過程のモデルについて説明する。


\subsection{3.1 Current\sphinxhyphen{}based vs Conductance\sphinxhyphen{}based シナプス}
\label{\detokenize{3-2_current-conductance-synapse:current-based-vs-conductance-based}}\label{\detokenize{3-2_current-conductance-synapse::doc}}

\subsubsection{3.1.1 化学シナプスの2つの記述形式}
\label{\detokenize{3-2_current-conductance-synapse:id1}}
具体的なシナプスのモデルの前に, この節では化学シナプスにおけるシナプス入力(synaptic drive)の2つの形式, \sphinxstylestrong{Current\sphinxhyphen{}based シナプス}と\sphinxstylestrong{Conductance\sphinxhyphen{}based シナプス}について説明する。簡単に言うと、Current\sphinxhyphen{}based シナプスは入力電流が変化するというモデルで, Conductance\sphinxhyphen{}based シナプスはイオンチャネルのコンダクタンス (電気抵抗の逆数, 電流の流れやすさ)が変化するというモデルである (cf. \sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3943173/}{Cavallari et al., 2014})。

以下では例として, 次のLIFニューロンの方程式におけるシナプス入力を考える。
\begin{equation*}
\begin{split}
\tau_m \frac{dV_{m}(t)}{dt}=-(V_{m}(t)-V_\text{rest})+R_m I_{\text{syn}}(t)    
\end{split}
\end{equation*}
とする。ただし, \(\tau_m\)は膜電位の時定数, \(V_m(t)\)は膜電位, \(V_\text{rest}\)は静止膜電位, \(R_m\)は膜抵抗である。ここで、シナプス入力の電流\(I_{\text{syn}}(t)\)が%
\begin{footnote}[1]\sphinxAtStartFootnote
シナプス(synapse)入力であることを明らかにするためにsynと添え字をつけている。
%
\end{footnote}2つのモデルにおいて異なる部分となる。


\subsubsection{3.1.2 Current\sphinxhyphen{}based シナプス}
\label{\detokenize{3-2_current-conductance-synapse:current-based}}
Current\sphinxhyphen{}based シナプスは単純に\sphinxstylestrong{入力電流が変化}するというモデルで, 簡略化したい場合によく用いられる。シナプス入力\(I_{\text{syn}}(t)\)はシナプス効率(synaptic efficacy)%
\begin{footnote}[2]\sphinxAtStartFootnote
シナプス強度(Synaptic strength)とは違い, 受容体の種類(GABA受容体やAMPA受容体,  およびそのサブタイプなど)によって決まる。
%
\end{footnote}を\(J_{\text{syn}}\) (単位はpA)とし , シナプスの動態(synaptic kinetics)を\(s_{\text{syn}}(t)\)とすると, 次式のようになる。ただし, シナプスの動態とは, 前細胞に注目すれば神経伝達物質の放出量, 後細胞に注目すれば神経伝達物質の結合量やイオンチャネルの開口率を表す。
\begin{equation*}
\begin{split}
\begin{equation}
I_{\text{syn}}(t)=\underbrace{J_{\text{syn}}s_{\text{syn}}(t)}_{電流の変化}    
\end{equation}
\end{split}
\end{equation*}
ただし, \(s_{\text{syn}}(t)\)は, 例えば次節で紹介する\(\alpha\)関数を用いる場合,
\begin{equation*}
\begin{split}
\begin{equation}
s_{\text{syn}}(t)=\dfrac{t}{\tau_s} \exp \left(1-\dfrac{t}{\tau_s}\right)    
\end{equation}
\end{split}
\end{equation*}
のようになる。


\subsubsection{3.1.3 Conductance\sphinxhyphen{}based シナプス}
\label{\detokenize{3-2_current-conductance-synapse:conductance-based}}
Conductance\sphinxhyphen{}based シナプスはイオンチャネルの\sphinxstylestrong{コンダクタンスが変化}するというモデルである。関連して、例えば Hodgkin\sphinxhyphen{}Huxley モデルはConductance\sphinxhyphen{}based モデルの1つである。Current\sphinxhyphen{}basedよりもConductance\sphinxhyphen{}based の方が生理学的に妥当である。例えば抑制性シナプスは膜電位が平衡電位と比べて脱分極側にあるか, 過分極側にあるかで抑制的に働くか興奮的に働くかが逆転する。これはCurrent\sphinxhyphen{}based シナプスでは再現できない。

Conductance\sphinxhyphen{}based モデルにおけるシナプス入力は\(I_{\text{syn}}(t)\)は次のようになる。
\begin{equation*}
\begin{split}
\begin{equation}
I_{\text{syn}}(t)=\underbrace{g_{\text{syn}}s_{\text{syn}}(t)}_{コンダクタンスの変化}\cdot\ \left(V_{\text{syn}}-V_{m}(t)\right)    
\end{equation}
\end{split}
\end{equation*}
ただし, \(g_{\text{syn}}\) (単位はnS)はシナプスの最大コンダクタンス%
\begin{footnote}[3]\sphinxAtStartFootnote
\(g_{\text{syn}}\)がシナプスの最大コンダクタンスとなるのは \(s_{\text{syn}}\)の最大値を1に正規化する場合である。正規化は必須ではないので, 単なる係数と思うのがよい。
%
\end{footnote}, \(V_{\text{syn}}\) (単位はmV)はシナプスの平衡電位を表す。これらも\(J_{\text{syn}}\)と同じく, シナプスにおける受容体の種類によって決まる定数である。

注意しなければならないことは, \(s_{\text{syn}}(t)\leq 0\)としたとき, Current\sphinxhyphen{}based モデルにおける\(J_{\text{syn}}\)は正の値(興奮性)と負の値(抑制性)を取るが, \(g_{\text{syn}}\)は正の値のみである、ということである %
\begin{footnote}[4]\sphinxAtStartFootnote
これはコンダクタンスが電気抵抗の逆数であり, 基本的に抵抗は正の値しか取らないことからも分かる。なお電子回路においては負性抵抗という,  素子の抵抗値が見かけ上, 負の値を取る場合もある。
%
\end{footnote}. Conductance\sphinxhyphen{}basedモデルで興奮性と抑制性を決定しているのは, 平衡電位\(V_{\text{syn}}\)である。興奮性シナプスの平衡電位は高く, 抑制性シナプスの平衡電位は低いため, 膜電位を引いた符号はそれぞれ正と負になる。


\bigskip\hrule\bigskip



\subsection{3.2 指数関数型シナプスモデル}
\label{\detokenize{3-3_expo-synapse:id1}}\label{\detokenize{3-3_expo-synapse::doc}}
シナプスのモデルは複数あるが, 良く用いられるのが\sphinxstylestrong{指数関数型シナプスモデル}(exponential synapse model)である。このモデルは生理学的な過程を無視した現象論的モデルであることに注意しよう。指数関数型シナプスモデルには2つの種類, \sphinxstylestrong{単一指数関数型モデル} (single exponential model)と\sphinxstylestrong{二重指数関数型モデル} (double exponential model)がある。

数式の説明の前にモデルの挙動を示す。次図は2種類のモデルにおいて\(t=0\)でスパイクが生じてからのシナプス後電流の変化を示している。ただし, 実際のシナプス後電流はこれに\sphinxstylestrong{シナプス強度} (Synaptic strength)%
\begin{footnote}[1]\sphinxAtStartFootnote
シナプス強度というのは便宜上の呼称で, 実際には神経伝達物質の種類や, その受容体の数など複数の要因によって決定されている. また, このシナプス強度はシナプス重みということもある。これはどちらかと言えば機械学習の表現に引っ張られたものである。このため, このサイトでは重みという語も使う。
%
\end{footnote}を乗じて総和を取ったものとなる。 シナプス強度については3.6で説明をする。

2種類の指数関数型シナプスの動態. 点線は単一指数関数型シナプスで, 実線は二重指数関数型シナプスである。なおコードは数式の説明後に掲載する。


\subsubsection{3.2.1 単一指数関数型モデル(Single exponential model)}
\label{\detokenize{3-3_expo-synapse:single-exponential-model}}
シナプス前ニューロンにおいてスパイクが生じてからのシナプス後電流の変化はおおよそ指数関数的に減少する, というのが単一指数関数型モデルである %
\begin{footnote}[2]\sphinxAtStartFootnote
薬学動態の静注1コンパートメントモデルと同じ式である。
%
\end{footnote}. 式は次のようになる。
\begin{equation*}
\begin{split}
\begin{equation}
f(t)=\frac{1}{\tau_{s}}\exp\left(-\frac{t}{\tau_s}\right)    
\end{equation}
\end{split}
\end{equation*}
この関数を時間的なフィルターとして, 過去の全てのスパイクについての総和を取る。
\begin{equation*}
\begin{split}
\begin{equation}
r(t)=\sum_{t_{k}< t} f\left(t-t_{k}\right)
\end{equation}
\end{split}
\end{equation*}
ここで\({r(t)}\)は前節におけるシナプス動態(\(s_{\text{syn}}\))で, \(t_{k}\)はあるニューロンの\(k\)番目のスパイクの発生時刻である。\({t_{k}<t}\)の意味は現在の時刻\(t\)までに発生したスパイクについての和を取るという意味である。なお、スパイクが生じてから, ある程度の時間が経過した後はそのスパイクの影響はないと見なせるので, 一定の時間までの総和を取るのがよい。

別の表記法としてスパイク列に対する畳み込みを行うというものもある。畳み込み演算子を\(*\)とし, シナプス前細胞のスパイク列を\(S(t)=\sum_{t_{k}< t} \delta\left(t-t_{k}\right)\)とする (ただし, \(\delta\)はDiracのdelta関数において\(\delta(0)=1\)とした関数)。このとき, \(r(t)=f*S(t)\)と表すことができる。畳み込み演算子を用いると簡略な表記ができるが、実装上は他と同じ手法を用いる。


\paragraph{微分方程式による表現}
\label{\detokenize{3-3_expo-synapse:id4}}
上の手法ではニューロンの発火時刻を記憶し, 時間毎に全てのスパイクについての和を取る必要がある。そこで, 実装する場合は次の等価な微分方程式を用いる。
\begin{equation*}
\begin{split}
\begin{equation}
\frac{dr}{dt}=-\frac{r}{\tau_{s}}+\frac{1}{\tau_{s}} \sum_{t_{k}< t} \delta\left(t-t_{k}\right)   
\end{equation}
\end{split}
\end{equation*}
ここで\(\tau_s\)はシナプスの時定数(synaptic time constant)である。 また, \(\delta(\cdot)\)はDiracのdelta関数です(ただし\(\delta(0)=1\)です). これをEuler法で差分化すると
\begin{equation*}
\begin{split}
\begin{equation}
r(t+\Delta t)=\left(1-\frac{\Delta t}{\tau_{s}}\right)r(t)+\frac{1}{\tau_{s}}\delta_{t,t_{k}} 
\end{equation}
\end{split}
\end{equation*}
となる。ここで\(\delta_{t,t_{k}}\)はKroneckerのdelta関数で, \(t=t_{k}\)のときに1, それ以外は0となる。また減衰度として\(\left(1-\Delta  t/\tau_{d}\right)\)の代わりに\(\exp\left(-\Delta t/\tau_{d}\right)\)を用いる場合もある。


\subsubsection{3.2.2 二重指数関数型モデル(Double exponential model)}
\label{\detokenize{3-3_expo-synapse:double-exponential-model}}
2重の指数関数によりシナプス後電流の立ち上がりも考慮するのが, 二重指数関数型モデル(Double exponential model)である%
\begin{footnote}[3]\sphinxAtStartFootnote
薬学動態の内服1コンパートメントモデルと同じ式である。
%
\end{footnote}。\(t=0\)にシナプス前細胞が発火したときのシナプス後電流の時間変化の関数は次のようになる。
\begin{equation*}
\begin{split}
\begin{equation}
f(t)=A\left[\exp\left(-\frac{t}{\tau_d}\right)-\exp\left(-\frac{t}{\tau_r}\right)\right]    
\end{equation}
\end{split}
\end{equation*}
ただし, \({\tau_r}\)は立ち上がり時定数(synaptic rise time constant), \({\tau_d}\)は減衰時定数(synaptic decay time constant)である。\(\tau_{d}\)は\(\tau_{s}\)と同じく神経伝達物質の減少速度を決定している。\(A\)は規格化定数で次のように表される。
\begin{equation*}
\begin{split}
\begin{equation}
A=\frac{\tau_d}{\tau_d-\tau_r}\cdot \left(\frac{\tau_r}{\tau_d}\right)^\frac{\tau_r}{\tau_r-\tau_d}    
\end{equation}
\end{split}
\end{equation*}
規格化定数\(A\)を乗じることで最大値が1となる。ただし, シミュレーションをする上で実際に規格化をする場合は少ない。


\paragraph{\protect\(\alpha\protect\)関数}
\label{\detokenize{3-3_expo-synapse:alpha}}
上記の式において, \(\tau=\tau_{r}=\tau_{d}\)の場合は \(\boldsymbol{\alpha}\) \sphinxstylestrong{関数} (alpha function, alpha synapse)と呼ぶ (\sphinxhref{https://pubmed.ncbi.nlm.nih.gov/6055351/}{Rall, 1967})。式としては次のようになる。
\begin{equation*}
\begin{split}
\begin{equation}
\alpha(t)=\frac{t}{\tau}\exp\left(1-\frac{t}{\tau}\right)    
\end{equation}
\end{split}
\end{equation*}
この式は二重指数関数型シナプスの式に単に代入するだけでは導出できない。これらの式の対応については後述する。


\paragraph{微分方程式による表現}
\label{\detokenize{3-3_expo-synapse:id6}}
ここで, 二重指数関数型シナプスの式に対応する, 補助変数\(h\)を用いた微分方程式を導入する。
\begin{equation*}
\begin{split}
\begin{align} 
\frac{dr}{dt}&=-\frac{r}{\tau_{d}}+h\\
\frac{dh}{dt}&=-\frac{h}{\tau_{r}}+\frac{1}{\tau_{r} \tau_{d}} \sum_{t_{k}< t} \delta\left(t-t_{k}\right) 
\end{align} 
\end{split}
\end{equation*}
単一指数関数型シナプスの場合と同様にEuler法で差分化すると
\begin{equation*}
\begin{split}
\begin{align} 
r(t+\Delta t)&=\left(1-\frac{\Delta t}{\tau_{d}}\right)r(t)+h(t)\cdot \Delta t\\ 
h(t+\Delta t)&=\left(1-\frac{\Delta t}{\tau_{r}}\right)h(t)+\frac{1}{\tau_{r}\tau_{d}} \delta_{t,t_{j k}}
\end{align}
\end{split}
\end{equation*}
となる。

念のため, 微分方程式と元の式が一致することを確認しておこう。\(t=0\)のときにシナプス前細胞が発火したとし, それ以降の発火はないとする。このとき, \(h(0)=1/\tau_{r}\tau_{d}, r(0)=0\) である。\(h\)についての微分方程式の解は
\begin{equation*}
\begin{split}
\begin{equation}
h(t)=h(0)\cdot \exp\left(-\frac{t}{\tau_r}\right)    
\end{equation}
\end{split}
\end{equation*}
となるので, これを\(r\)についての式に代入して
\begin{equation*}
\begin{split}
\begin{equation}
\frac{dr}{dt}=-\frac{r}{\tau_{d}}+h(0)\cdot \exp\left(-\frac{t}{\tau_r}\right) 
\end{equation}
\end{split}
\end{equation*}
となる。これを解くには両辺に積分因子\(\exp({t}/{\tau_d})\)をかけてから積分をするかLaplace変換をするかである。今回はLaplace変換を用いる。右辺一項目を移行した後に両辺をLaplace変換すると以下のようになる。
\begin{equation*}
\begin{split}
\begin{align}
\mathcal{L}\left[\frac{dr}{dt}+r/\tau_{d}\right]&=\mathcal{L}\left[h(0)\cdot \exp\left(-t/\tau_r\right)\right]\\
sF(s)-r(0)+\frac{1}{\tau_{d}}F(s)&=\frac{h(0)}{s+1/\tau_r}\\
F(s)&=\frac{h(0)}{(s+1/\tau_r)(s+1/\tau_d)}
\end{align}
\end{split}
\end{equation*}
ただし\(r(t)\)のLaplace変換を\(F(s)\)とした. ここで逆Laplace変換を行うと次のようになる。
\begin{equation*}
\begin{split}
\begin{align}
r(t)&=\mathcal{L}^{-1}(F(s))\\
&=\mathcal{L}^{-1}\left[\frac{h(0)}{(s+1/\tau_r)(s+1/\tau_d)}\right]\\
&=\mathcal{L}^{-1}\left[\frac{h(0)}{1/\tau_r-1/\tau_d}\left(\frac{1}{s+1/\tau_d}-\frac{1}{s+1/\tau_r}\right)\right]\\
&=\frac{1}{\tau_d-\tau_r}\left[\exp(-t/\tau_d)-\exp(-t/\tau_r)\right]
\end{align}
\end{split}
\end{equation*}
この式の最大値\(r_{\max}\)を求めておこう。 \(r(t)\)を微分して0と置いた式の解\(t_{\max}\)を代入すれば求められる。計算すると,
\begin{equation*}
\begin{split}
\begin{equation}
t_{\max}=\dfrac{\ln(\tau_d/\tau_r)}{1/\tau_r-1/\tau_d},\ \ r_{\max}=\dfrac{1}{\tau_{d}}\cdot \left(\dfrac{\tau_{r}}{\tau_{d}}\right)^{\frac{\tau_{r}}{\tau_d-\tau_{r}}}    
\end{equation}
\end{split}
\end{equation*}
となる。

なお, \(\alpha\)関数の導出は逆Laplace変換をする前に\(\tau=\tau_d=\tau_r\)とすればよく,
\begin{equation*}
\begin{split}
\begin{align}
F_\alpha(s)&=\frac{h(0)}{(s+1/\tau)^2}\\
\alpha(t)&=\frac{t}{\tau^2}\exp\left(-\frac{t}{\tau}\right)
\end{align}
\end{split}
\end{equation*}
となる。若干の係数の違いはあるが, 同じ形の関数が導出された。


\bigskip\hrule\bigskip



\subsection{3.3 動力学モデル}
\label{\detokenize{3-4_kinetic-synapse:id1}}\label{\detokenize{3-4_kinetic-synapse::doc}}

\subsubsection{3.3.1 チャネル動態の動力学的表現}
\label{\detokenize{3-4_kinetic-synapse:id2}}
指数関数型シナプスとモデルの振る舞いはほぼ同一だが, 式の構成が少し異なるモデルとして\sphinxstylestrong{動力学モデル} (Kinetic model, またはMarkov kinetic model)がある (\sphinxhref{https://link.springer.com/article/10.1007/BF00961734}{Destexhe et al., 1994}; \sphinxhref{http://cns.iaf.cnrs-gif.fr/files/handbook98.pdf}{Destexhe et al., 2002})。動力学モデルはHHモデルのゲート変数の式と類似した式で表される。このモデルではチャネルが開いた状態(Open)と閉じた状態(Close), および神経伝達物質(neurotransmitter)の放出状態(T)の2つの要素に関する状態がある。また, 閉\(\to\)開の反応速度を\(\alpha\), 開\(\to\)閉の反応速度を\(\beta\)とする。このとき、これらを表す状態遷移の式は次のようになる。
\begin{equation*}
\begin{split}
\begin{equation}
\text{Close}+\text{T}  \underset{\beta}{\overset{\alpha}{\rightleftharpoons}}\text{Open}    
\end{equation}
\end{split}
\end{equation*}
ここで, シナプス動態を\(r\)とすると
\begin{equation*}
\begin{split}
\begin{equation}
\frac{dr}{dt}=\alpha T (1-r) - \beta r
\end{equation}
\end{split}
\end{equation*}
となる。ただし, Tはシナプス前細胞が発火したときにインパルス的に1だけ増加するとする。また, \(\alpha, \beta\)は速度なので, 時定数の逆数であることに注意しよう。 \(\alpha=2000 \text{ms}^{-1}\), \(\beta=200 \text{ms}^{-1}\)とすると, シナプス動態は図のようになる。


\subsubsection{3.3.2 Hodgkin\sphinxhyphen{}Huxleyモデルにおけるシナプスモデル}
\label{\detokenize{3-4_kinetic-synapse:hodgkin-huxley}}
これまで明示的にスパイクの発生が表現されたモデルを用いてきたが、HHモデルでは単なる膜電位の変数があるのみである。ここでは前述した動力学的モデルを用いてHHモデルにおけるシナプス動態の記述を行う (\sphinxhref{https://www.mitpressjournals.org/doi/10.1162/neco.1994.6.1.14}{Destexhe et al., 1994}; \sphinxhref{https://www.sciencedirect.com/science/article/pii/S0378437114004592}{Batista et al., 2014})。

\(r_{j}\)を\(j\)番目のニューロンのpre\sphinxhyphen{}synaptic dynamicsとすると、\(r_{j}\)は次式に従う。
\begin{equation*}
\begin{split}
\frac{\mathrm{d} r_{j}}{\mathrm{d} t}=\left(\frac{1}{\tau_{r}}-\frac{1}{\tau_{d}}\right) \frac{1-r_{j}}{1+\exp \left(-V_{j}+V_{0}\right)}-\frac{r_{j}}{\tau_{d}}
\end{split}
\end{equation*}
ただし、時定数 \(\tau_r=0.5, \tau_d = 8\) (ms), 反転電位 \(V_0 = -20\) (mV)とする。前節で既に\(r\)の描画は行ったが、パルス波を印加した場合の挙動を確認する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
using Plots
using Base: @kwdef
using Parameters: @unpack \PYGZsh{} or using UnPack

@kwdef struct HHParameter\PYGZob{}FT\PYGZcb{}
    Cm::FT = 1.0 \PYGZsh{} 膜容量(uF/cm\PYGZca{}2)
    gNa::FT = 120.0 \PYGZsh{} Na+ の最大コンダクタンス(mS/cm\PYGZca{}2)
    gK::FT = 36.0 \PYGZsh{} K+ の最大コンダクタンス(mS/cm\PYGZca{}2)
    gL::FT = 0.3 \PYGZsh{} 漏れイオンの最大コンダクタンス(mS/cm\PYGZca{}2)
    ENa::FT = 50.0 \PYGZsh{} Na+ の平衡電位(mV)
    EK::FT = \PYGZhy{}77.0 \PYGZsh{} K+ の平衡電位(mV)
    EL::FT = \PYGZhy{}54.387 \PYGZsh{}漏れイオンの平衡電位(mV)
    tr::FT = 0.5 \PYGZsh{} ms
    td::FT = 8.0 \PYGZsh{} ms
    invtr::FT = 1.0 / tr
    invtd::FT = 1.0 / td
    v0::FT = \PYGZhy{}20.0 \PYGZsh{} mV
end

@kwdef mutable struct HH\PYGZob{}FT\PYGZcb{}
    param::HHParameter = HHParameter\PYGZob{}FT\PYGZcb{}()
    N::Int32
    v::Vector\PYGZob{}FT\PYGZcb{} = fill(\PYGZhy{}65.0, N)
    m::Vector\PYGZob{}FT\PYGZcb{} = fill(0.05, N)
    h::Vector\PYGZob{}FT\PYGZcb{} = fill(0.6, N)
    n::Vector\PYGZob{}FT\PYGZcb{} = fill(0.32, N)
    r::Vector\PYGZob{}FT\PYGZcb{} = zeros(N)
end

function updateHH!(variable::HH, param::HHParameter, I::Vector, dt)
    @unpack N, v, m, h, n, r = variable
    @unpack Cm, gNa, gK, gL, ENa, EK, EL, tr, td, invtr, invtd, v0= param
    @inbounds for i = 1:N
        m[i] += dt * ((0.1(v[i]+40.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+40.0))))*(1.0 \PYGZhy{} m[i]) \PYGZhy{} 4.0exp(\PYGZhy{}(v[i]+65.0) / 18.0)*m[i])
        h[i] += dt * ((0.07exp(\PYGZhy{}0.05(v[i]+65.0)))*(1.0 \PYGZhy{} h[i]) \PYGZhy{} 1.0/(1.0 + exp(\PYGZhy{}0.1(v[i]+35.0)))*h[i])
        n[i] += dt * ((0.01(v[i]+55.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+55.0))))*(1.0 \PYGZhy{} n[i]) \PYGZhy{} (0.125exp(\PYGZhy{}0.0125(v[i]+65)))*n[i])
        v[i] += dt / Cm * (I[i] \PYGZhy{} gNa * m[i]\PYGZca{}3 * h[i] * (v[i] \PYGZhy{} ENa) \PYGZhy{} gK * n[i]\PYGZca{}4 * (v[i] \PYGZhy{} EK) \PYGZhy{} gL * (v[i] \PYGZhy{} EL))
        r[i] += dt * ((invtr \PYGZhy{} invtd) * (1.0 \PYGZhy{} r[i])/(1.0 + exp(\PYGZhy{}v[i] + v0)) \PYGZhy{} r[i] * invtd)
    end
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
updateHH! (generic function with 1 method)
\end{sphinxVerbatim}

シミュレーションを実行する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 50 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = Int32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(5f0 * ((t .\PYGZgt{} 10) \PYGZhy{} (t .\PYGZgt{} 15)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} 記録用
varr = zeros(Float32, nt, N)
rarr = zeros(Float32, nt, N)

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} simulation
@time for i = 1:nt
    updateHH!(neurons, neurons.param, I[i, :], dt)
    varr[i, :] = neurons.v
    rarr[i, :] = neurons.r
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.116599 seconds (441.13 k allocations: 22.111 MiB, 8.30\PYGZpc{} gc time)
\end{sphinxVerbatim}

描画してみる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{rarr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Membrane}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ potential (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Pre\PYGZhy{}synaptic}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ dynamics}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Injection}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current (nA)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{legend} \PYG{o}{=} \PYG{n}{false}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{3.5 シナプス入力の重みづけ}
\label{\detokenize{3-6_synaptic-weighted:id1}}\label{\detokenize{3-6_synaptic-weighted::doc}}
ここまでは, シナプス前細胞と後細胞がそれぞれ1つずつである場合について考えていたが, 実際には多数の細胞がネットワークを作っている。また, それぞれの入力は均等ではなく, 異なるシナプス強度 (Synaptic strength)を持つ。この場合のシナプス入力の計算について述べておく。

シナプス前細胞が\(N_{\text{pre}}\)個, シナプス後細胞が\(N_{\text{post}}\)個あるとする。このとき\sphinxstylestrong{シナプス前過程に注目した}シナプス動態を\(\boldsymbol{s_{\text{syn}}}\in \mathbb{R}^{N_{\text{pre}}}\), シナプス後細胞の入力電流を\(\boldsymbol{I_{\text{syn}}}\in \mathbb{R}^{N_{\text{post}}}\), シナプス結合強度の行列を\(W\in \mathbb{R}^{N_{\text{post}} \times N_{\text{pre}}}\)とすると, Current\sphinxhyphen{}basedの場合は
\begin{equation*}
\begin{split}
\begin{equation}
\boldsymbol{I_{\text{syn}}}(t)=W \boldsymbol{s_{\text{syn}}}  
\end{equation}
\end{split}
\end{equation*}
となる。ただし, シナプス強度にシナプス効率が含まれるとした. また, Conductance\sphinxhyphen{}basedの場合はシナプス後細胞の膜電位を\(\boldsymbol{V}_{m}\in \mathbb{R}^{N_{\text{post}}}\)として,
\begin{equation*}
\begin{split}
\begin{equation}
\boldsymbol{I_{\text{syn}}}(t)=\left(V_{\text{syn}}-\boldsymbol{V}_{m}(t)\right)\odot W \boldsymbol{s_{\text{syn}}}
\end{equation}
\end{split}
\end{equation*}
となる。ただし, \(\odot\)はHadamard積である。

これらの式は順序を入れ替えることも可能である。シナプス前細胞でスパイクが生じたことを表すベクトルを\(\boldsymbol{\delta}_{t,t_{\text{spike}}}\in \mathbb{R}^{N_{\text{pre}}}\)とする。ただし, \(t_{\text{spike}}\)は各ニューロンにおいてスパイクが生じた時刻である。 \(\boldsymbol{s_{\text{syn}}}\)は\(\boldsymbol{\delta}_{t,t_{\text{spike}}}\)の関数であり, \(\boldsymbol{s_{\text{syn}}}(\boldsymbol{\delta}_{t,t_{\text{spike}}})\)と表せる。このとき\sphinxstylestrong{シナプス後過程に注目した}シナプス動態を\(\boldsymbol{s}^\prime_{\text{syn}}\in \mathbb{R}^{N_{\text{post}}}\)とすると, Current\sphinxhyphen{}basedの場合は
\begin{equation*}
\begin{split}
\begin{equation}
\boldsymbol{I_{\text{syn}}}(t)=\boldsymbol{s}^\prime_{\text{syn}}(W\boldsymbol{\delta}_{t,t_{\text{spike}}})  
\end{equation}
\end{split}
\end{equation*}
Conductance\sphinxhyphen{}basedの場合は
\begin{equation*}
\begin{split}
\begin{equation}
\boldsymbol{I_{\text{syn}}}(t)=\left(V_{\text{syn}}-\boldsymbol{V}_{m}(t)\right)\odot \boldsymbol{s}^\prime_{\text{syn}}(W\boldsymbol{\delta}_{t,t_{\text{spike}}})
\end{equation}
\end{split}
\end{equation*}
と表すことができる。

シナプス動態を前過程か後過程のどちらに注目したものとするかは, 実装によって様々である。シナプス入力の計算における中間の値を学習に用いるということもあるため, 単なる計算量の観点だけではどちらを選ぶかは決めることができない (計算量だけならシナプス変数に先に重み行列をかけた方がよい場合が多い)。実装の中で異なってくるのは計算順序と保持するベクトルの要素数である。 同じ実装の中で2つとも用いる場合もあるので注意してほしい。


\section{11. ベイズ脳理論と生成モデル}
\label{\detokenize{11_intro:id1}}\label{\detokenize{11_intro::doc}}

\subsection{まえがき}
\label{\detokenize{preface:id1}}\label{\detokenize{preface::doc}}
何故このサイトを作ろうと思ったか、というと鬱憤が溜まっていたためである。\sphinxstylestrong{物理シミュレーションの本はあるのに神経科学のシミュレーションの本は何故無いのか？} もちろん、洋書を探せば何冊かあるが、ブラックボックス的なライブラリを用いたり、幅広い分野の内容を扱っていなかったりする。数理モデルを扱う洋書・和書の名著は多数あるが、実装まで記述してくれない本がほとんどである。そのくせ、関連分野のニューラルネットワークの本は腐るほど出版されている (というと著者の方々に大変失礼だが)。有名な論文でも再現するためのパラメータの記載が不足していたり、著者実装がなかったり、あっても古いMATLAB実装は動かない。シミュレーションを実施する講義を受けたいと思っても医学部のカリキュラムで試験と被る。潜りも難しい。所属している研究室は理論や数値計算がメインではないので研究室の「秘伝のタレ」があるわけでもない。Twitterで流れてくる講義の情報を見ても「どうして資料を公開してくれないんだ」と嘆きながら枕を濡らすしかない。

「本が無いなら本を書けばいいじゃない」という言葉は…今作った言葉だが、一先ずこれまでに自分が書いてきたコードやノート (\sphinxhref{https://salad-bowl-of-knowledge.github.io/hp/}{ブログ}、\sphinxhref{https://compneuro-julia.github.io/\#spiking-neural-networks}{同人誌 (SNN本)}などを含む)をまとめるだけでも良い資料になるのではないか、という中々思い上がった考えからこのサイトを始めた (目次を見るとかなり無謀なことが察せられる。何事も勢いで初めてはいけない)。このサイトの元となったSNN本が「SNNを如何に学習させるか」ということに重きを置いた同人誌であったため、このサイトでも「学習」に重きを置く予定である。

目標とするのは\sphinxstylestrong{幅広い分野を取り扱い、数式や理論も書いていて、実装を掲載しているサイト}である。実装も線形代数の演算は許容するが、できるだけブラックボックスな演算を無くし、「手を動かして学ぶ」を基本とする。また、可読性が高く、高速に実行出来て研究にも使える実装が望ましい (もちろん、現時点でこれが実現できているとは思わない)。

問題はサイトで使用するプログラミング言語であった。一体何を使えばいいだろうか？神経科学ではMATLABがよく使われているし、関連分野の機械学習ではPythonがよく使われる。ただ、MATLABはライセンスが必要だし、Pythonが数値計算に向いているとは思えない。高速に実行したいし、これらのユーザーが簡単に移行できる言語を考えたとき、\sphinxhref{https://julialang.org/}{Julia}が望ましいという結論に至った。

\sphinxhref{https://www.geidai.ac.jp/~marui/julialang/why\_we\_created\_julia/index.html}{欲張りな言語であるJulia}は欲張りなこのサイトの理念に合致している。Juliaはまだ発展途上ではあるが、ユーザーも増えており、このサイトが一通り完成するころ (3～5年後?) にはかなり良い環境が整っているはずである。

サイトの作成に関しては\sphinxhref{https://jupyterbook.org/intro.html}{Jupyter book}を用いている。Jupyter bookはJupyter notebookやmarkdownをちゃんとしたサイトの形式に変換してくれ、大変便利である。

内容に関しての疑問点や指摘に関しては、各記事末尾のコメント欄からお願いしたい (\sphinxhref{https://utteranc.es/}{utterances.es}を用いているためGitHubアカウントが必要である)。他に、もしこのサイトに協力してくれるという方がいれば、Twitter(\sphinxhref{https://twitter.com/tak\_yamm}{@tak\_yamm})でDMをしていただければ幸いである。

なお、知り合いの方向けに言っておくと、このサイトはあくまで趣味であって、勉強や研究や仕事の方を優先するので安心してほしい。などという言葉を試験3日前に書いている私であった。

2020年7月24日山本 拓都


\subsection{1.2 記号の表記}
\label{\detokenize{notation:id1}}\label{\detokenize{notation::doc}}
このサイトは次のような記号表記を用いている。
\begin{itemize}
\item {} 
実数全体を\(\mathbb{R}\), 複素数全体は\(\mathbb{C}\)と表記する。

\item {} 
スカラーは小文字・斜体で \(x\) のように表記する。

\item {} 
ベクトルは小文字・立体・太字で \(\mathbf{x}\) のように表記し、列ベクトル (縦ベクトル) として扱う。

\item {} 
行列やテンソルは大文字・立体・太字で \(\mathbf{X}\) のように表記する。

\item {} 
\(n\times 1\)の実ベクトルの集合を \(\mathbb{R}^n\), \(n\times m\) の実行列の集合を \(\mathbb{R}^{n\times m}\)と表記する。

\item {} 
行列 \(\mathbf{X}\) の置換は \(\mathbf{X}^T\)と表記する。ベクトルの要素を表す場合は \(\mathbf{x} = (x_1, x_2,\cdots, x_n)^T\)のように表記する。

\item {} 
単位行列を \(\mathbf{I}\) と表記する。

\item {} 
ゼロベクトルは \(\mathbf{0}\) , 要素が全て1のベクトルは \(\mathbf{1}\) と表記する。

\item {} 
\(e\)を自然対数の底とし、指数関数を \(e^x=\exp(x)\)と表記する。また、自然対数を \(\ln(x)\)と表記する。

\item {} 
平均 \(\mu\), 標準偏差 \(\sigma\) の正規分布を \(\mathcal{N}(\mu, \sigma^2)\) と表記する。

\end{itemize}


\subsection{2.2 Hodgkin\sphinxhyphen{}Huxleyモデル}
\label{\detokenize{2-2_hodgkinhuxley:hodgkin-huxley}}\label{\detokenize{2-2_hodgkinhuxley::doc}}

\subsubsection{2.2.1 Hodgkin\sphinxhyphen{}Huxleyモデルにおける膜の等価回路モデル}
\label{\detokenize{2-2_hodgkinhuxley:id1}}
\sphinxstylestrong{Hodgkin\sphinxhyphen{}Huxleyモデル} (HH モデル)は, A.L. HodgkinとA.F. Huxleyによって1952年に考案されたニューロンの膜興奮を表すモデルである (\sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/}{Hodgkin \& Huxley, 1952})。Hodgkinらはヤリイカの巨大神経軸索に対する\sphinxstylestrong{電位固定法}(voltage\sphinxhyphen{}clamp)を用いた実験を行い, 実験から得られた観測結果を元にモデルを構築した。

HHモデルには等価な電気回路モデルがあり, \sphinxstylestrong{膜の並列等価回路モデル} (parallel conductance model)と呼ばれている。膜の並列等価回路モデルでは, ニューロンの細胞膜をコンデンサ, 細胞膜に埋まっているイオンチャネルを可変抵抗 (動的に変化する抵抗) として置き換える。

\sphinxstylestrong{イオンチャネル} (ion channel)は特定のイオン(例えばナトリウムイオンやカリウムイオンなど)を選択的に通す膜輸送体の一種である。それぞれのイオンの種類において, 異なるイオンチャネルがある (同じイオンでも複数の種類のイオンチャネルがある)。また, イオンチャネルにはイオンの種類に応じて異なる\sphinxstylestrong{コンダクタンス}(抵抗の逆数で電流の「流れやすさ」を意味する)と\sphinxstylestrong{平衡電位}(equilibrium potential)がある。HHモデルでは, ナトリウム(Na\(^{+}\))チャネル, カリウム(K\(^{+}\))チャネル, 漏れ電流(leak current)のイオンチャネルを仮定する。漏れ電流のイオンチャネルは当時特定できなかったチャネルで, 膜から電流が漏れ出すチャネルを意味する。なお, 現在では漏れ電流の多くはCl\(^{-}\)イオン(chloride ion)によることが分かっている。

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{parallel_conductance_model}.JPG}
\caption{Hodgkin\sphinxhyphen{}Huxleyモデルの膜の等価回路モデル}\label{\detokenize{2-2_hodgkinhuxley:parallel-conductance-model}}\end{figure}

それでは, 等価回路モデルを用いて電位変化の式を立ててみよう。上図において, \(C_m\)は細胞膜のキャパシタンス(膜容量), \(I_{m}(t)\)は細胞膜を流れる電流(外部からの入力電流), \(I_\text{Cap}(t)\)は膜のコンデンサを流れる電流, \(I_\text{Na}(t)\)及び \(I_K(t)\)はそれぞれナトリウムチャネルとカリウムチャネルを通って膜から流出する電流, \(I_\text{L}(t)\)は漏れ電流である。このとき,
\begin{equation*}
\begin{split}
I_{m}(t)=I_\text{Cap}(t)+I_\text{Na}(t)+I_\text{K}(t)+I_\text{L}(t)    
\end{split}
\end{equation*}
という仮定をしている。

膜電位を\(V(t)\)とすると, Kirchhoffの第二法則 (Kirchhoff’s Voltage Law)より,
\begin{equation*}
\begin{split}
\underbrace{C_m\frac {dV(t)}{dt}}_{I_\text{Cap} (t)}=I_{m}(t)-I_\text{Na}(t)-I_\text{K}(t)-I_\text{L}(t)
\end{split}
\end{equation*}
となる。Hodgkinらはチャネル電流\(I_\text{Na}, I_K, I_\text{L}\)が従う式を実験的に求めた。
\begin{equation*}
\begin{split}
\begin{aligned}
I_\text{Na}(t) &= g_{\text{Na}}\cdot m^{3}h(V-E_{\text{Na}})\\
I_\text{K}(t) &= g_{\text{K}}\cdot n^{4}(V-E_{\text{K}})\\
I_\text{L}(t) &= g_{\text{L}}(V-E_{\text{L}})
\end{aligned}
\end{split}
\end{equation*}
ただし, \(g_{\text{Na}}, g_{\text{K}}\)はそれぞれNa\(^+\), K\(^+\)の最大コンダクタンスである。\(g_{\text{L}}\)はオームの法則に従うコンダクタンスで, Lコンダクタンスは時間的に変化はしないと仮定する。また, \(m\)はNa\(^+\)コンダクタンスの活性化パラメータ, \(h\)はNa\(^+\)コンダクタンスの不活性化パラメータ, \(n\)はK\(^+\)コンダクタンスの活性化パラメータであり, ゲートの開閉確率を表している。よって, HHモデルの状態は\(V, m, h, n\)の4変数で表される。これらの変数は以下の\(x\)を\(m, n, h\)に置き換えた3つの微分方程式に従う。
\begin{equation*}
\begin{split}
\frac{dx}{dt}=\alpha_{x}(V)(1-x)-\beta_{x}(V)x
\end{split}
\end{equation*}
ただし, \(V\)の関数である\(\alpha_{x}(V),\ \beta_{x}(V)\)は\(m, h, n\)によって異なり, 次の6つの式に従う。
\begin{equation*}
\begin{split}
\begin{array}{ll}
\alpha_{m}(V)=\dfrac {0.1(25-V)}{\exp \left[(25-V)/10\right]-1}, &\beta_{m}(V)=4\exp {(-V/18)}\\
\alpha_{h}(V)=0.07\exp {(-V/20)}, & \beta_{h}(V)={\dfrac{1}{\exp {\left[(30-V)/10 \right]}+1}}\\
\alpha_{n}(V)={\dfrac {0.01(10-V)}{\exp {\left[(10-V)/10\right]}-1}},& \beta_{n}(V)=0.125\exp {(-V/80)} 
\end{array}
\end{split}
\end{equation*}
なお、この式は6.3℃の条件下においてイカの巨大軸索の活動から得たデータを用いて導かれたものであることに注意しよう。


\subsubsection{2.2.2 Hodgkin\sphinxhyphen{}Huxley モデルの定義}
\label{\detokenize{2-2_hodgkinhuxley:id2}}
これまでに説明した式を用いてHHモデルを実装する。まず必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\end{sphinxVerbatim}

変更しない定数を保持する \sphinxcode{\sphinxupquote{struct}} の \sphinxcode{\sphinxupquote{HHParameter}} と, 変数を保持する \sphinxcode{\sphinxupquote{mutable struct}} の \sphinxcode{\sphinxupquote{HH}} を作成する。\sphinxcode{\sphinxupquote{v, m, h, n}} はHH modelの4変数だが, \sphinxcode{\sphinxupquote{r}} はpre\sphinxhyphen{}synaptic dynamicsを表す変数である。詳細は3章で解説する。 定数は次のように設定する。

\textbackslash{}begin\{align*\}
C\_m=1.0, g\_\{\textbackslash{}text\{Na\}\}=120, g\_\{\textbackslash{}text\{K\}\}=36, g\_\{\textbackslash{}text\{L\}\}=0.3\textbackslash{}
E\_\{\textbackslash{}text\{Na\}\}=50.0, E\_\{\textbackslash{}text\{K\}\}=\sphinxhyphen{}77, E\_\{\textbackslash{}text\{L\}\}=\sphinxhyphen{}54.387
\textbackslash{}end\{align*\}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{Cm}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{c+c1}{\PYGZsh{} 膜容量(uF/cm\PYGZca{}2)}
    \PYG{n}{gNa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{120.0} \PYG{c+c1}{\PYGZsh{} Na+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{36.0} \PYG{c+c1}{\PYGZsh{} K+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.3} \PYG{c+c1}{\PYGZsh{} 漏れイオンの最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{ENa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{50.0} \PYG{c+c1}{\PYGZsh{} Na+ の平衡電位(mV)}
    \PYG{n}{EK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{77.0} \PYG{c+c1}{\PYGZsh{} K+ の平衡電位(mV)}
    \PYG{n}{EL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{54.387} \PYG{c+c1}{\PYGZsh{}漏れイオンの平衡電位(mV)}
    \PYG{n}{tr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.5} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{td}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{8.0} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{invtr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{tr}
    \PYG{n}{invtd}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{td}
    \PYG{n}{v0}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{20.0} \PYG{c+c1}{\PYGZsh{} mV}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{HH}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{HHParameter} \PYG{o}{=} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{N}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16}
    \PYG{n}{v}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{65.0}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{m}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{h}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{n}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.32}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{r}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

次に変数を更新する関数\sphinxcode{\sphinxupquote{update!}}を書く。ソルバーとしては陽的Euler法または4次のRunge\sphinxhyphen{}Kutta法を用いる。以下ではEuler法を用いている。Juliaではforループを用いて1つのニューロンごとにパラメータを更新する方がベクトルを用いるよりも高速である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function update!(variable::HH, param::HHParameter, I::Vector, dt)
    @unpack N, v, m, h, n, r = variable
    @unpack Cm, gNa, gK, gL, ENa, EK, EL, tr, td, invtr, invtd, v0 = param
    @inbounds for i = 1:N
        m[i] += dt * ((0.1(v[i]+40.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+40.0))))*(1.0 \PYGZhy{} m[i]) \PYGZhy{} 4.0exp(\PYGZhy{}(v[i]+65.0) / 18.0)*m[i])
        h[i] += dt * ((0.07exp(\PYGZhy{}0.05(v[i]+65.0)))*(1.0 \PYGZhy{} h[i]) \PYGZhy{} 1.0/(1.0 + exp(\PYGZhy{}0.1(v[i]+35.0)))*h[i])
        n[i] += dt * ((0.01(v[i]+55.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+55.0))))*(1.0 \PYGZhy{} n[i]) \PYGZhy{} (0.125exp(\PYGZhy{}0.0125(v[i]+65)))*n[i])
        v[i] += dt / Cm * (I[i] \PYGZhy{} gNa * m[i]\PYGZca{}3 * h[i] * (v[i] \PYGZhy{} ENa) \PYGZhy{} gK * n[i]\PYGZca{}4 * (v[i] \PYGZhy{} EK) \PYGZhy{} gL * (v[i] \PYGZhy{} EL))
        r[i] += dt * ((invtr \PYGZhy{} invtd) * (1.0 \PYGZhy{} r[i])/(1.0 + exp(\PYGZhy{}v[i] + v0)) \PYGZhy{} r[i] * invtd)
    end
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
update! (generic function with 1 method)
\end{sphinxVerbatim}


\subsubsection{2.2.3 Hodgkin\sphinxhyphen{}Huxleyモデルのシミュレーションの実行}
\label{\detokenize{2-2_hodgkinhuxley:id3}}
いくつかの定数を設定してシミュレーションを実行する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * ((t .\PYGZgt{} 50) \PYGZhy{} (t .\PYGZgt{} 200)) + 35f0 * ((t .\PYGZgt{} 250) \PYGZhy{} (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} 記録用
varr = zeros(Float32, nt, N)
gatearr = zeros(Float32, nt, 3, N)

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr[i, :] = neurons.v
    gatearr[i, 1, :] = neurons.m
    gatearr[i, 2, :] = neurons.h
    gatearr[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.217901 seconds (1.36 M allocations: 46.979 MiB, 3.69\PYGZpc{} gc time)
\end{sphinxVerbatim}

結果を表示するために \sphinxcode{\sphinxupquote{Plots}}を読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Plots}
\end{sphinxVerbatim}

ニューロンの膜電位 \sphinxcode{\sphinxupquote{v}}, ゲート変数 \sphinxcode{\sphinxupquote{m, h, n}}, 刺激電流 \sphinxcode{\sphinxupquote{I}}の描画をする。入力電流の単位は \(\mu\text{A/cm}^2\)である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

次項で用いるために発火回数を求める。\sphinxcode{\sphinxupquote{bitwise and}}を用いると楽である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Num. of spikes : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}spikes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Num. of spikes : 27
\end{sphinxVerbatim}

50msから200msまでで11回, 250msから400msまでで16回発火しているので発火回数は計27回であり、この結果は正しい。


\subsubsection{2.2.4 Frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve}
\label{\detokenize{2-2_hodgkinhuxley:frequency-current-f-i-curve}}
この項ではHodgkin\sphinxhyphen{}Huxleyモデルにおいて、入力電流に対する発火率がどのように変化するかを調べる。次のコードのように入力電流を徐々に増加させたときの発火率を見てみよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 1000 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps

N = 100 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
maxcurrent = 30
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = Array\PYGZob{}Float32\PYGZcb{}(range(1,maxcurrent,length=N)) \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr\PYGZus{}fi = zeros(Float32, nt, N)

\PYGZsh{} simulation
for i = 1:nt
    update!(neurons, neurons.param, I[:], dt)
    varr\PYGZus{}fi[i, :] = neurons.v
end
\end{sphinxVerbatim}

発火率を計算して結果を描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{rate} \PYG{o}{=} \PYG{n}{num\PYGZus{}spikes}\PYG{o}{/}\PYG{n}{T}\PYG{o}{*}\PYG{l+m+mf}{1e3}

\PYG{n}{plot}\PYG{p}{(}\PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{rate}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Input current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Firing rate (Hz)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{400}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

このような曲線を\sphinxstylestrong{frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve} (または neuronal input/output (I/O) curve)と呼ぶ。


\subsubsection{2.2.5  抑制後リバウンド (Postinhibitory rebound; PIR)}
\label{\detokenize{2-2_hodgkinhuxley:postinhibitory-rebound-pir}}
ニューロンは電流が流入することで膜電位が変化し, 膜電位がある一定の閾値を超えると発火が起こる, というのはニューロンの活動電位発生についての典型的な説明である。それではHHモデルの膜電位閾値はどのくらいの値になるのだろうか。答えは「\sphinxstylestrong{膜電位閾値は一定ではない}」である。それを示す現象として \sphinxstylestrong{抑制後リバウンド} (Postinhibitory rebound; PIR)がある。この時生じる発火を\sphinxstylestrong{リバウンド発火} (rebound spikes)
と呼ぶ。抑制後リバウンドは過分極性の電流の印加を止めた際に膜電位が静止膜電位に回復するのみならず, さらに脱分極をして発火をするという現象である。この現象が生じる要因として
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{アノーダルブレイク} (anodal break, またはanode break excitation; ABE)

\item {} 
遅いT型カルシウム電流 (slow T\sphinxhyphen{}type calcium current)

\end{enumerate}

がある (\sphinxhref{https://pubmed.ncbi.nlm.nih.gov/15324089/}{Chik et al., 2004})。HH モデルはこのうちアノーダルブレイクを再現できるため, シミュレーションによりどのような現象か確認してみよう。これは入力電流を変更するだけで行える。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * (\PYGZhy{}(t .\PYGZgt{} 50) + (t .\PYGZgt{} 200)) + 20f0 * (\PYGZhy{}(t .\PYGZgt{} 250) + (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr2 = zeros(Float32, nt, N)
gatearr2 = zeros(Float32, nt, 3, N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr2[i, :] = neurons.v
    gatearr2[i, 1, :] = neurons.m
    gatearr2[i, 2, :] = neurons.h
    gatearr2[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.074358 seconds (848.89 k allocations: 19.134 MiB)
\end{sphinxVerbatim}

結果は次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Injection}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

なぜこのようなことが起こるか, というと過分極の状態から静止膜電位へと戻る際にNa\(^+\)チャネルが活性化 (Na\(^+\)チャネルの活性化パラメータ\(m\)が増加し, 不活性化パラメータ\(h\)が減少)し, 膜電位が脱分極することで再度Na\(^+\)チャネルが活性化する, というポジティブフィードバック過程(\sphinxstylestrong{自己再生的過程})に突入するためである (もちろん, この過程は通常の活動電位発生のメカニズムである)。 この際, 発火に必要な閾値が膜電位の低下に応じて下がった, ということもできる。

このように膜電位閾値は一定ではない。しかし, この後の節で紹介するLIFモデルなどでは簡略化のためにif文を用い, 膜電位閾値を超えたから発火, というものもある。実際には違うということを頭の片隅に残しながら読み進めることを推奨する。

\begin{sphinxadmonition}{note}{Note:}
PIRに関連する現象として抑制後促通 (Postinhibitory facilitation; PIF)がある。これは抑制入力の後に興奮入力がある一定の時間内で入ると発火が起こるという現象である (\sphinxhref{http://www.brain.riken.jp/en/summer/prev/2006/files/j\_rinzel04.pdf}{Dolda et al., 2006}, \sphinxhref{https://link.springer.com/referenceworkentry/10.1007\%2F978-1-4614-7320-6\_152-1}{Dodla, 2014})。
\end{sphinxadmonition}


\subsection{3.1 シナプス伝達}
\label{\detokenize{3-1_synapse:id1}}\label{\detokenize{3-1_synapse::doc}}
\begin{sphinxadmonition}{note}{Note:}
内容は大幅に追記が必要。
\end{sphinxadmonition}

スパイクが生じたことによる膜電位変化は軸索を伝播し, \sphinxstylestrong{シナプス}という構造により, 次のニューロンへと興奮が伝わる. このときの伝達の仕組みとして, シナプスには\sphinxstylestrong{化学シナプス}(chemical synapse)とGap junctionによる\sphinxstylestrong{電気シナプス}(electrical synapse)がある。

化学シナプスの場合, シナプス前膜からの\sphinxstylestrong{神経伝達物質}の放出, シナプス後膜の受容体への神経伝達物質の結合, イオンチャネル開口による\sphinxstylestrong{シナプス後電流}(postsynaptic current; PSC)の発生, という過程が起こる。かなり簡略化して書いたが, 実際にはかなりの過程を含くむ。しかし, これらの過程を全てモデル化するのは計算量がかなり大きくなるので, 基本的には簡易的な現象論的なモデルを用いる。

このように, シナプス前細胞のスパイク列(spike train)は次のニューロンにそのまま伝わるのではなく, ある種の時間的フィルターをかけられて伝わる。このフィルターを\sphinxstylestrong{シナプスフィルター}(synaptic filter)と呼ぶ。3章では, このようにシナプス前細胞で生じた発火が, シナプス後細胞の膜電位に与える過程のモデルについて説明する。


\subsection{11.1 ベイズ統計の基礎}
\label{\detokenize{11-1_bayes_statistics:id1}}\label{\detokenize{11-1_bayes_statistics::doc}}
この節では本章で用いるベイズ統計の基礎的概念の説明を行う。

\begin{sphinxadmonition}{note}{Note:}
悪いこと言わないので\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/index-j.html}{渡辺澄夫先生のHP}の講義録、特に\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/joho-gakushu6.html}{ベイズ統計入門}を読もう。
\end{sphinxadmonition}


\subsection{JuliaのTips集}
\label{\detokenize{tips:juliatips}}\label{\detokenize{tips::doc}}
このページはJuliaでの実装におけるTips (詰まったところの解決策)をまとめたものである。体系的にまとまってはいない。


\subsubsection{1. 関数名の!記号}
\label{\detokenize{tips:id1}}
単なる\sphinxstylestrong{慣習}として関数への入力を変更する場合に!を付ける。

関数内で配列を変更する場合には注意が必要である。以下に入力された配列を同じサイズの要素1の配列で置き換える、ということを目的として書かれた2つの関数がある。違いは\sphinxcode{\sphinxupquote{v}}の後に\sphinxcode{\sphinxupquote{{[}:{]}}}としているかどうかである。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function wrong!(A::Array)
    a = ones(size(a))
end

function right!(a::Array)
    a[:] = ones(size(a))
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
right! (generic function with 2 methods)
\end{sphinxVerbatim}

実行すると\sphinxcode{\sphinxupquote{wrong!}}の場合には入力された配列が変更されていないことがわかる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
using Random
v = rand(2, 2)
print(\PYGZdq{}v : \PYGZdq{}, v)

wrong!(v)
print(\PYGZdq{}\PYGZbs{}nwrong : \PYGZdq{}, v)

right!(v)
print(\PYGZdq{}\PYGZbs{}nright : \PYGZdq{}, v)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
v : [0.42692589476552056 0.34730711983824536; 0.9208034639794132 0.41699915079158134]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+ne}{UndefVarError}: wrong! not defined

\PYG{n+ne}{Stacktrace}:
 \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{n}{top}\PYG{o}{\PYGZhy{}}\PYG{n}{level} \PYG{n}{scope} \PYG{n}{at} \PYG{n}{In}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}\PYG{l+m+mi}{5}
 \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{n}{include\PYGZus{}string}\PYG{p}{(}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Function}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{Module}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{)} \PYG{n}{at} \PYG{o}{.}\PYGZbs{}\PYG{n}{loading}\PYG{o}{.}\PYG{n}{jl}\PYG{p}{:}\PYG{l+m+mi}{1091}
\end{sphinxVerbatim}


\subsubsection{2. 配列の1次元化}
\label{\detokenize{tips:id2}}
配列を一次元化(flatten)する方法。まずは3次元配列を作成する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2×2 Array\PYGZob{}Float64,3\PYGZcb{}:
[:, :, 1] =
 0.997107  0.0827965
 0.394606  0.847174

[:, :, 2] =
 0.0305417  0.448929
 0.270553   0.601419
\end{sphinxVerbatim}

用意されている\sphinxcode{\sphinxupquote{flatten}}を素直に用いると次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{Base}\PYG{n+nn}{.}\PYG{n+nn}{Iterators}\PYG{p}{:} \PYG{n}{flatten}
\PYG{n}{collect}\PYG{p}{(}\PYG{n}{flatten}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}

ただし、単に\sphinxcode{\sphinxupquote{B{[}:{]}}}とするだけでもよい。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}


\subsubsection{3. 行列の行・列ごとの正規化}
\label{\detokenize{tips:id3}}
シミュレーションにおいてニューロン間の重み行列を行あるいは列ごとに正規化 (weight normalization)する場合がある。これは各ニューロンへの入力の大きさを同じにする働きや重みの発散を防ぐ役割がある。以下では行ごとの和を1にする。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.166781  0.0552745  0.973622
 0.100336  0.753766   0.370005
 0.378222  0.165159   0.471515
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Wnormed} \PYG{o}{=} \PYG{n}{W} \PYG{o}{.}\PYG{o}{/} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{W}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.258439  0.0567384  0.536389
 0.155479  0.773729   0.203843
 0.586082  0.169533   0.259767
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{Wnormed}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1.0 1.0 1.0]
\end{sphinxVerbatim}


\subsubsection{4. 行列の結合 (concatenate)}
\label{\detokenize{tips:concatenate}}
行列の結合はMATLABに近い形式で行うことができる。まず、2つの行列A, Bを用意する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{l+m+mi}{3} \PYG{l+m+mi}{4}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{4} \PYG{l+m+mi}{5} \PYG{l+m+mi}{6}\PYG{p}{;} \PYG{l+m+mi}{7} \PYG{l+m+mi}{8} \PYG{l+m+mi}{9}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×3 Array\PYGZob{}Int64,2\PYGZcb{}:
 4  5  6
 7  8  9
\end{sphinxVerbatim}


\paragraph{4.1 水平結合 (Horizontal concatenation)}
\label{\detokenize{tips:horizontal-concatenation}}
\sphinxcode{\sphinxupquote{hcat}}を使うやり方と、\sphinxcode{\sphinxupquote{{[} {]}}}を使うやり方がある。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H1} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,}\PYG{n}{B}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

なお、MATLABのように次のようにすると正しく結合はされない。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H3} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{,} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2\PYGZhy{}element Array\PYGZob{}Array\PYGZob{}Int64,2\PYGZcb{},1\PYGZcb{}:
 [1 2; 3 4]
 [4 5 6; 7 8 9]
\end{sphinxVerbatim}


\paragraph{4.2 垂直結合 (Vertical concatenation)}
\label{\detokenize{tips:vertical-concatenation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V1} \PYG{o}{=} \PYG{n}{vcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{V2} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;}\PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×4 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  1  2
 3  4  3  4
 4  7  4  7
 5  8  5  8
 6  9  6  9
\end{sphinxVerbatim}


\subsubsection{5. 配列に新しい軸を追加}
\label{\detokenize{tips:id4}}
要はnumpyでの\sphinxcode{\sphinxupquote{A{[}None, :{]}}}や\sphinxcode{\sphinxupquote{A{[}np.newaxis, :{]}}}のようなことがしたい場合。やや面倒だが、\sphinxcode{\sphinxupquote{reshape}}を使うか、\sphinxcode{\sphinxupquote{{[}CartesianIndex(){]}}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.3487808667518093
 0.717776723811032
 0.3369916406749496
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{newaxis} \PYG{o}{=} \PYG{p}{[}\PYG{n}{CartesianIndex}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{v1} \PYG{o}{=} \PYG{n}{v}\PYG{p}{[}\PYG{n}{newaxis}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.348781  0.717777  0.336992
\end{sphinxVerbatim}


\subsubsection{6. Array\{Array\{Float64, x\},1\}をArray\{Float64, x+1\}に変換}
\label{\detokenize{tips:array-array-float64-x-1-array-float64-x-1}}
numpyでは\sphinxcode{\sphinxupquote{array({[}matrix for i in range(){]})}}などを用いると、1次元配列のリストを2次元配列に変換できた。Juliaでも同様にする場合は\sphinxcode{\sphinxupquote{hcat(...)}}や\sphinxcode{\sphinxupquote{cat(...)}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,1\PYGZcb{},1\PYGZcb{}
Size : (5,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A2} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : LinearAlgebra.Adjoint\PYGZob{}Float64,Array\PYGZob{}Float64,2\PYGZcb{}\PYGZcb{}
Size : (5, 3)
\end{sphinxVerbatim}

以下は多次元配列の場合。\sphinxcode{\sphinxupquote{cat(...)}}で配列を結合し、\sphinxcode{\sphinxupquote{permitedims}}で転置する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,3\PYGZcb{},1\PYGZcb{}
Size : (6,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B2} \PYG{o}{=} \PYG{n}{permutedims}\PYG{p}{(}\PYG{n}{cat}\PYG{p}{(}\PYG{n}{B1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Float64,4\PYGZcb{}
Size : (6, 3, 4, 5)
\end{sphinxVerbatim}


\subsection{11.1 ベイズ統計の基礎}
\label{\detokenize{11-1_bayes_statistics:id1}}\label{\detokenize{11-1_bayes_statistics::doc}}
この節では本章で用いるベイズ統計の基礎的概念の説明を行う。

\begin{sphinxadmonition}{note}{Note:}
悪いこと言わないので\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/index-j.html}{渡辺澄夫先生のHP}の講義録、特に\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/joho-gakushu6.html}{ベイズ統計入門}を読もう。
\end{sphinxadmonition}


\subsection{11.2 Sparse coding (Olshausen \& Field, 1996) モデル}
\label{\detokenize{11-2_sparse-coding:sparse-coding-olshausen-field-1996}}\label{\detokenize{11-2_sparse-coding::doc}}

\subsubsection{11.2.1 Sparse codingと生成モデル}
\label{\detokenize{11-2_sparse-coding:sparse-coding}}
Sparse codingモデル(\sphinxhref{https://www.nature.com/articles/381607a0}{Olshausen \& Field, \sphinxstyleemphasis{Nature}. 1996})はV1のニューロンの応答特性を説明する\sphinxstylestrong{線形生成モデル} (linear generative model)である。まず、画像パッチ \(\mathbf{x}\) が基底関数(basis function) \(\mathbf{\Phi} = [\phi_i]\) のノイズを含む線形和で表されるとする (係数は \(\mathbf{r}=[r_i]\) とする)。
\begin{equation*}
\begin{split}
\mathbf{x} = \sum_i r_i \phi_i + \mathbf{\epsilon} = \mathbf{\Phi} \mathbf{r}+ \mathbf{\epsilon} \quad \tag{1}
\end{split}
\end{equation*}
ただし、\(\mathbf{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})\) である。このモデルを神経ネットワークのモデルと考えると、 \(\mathbf{\Phi}\) は重み行列、係数 \(\mathbf{r}\) は入力よりも高次の神経細胞の活動度を表していると解釈できる。ただし、\(r_i\) は負の値も取るので単純に発火率と捉えられないのはこのモデルの欠点である。

Sparse codingでは神経活動 \(\mathbf{r}\) が潜在変数の推定量を表現しているという仮定の下、少数の基底で画像 (や目的変数)を表すことを目的とする。要は上式において、ほとんどが0で、一部だけ0以外の値を取るという疎 (=sparse)な係数\(\mathbf{r}\)を求めたい。


\paragraph{確率的モデルの記述}
\label{\detokenize{11-2_sparse-coding:id1}}
入力される画像パッチの真の分布を\(q(\mathbf{x})\), 生成モデルの分布を\(p(\mathbf{x}|\mathbf{\Phi})\)とする。さらに潜在変数 \(\mathbf{r}\)の事前分布 (prior)を\(p(\mathbf{r})\), 画像パッチ \(\mathbf{x}\)の尤度 (likelihood)を\(p(\mathbf{x}|\mathbf{r}, \mathbf{\Phi})\)とする。このとき、
\begin{equation*}
\begin{split}
p(\mathbf{x}|\mathbf{\Phi})=\int p(\mathbf{x}|\mathbf{r}, \mathbf{\Phi})p(\mathbf{r})d\mathbf{r} \quad \tag{2}
\end{split}
\end{equation*}
が成り立つ。\(p(\mathbf{x}|\mathbf{r}, \mathbf{\Phi})\)は、(1)式においてノイズ項を\(\epsilon_j \sim\mathcal{N}(0, \sigma^2)\)としたことから、
\begin{equation*}
\begin{split}
\begin{align}
p(\mathbf{x}|\ \mathbf{r}, \mathbf{\Phi})&=\mathcal{N}\left(\mathbf{x}|\ \mathbf{\Phi} \mathbf{r}, \sigma^2 \mathbf{I} \right)\\
&=\frac{1}{Z_{\sigma}} \exp\left(-\frac{\|\mathbf{x} - \mathbf{\Phi} \mathbf{r})\|^2}{2\sigma^2}\right)\quad \tag{3}
\end{align}
\end{split}
\end{equation*}
と表せる。ただし、\(Z_{\sigma}\)は規格化定数である。


\paragraph{事前分布の設定}
\label{\detokenize{11-2_sparse-coding:id2}}
事前分布\(p(\mathbf{r})\)としては、0においてピークがあり、裾の重い(heavy tail)を持つsparse distributionあるいは \sphinxstylestrong{super\sphinxhyphen{}Gaussian distribution} (Laplace 分布やCauchy分布などGaussian分布よりもkurtoticな分布)を用いるのが良い。このような分布では、\(\mathbf{r}\)の各要素\(r_i\)はほとんど0に等しく、ある入力に対しては大きな値を取る。\(p(\mathbf{r})\)は一般化して式(4), (5)のように表記する。
\begin{equation*}
\begin{split}
\begin{align}
p(\mathbf{r})&=\prod_i p(r_i) \quad \tag{4}\\
p(r_i)&=\frac{1}{Z_{\beta}}\exp \left[-\beta S(r_i)\right] \quad \tag{5}
\end{align}
\end{split}
\end{equation*}
ただし、\(\beta\)は逆温度(inverse temperature), \(Z_{\beta}\)は規格化定数 (分配関数) である%
\begin{footnote}[1]\sphinxAtStartFootnote
これらの用語は統計力学における正準分布 (ボルツマン分布)から来ている。
%
\end{footnote}。\(S(x)\)と分布の関係をまとめた表が以下となる (cf. \sphinxhref{https://pdfs.semanticscholar.org/be08/da912362bf40fe3ded78bdadc644f921b4e7.pdf}{Harpur, 1997})。


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\(S(r)\)
&\sphinxstyletheadfamily 
\(\dfrac{dS(r)}{dr}\)
&\sphinxstyletheadfamily 
\(p(r)\)
&\sphinxstyletheadfamily 
分布名
&\sphinxstyletheadfamily 
尖度(kurtosis)
\\
\hline
\(r^2\)
&
\(2r\)
&
\(\dfrac{1}{\alpha \sqrt{2\pi}}\exp\left(-\dfrac{r^2}{2\alpha^2}\right)\)
&
Gaussian 分布
&
0
\\
\hline
\(\vert r\vert\)
&
\(\text{sign}(r)\)
&
\(\dfrac{1}{2\alpha}\exp\left(-\dfrac{\vert r\vert}{\alpha}\right)\)
&
Laplace 分布
&
3.0
\\
\hline
\(\ln (\alpha^2+r^2)\)
&
\(\dfrac{2r}{\alpha^2+r^2}\)
&
\(\dfrac{\alpha}{\pi}\dfrac{1}{\alpha^2+r^2}=\dfrac{\alpha}{\pi}\exp[-\ln (\alpha^2+r^2)]\)
&
Cauchy 分布
&
\sphinxhyphen{}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

分布\(p(r)\)や\(S(r)\)を描画すると次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{PyPlot}

\PYG{n}{x} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{length}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}
\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{title}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}p(x)\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{n}{pi}\PYG{p}{)}\PYG{o}{*}\PYG{n}{exp}\PYG{o}{.}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gaussian}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{exp}\PYG{o}{.}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{abs}\PYG{o}{.}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Laplace}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{.}\PYG{o}{/} \PYG{p}{(}\PYG{n}{pi}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{.}\PYG{o}{+} \PYG{n}{x}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cauchy}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{;} 
\PYG{n}{xlabel}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}x\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{title}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{S(x)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gaussian}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n+nb}{abs}\PYG{o}{.}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Laplace}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{log}\PYG{o}{.}\PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{.}\PYG{o}{+} \PYG{n}{x}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cauchy}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{;} \PYG{n}{ylim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{xlabel}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}x\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{11-2_sparse-coding_6_0}.png}


\subsubsection{11.2.2 MAP推定と目的関数の設定}
\label{\detokenize{11-2_sparse-coding:map}}
最適な生成モデルを得るために、入力される画像パッチの真の分布 \(q(\mathbf{x})\)と生成モデルの分布 \(p(\mathbf{x}|\mathbf{\Phi})\)を近づける。すなわち、2つの分布のKullback\sphinxhyphen{}Leibler ダイバージェンス \(D_{\text{KL}}\left(q(\mathbf{x}) \Vert\ p(\mathbf{x}|\mathbf{\Phi})\right)\)を最小化する。ただし、
\begin{equation*}
\begin{split}
\begin{align}
D_{\text{KL}}(q(\mathbf{x}) \| p(\mathbf{x}|\mathbf{\Phi}))&=\int q(\mathbf{x}) \log \frac{q(\mathbf{x})}{p(\mathbf{x}|\mathbf{\Phi})} d\mathbf{x}\\
&=\mathbb{E}_q \left[\ln \frac{q(\mathbf{x})}{p(\mathbf{x}|\mathbf{\Phi})}\right]\\
&=\mathbb{E}_q \left[\ln q(\mathbf{x})\right]-\mathbb{E}_q \left[\ln p(\mathbf{x}|\mathbf{\Phi})\right] \tag{6}
\end{align}
\end{split}
\end{equation*}
が成り立つ。(6)式の1番目の項は一定なので、\(D_{\text{KL}}\)を最小化するには\(\mathbb{E}_q \left[\ln p(\mathbf{x}|\mathbf{\Phi})\right]\)を最大化すればよい。ここで、(2)式より、
\begin{equation*}
\begin{split}
\mathbb{E}_q \left[\ln p(\mathbf{x}|\mathbf{\Phi})\right]=\mathbb{E}_q \left[\ln \int p(\mathbf{x}|\mathbf{r}, \mathbf{\Phi})p(\mathbf{r})d\mathbf{r}\right]\tag{7}
\end{split}
\end{equation*}
が成り立つ。ここで近似として \(\int p(\mathbf{x}|\mathbf{r}, \mathbf{\Phi})p(\mathbf{r})d\mathbf{r}\) を \(p(\mathbf{x}|\mathbf{r}, \mathbf{\Phi})p(\mathbf{r}) \left(=p(\mathbf{x}, \mathbf{r}| \mathbf{\Phi})\right)\) の最大値で評価することにする。この近似の下、最適な\(\mathbf{\Phi}=\mathbf{\Phi}^*\)は次のようにして求められる。
\begin{equation*}
\begin{split}
\begin{align}
\mathbf{\Phi}^*&=\text{arg} \min_{\mathbf{\Phi}} \min_{\mathbf{r}} D_{\text{KL}}(q(\mathbf{x}) \| p(\mathbf{x}|\mathbf{\Phi}))\\
&=\text{arg} \max_{\mathbf{\Phi}} \max_{\mathbf{r}} \mathbb{E}_q \left[\ln p(\mathbf{x}|\mathbf{\Phi})\right]\\
&\approx \text{arg} \max_{\mathbf{\Phi}}\max_{\mathbf{r}} \ln p(\mathbf{x}|\mathbf{r}, \mathbf{\Phi})p(\mathbf{r})\\
&=\text{arg}\min_{\mathbf{\Phi}} \min_{\mathbf{r}}\ E(\mathbf{x}, \mathbf{r}|\mathbf{\Phi})\tag{8}
\end{align}
\end{split}
\end{equation*}
ただし、\(E(\mathbf{x}, \mathbf{r}|\mathbf{\Phi})\)はコスト関数であり、次式のように表される。
\begin{equation*}
\begin{split}
\begin{align}
E(\mathbf{x}, \mathbf{r}|\mathbf{\Phi}):=&-\ln p(\mathbf{x}|\mathbf{r}, \mathbf{\Phi})p(\mathbf{r})\\
=&\underbrace{\left\|\mathbf{x}-\mathbf{\Phi} \mathbf{r}\right\|^2}_{\text{preserve information}} + \lambda \underbrace{\sum_i S\left(r_i\right)}_{\text{sparseness of}\ r_i}\tag{9}
\end{align}
\end{split}
\end{equation*}
ただし、\(\lambda=2\sigma^2\beta\)は正則化係数%
\begin{footnote}[2]\sphinxAtStartFootnote
この式から逆温度\(\beta\)が正則化の度合いを調整するパラメータであることがわかる。
%
\end{footnote}であり、1行目から2行目へは式(3), (4), (5)を用いた。ここで、第1項が復元損失、第2項が罰則項 (正則化項)となっている。勾配法により、\(E(\mathbf{x}, \mathbf{r}|\mathbf{\Phi})\)を最小化する。これには\(\mathbf{\Phi}\)を固定した下で\(E(\mathbf{x}, \mathbf{r}|\mathbf{\Phi})\)を最小化する\(\mathbf{r}=\hat{\mathbf{r}}\)を求める ({\hyperref[\detokenize{11-2_sparse-coding:locally-competitive-algorithm-lca}]{\emph{11.1.3}}})。
\begin{equation*}
\begin{split}
\hat{\mathbf{r}}=\text{arg}\min_{\mathbf{r}}E(\mathbf{x}, \mathbf{r}|\mathbf{\Phi})
\end{split}
\end{equation*}
次に\(\hat{\mathbf{r}}\)を用いて
\begin{equation*}
\begin{split}
\mathbf{\Phi}^*=\text{arg}\min_{\mathbf{\Phi}}\langle E(\mathbf{x}, \hat{\mathbf{r}}|\mathbf{\Phi})\rangle
\end{split}
\end{equation*}
とすることにより、\(\mathbf{\Phi}\)を最適化する ({\hyperref[\detokenize{11-2_sparse-coding:id6}]{\emph{11.1.4}}})。ただし、\(\langle\cdot \rangle\)は複数の画像に対する平均を取ることを意味する。


\subsubsection{11.2.3 Locally competitive algorithm (LCA)}
\label{\detokenize{11-2_sparse-coding:locally-competitive-algorithm-lca}}
\(\mathbf{r}\)の勾配法による更新則は、\(E\)の微分により次のように得られる。
\begin{equation*}
\begin{split}
\frac{d \mathbf{r}}{dt}= -\frac{\eta_\mathbf{r}}{2}\frac{\partial E}{\partial \mathbf{r}}=\eta_\mathbf{r} \cdot\left[\mathbf{\Phi}^T (\mathbf{x}-\mathbf{\Phi}\mathbf{r})- \frac{\lambda}{2}S'\left(\mathbf{r}\right)\right]
\end{split}
\end{equation*}
ただし、\(\eta_{\mathbf{r}}\)は学習率である。この式により\(\mathbf{r}\)が収束するまで最適化するが、単なる勾配法ではなく、(Olshausen \& Field, 1996)では\sphinxstylestrong{共役勾配法} (conjugate gradient method)を用いている。しかし、共役勾配法は実装が煩雑で非効率であるため、より効率的かつ生理学的な妥当性の高い学習法として、\sphinxstylestrong{LCA}  (locally competitive algorithm)が提案されている (\sphinxhref{https://www.ece.rice.edu/~eld1/papers/Rozell08.pdf}{Rozell et al., \sphinxstyleemphasis{Neural Comput}. 2008})。LCAは\sphinxstylestrong{側抑制} (local competition, lateral inhibition)と\sphinxstylestrong{閾値関数} (thresholding function)を用いる更新則である。LCAによる更新を行うRNNは通常のRNNとは異なり、コスト関数(またはエネルギー関数)を最小化する動的システムである。このような機構はHopfield networkで用いられているために、Olshausenは\sphinxstylestrong{Hopfield trick}と呼んでいる。


\paragraph{軟判定閾値関数を用いる場合 (ISTA)}
\label{\detokenize{11-2_sparse-coding:ista}}
\(S(x)=|x|\)とした場合の閾値関数を用いる手法として\sphinxstylestrong{ISTA}(Iterative Shrinkage Thresholding Algorithm)がある。ISTAはL1\sphinxhyphen{}norm正則化項に対する近接勾配法で、要はLasso回帰に用いる勾配法である。

解くべき問題は次式で表される。
\begin{equation*}
\begin{split}
\mathbf{r} = \mathop{\rm arg~min}\limits_{\mathbf{r}}\left\{\|\mathbf{x}-\mathbf{\Phi}\mathbf{r}\|^2_2+\lambda\|\mathbf{r}\|_1\right\}
\end{split}
\end{equation*}
詳細は後述するが、次のように更新することで解が得られる。
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\(\mathbf{r}(0)\)を要素が全て0のベクトルで初期化：\(\mathbf{r}(0)=\mathbf{0}\)

\item {} 
\(\mathbf{r}_*(t+1)=\mathbf{r}(t)+\eta_\mathbf{r}\cdot \mathbf{\Phi}^T(\mathbf{x}-\mathbf{\Phi}\mathbf{r}(t))\)

\item {} 
\(\mathbf{r}(t+1) = \Theta_\lambda(\mathbf{r}_*(t+1))\)

\item {} 
\(\mathbf{r}\)が収束するまで2と3を繰り返す

\end{enumerate}

ここで\(\Theta_\lambda(\cdot)\)は\sphinxstylestrong{軟判定閾値関数} (Soft thresholding function)と呼ばれ、次式で表される。
\begin{equation*}
\begin{split}
\Theta_\lambda(y)= 
\begin{cases} 
y-\lambda & (y>\lambda)\\ 
0 & (-\lambda\leq y\leq\lambda)\\ 
 y+\lambda & (y<-\lambda) 
\end{cases}
\end{split}
\end{equation*}
\(\Theta_\lambda(\cdot)\)を関数として定義すると次のようになる %
\begin{footnote}[3]\sphinxAtStartFootnote
ReLU (ランプ関数)は\sphinxcode{\sphinxupquote{max(x, 0)}}で実装できる。この点から考えればReLUを軟判定非負閾値関数 (soft nonnegative thresholding function)と捉えることもできる (\sphinxhref{https://ieeexplore.ieee.org/document/8398588}{Papyan et al., 2018})。
%
\end{footnote}。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} thresholding function of S(x)=|x|}
\PYG{n}{function} \PYG{n}{soft\PYGZus{}thresholding\PYGZus{}func}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{lmda}\PYG{p}{)}
    \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{lmda}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n+nb}{max}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{lmda}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
soft\PYGZus{}thresholding\PYGZus{}func (generic function with 1 method)
\end{sphinxVerbatim}

次に\(\Theta_\lambda(\cdot)\)を描画すると次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{x} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{p}{,} \PYG{n}{length}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{soft\PYGZus{}thresholding\PYGZus{}func}\PYG{o}{.}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mf}{4.5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{title}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}S(x)=|x|\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n+nb}{abs}\PYG{o}{.}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{xlim}\PYG{p}{(}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{p}{)}\PYG{p}{;} \PYG{n}{ylim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{hlines}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{xmax}\PYG{p}{,} \PYG{n}{xmin}\PYG{o}{=}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{o}{=}\PYG{n}{xmax}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{vlines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{n}{xmax}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}

\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{title}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+se}{\PYGZbs{}f}\PYG{l+s+s2}{rac}\PYG{l+s+s2}{\PYGZob{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{partial S(x)\PYGZcb{}}\PYG{l+s+s2}{\PYGZob{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{partial x\PYGZcb{}\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sign}\PYG{o}{.}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{xlim}\PYG{p}{(}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{p}{)}\PYG{p}{;} \PYG{n}{ylim}\PYG{p}{(}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{p}{)}
\PYG{n}{hlines}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{xmin}\PYG{o}{=}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{o}{=}\PYG{n}{xmax}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{vlines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{n}{xmax}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}

\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{title}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}f\PYGZus{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{lambda(x)=x+}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{lambda}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{cdot}\PYG{l+s+se}{\PYGZbs{}f}\PYG{l+s+s2}{rac}\PYG{l+s+s2}{\PYGZob{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{partial S(x)\PYGZcb{}}\PYG{l+s+s2}{\PYGZob{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{partial x\PYGZcb{}\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{o}{*}\PYG{n}{sign}\PYG{o}{.}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{xlabel}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}x\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{;} \PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{hlines}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{xmin}\PYG{o}{=}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{o}{=}\PYG{n}{xmax}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{vlines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{n}{xmax}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}

\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n}{title}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{Theta\PYGZus{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{lambda(x)\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{xlabel}\PYG{p}{(}\PYG{n}{L}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}x\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{;} \PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{hlines}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{xmin}\PYG{o}{=}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{o}{=}\PYG{n}{xmax}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{vlines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{n}{xmax}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{k}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}

\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{11-2_sparse-coding_12_0}.png}

なお、軟判定閾値関数は次の目的関数\(C\)を最小化する\(x\)を求めることで導出できる。
\begin{equation*}
\begin{split}
C=\frac{1}{2}(y-x)^2+\lambda |x|
\end{split}
\end{equation*}
ただし、\(x, y, \lambda\)はスカラー値とする。\(|x|\)が微分できないが、これは場合分けを考えることで解決する。\(x\geq 0\)を考えると、(6)式は
\begin{equation*}
\begin{split}
C=\frac{1}{2}(y-x)^2+\lambda x = \{x-(y-\lambda)\}^2+\lambda(y-\lambda)
\end{split}
\end{equation*}
となる。(7)式の最小値を与える\(x\)は場合分けをして考えると、\(y-\lambda\geq0\)のとき二次関数の頂点を考えて\(x=y-\lambda\)となる。 一方で\(y-\lambda<0\)のときは\(x\geq0\)において単調増加な関数となるので、最小となるのは\(x=0\)のときである。同様の議論を\(x\leq0\)に対しても行うことで (5)式が得られる。

なお、閾値関数としては軟判定閾値関数だけではなく、硬判定閾値関数や\(y=x - \text{tanh}(x)\) (Tanh\sphinxhyphen{}shrink)など様々な関数を用いることができる。


\subsubsection{11.2.4 重み行列の更新則}
\label{\detokenize{11-2_sparse-coding:id6}}
\(\mathbf{r}\)が収束したら勾配法により\(\mathbf{\Phi}\)を更新する。
\begin{equation*}
\begin{split}
\Delta \phi_i(\boldsymbol{x}) = -\eta \frac{\partial E}{\partial \mathbf{\Phi}}=\eta\cdot\left[\left([\mathbf{x}-\mathbf{\Phi}\mathbf{r}\right)\mathbf{r}^T\right]
\end{split}
\end{equation*}

\subsubsection{11.2.5 Sparse coding networkの実装}
\label{\detokenize{11-2_sparse-coding:sparse-coding-network}}
ネットワークは入力層を含め2層の単純な構造である。今回は、入力はランダムに切り出した16×16 (＝256)の画像パッチとし、これを入力層の256個のニューロンが受け取るとする。入力層のニューロンは次層の100個のニューロンに投射するとする。100個のニューロンが入力をSparseに符号化するようにその活動および重み行列を最適化する。


\paragraph{画像データの読み込み}
\label{\detokenize{11-2_sparse-coding:id7}}
データは\sphinxurl{http://www.rctn.org/bruno/sparsenet/}からダウンロードできる %
\begin{footnote}[4]\sphinxAtStartFootnote
これはアメリカ北西部で撮影された自然画像であり、\sphinxhref{http://bethgelab.org/datasets/vanhateren/}{van Hateren’s Natural Image Dataset}から取得されたものである。
%
\end{footnote}。\sphinxcode{\sphinxupquote{IMAGES\_RAW.mat}}は10枚の自然画像で、\sphinxcode{\sphinxupquote{IMAGES.mat}}はそれを白色化したものである。\sphinxcode{\sphinxupquote{mat}}ファイルの読み込みには\sphinxhref{https://github.com/JuliaIO/MAT.jl}{MAT.jl}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{MAT}
\PYG{c+c1}{\PYGZsh{}using PyPlot}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} datasets from http://www.rctn.org/bruno/sparsenet/}
\PYG{n}{mat\PYGZus{}images\PYGZus{}raw} \PYG{o}{=} \PYG{n}{matopen}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}static/datasets/IMAGES\PYGZus{}RAW.mat}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{imgs\PYGZus{}raw} \PYG{o}{=} \PYG{n}{read}\PYG{p}{(}\PYG{n}{mat\PYGZus{}images\PYGZus{}raw}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IMAGESr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{mat\PYGZus{}images} \PYG{o}{=} \PYG{n}{matopen}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}static/datasets/IMAGES.mat}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{imgs} \PYG{o}{=} \PYG{n}{read}\PYG{p}{(}\PYG{n}{mat\PYGZus{}images}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IMAGES}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{close}\PYG{p}{(}\PYG{n}{mat\PYGZus{}images\PYGZus{}raw}\PYG{p}{)}
\PYG{n}{close}\PYG{p}{(}\PYG{n}{mat\PYGZus{}images}\PYG{p}{)}
\end{sphinxVerbatim}

画像データを描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{subplots\PYGZus{}adjust}\PYG{p}{(}\PYG{n}{hspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{wspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{10}
    \PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}
    \PYG{n}{imshow}\PYG{p}{(}\PYG{n}{imgs\PYGZus{}raw}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gray}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{end}
\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Natural Images}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{subplots\PYGZus{}adjust}\PYG{p}{(}\PYG{n}{top}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{p}{)}  
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{11-2_sparse-coding_20_0}.png}


\paragraph{モデルの定義}
\label{\detokenize{11-2_sparse-coding:id9}}
必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\PYG{n}{using} \PYG{n}{LinearAlgebra}
\PYG{n}{using} \PYG{n}{Random}
\PYG{n}{using} \PYG{n}{Statistics}
\PYG{n}{using} \PYG{n}{ProgressMeter}
\end{sphinxVerbatim}

モデルを定義する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{OFParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{lr\PYGZus{}r}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}2} \PYG{c+c1}{\PYGZsh{} learning rate of r}
    \PYG{n}{lr\PYGZus{}Phi}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}2} \PYG{c+c1}{\PYGZsh{} learning rate of Phi}
    \PYG{n}{lmda}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{5e\PYGZhy{}3} \PYG{c+c1}{\PYGZsh{} regularization parameter}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{OlshausenField1996Model}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{OFParameter} \PYG{o}{=} \PYG{n}{OFParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{num\PYGZus{}inputs}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Int32}
    \PYG{n}{num\PYGZus{}units}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Int32}
    \PYG{n}{batch\PYGZus{}size}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Int32}
    \PYG{n}{r}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Array}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{num\PYGZus{}units}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} activity of neurons}
    \PYG{n}{Phi}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Array}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}inputs}\PYG{p}{,} \PYG{n}{num\PYGZus{}units}\PYG{p}{)} \PYG{o}{.}\PYG{o}{*} \PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{num\PYGZus{}units}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

パラメータを更新する関数を定義する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function updateOF!(variable::OlshausenField1996Model, param::OFParameter, inputs::Array, training::Bool)
    @unpack num\PYGZus{}inputs, num\PYGZus{}units, batch\PYGZus{}size, r, Phi = variable
    @unpack lr\PYGZus{}r, lr\PYGZus{}Phi, lmda = param

    \PYGZsh{} Updates                
    error = inputs .\PYGZhy{} r * Phi\PYGZsq{}
    r\PYGZus{} = r +lr\PYGZus{}r .* error * Phi

    r[:, :] = soft\PYGZus{}thresholding\PYGZus{}func.(r\PYGZus{}, lmda)

    if training 
        error = inputs \PYGZhy{} r * Phi\PYGZsq{}
        dPhi = error\PYGZsq{} * r
        Phi[:, :] += lr\PYGZus{}Phi * dPhi
    end
    
    return error
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
updateOF! (generic function with 1 method)
\end{sphinxVerbatim}

行ごとに正規化する関数を定義する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{function} \PYG{n}{normalize\PYGZus{}rows}\PYG{p}{(}\PYG{n}{A}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Array}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{A} \PYG{o}{.}\PYG{o}{/} \PYG{n}{sqrt}\PYG{o}{.}\PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{A}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{.}\PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}8}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
normalize\PYGZus{}rows (generic function with 1 method)
\end{sphinxVerbatim}

損失関数を定義する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{function} \PYG{n}{calculate\PYGZus{}total\PYGZus{}error}\PYG{p}{(}\PYG{n}{error}\PYG{p}{,} \PYG{n}{r}\PYG{p}{,} \PYG{n}{lmda}\PYG{p}{)}
    \PYG{n}{recon\PYGZus{}error} \PYG{o}{=} \PYG{n}{mean}\PYG{p}{(}\PYG{n}{error}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{sparsity\PYGZus{}r} \PYG{o}{=} \PYG{n}{lmda}\PYG{o}{*}\PYG{n}{mean}\PYG{p}{(}\PYG{n+nb}{abs}\PYG{o}{.}\PYG{p}{(}\PYG{n}{r}\PYG{p}{)}\PYG{p}{)} 
    \PYG{k}{return} \PYG{n}{recon\PYGZus{}error} \PYG{o}{+} \PYG{n}{sparsity\PYGZus{}r}
\PYG{n}{end}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
calculate\PYGZus{}total\PYGZus{}error (generic function with 1 method)
\end{sphinxVerbatim}

シミュレーションを実行する関数を定義する。外側の\sphinxcode{\sphinxupquote{for loop}}では画像パッチの作成と\sphinxcode{\sphinxupquote{r}}の初期化を行う。内側の\sphinxcode{\sphinxupquote{for loop}}では\sphinxcode{\sphinxupquote{r}}が収束するまで更新を行い、収束したときに重み行列\sphinxcode{\sphinxupquote{Phi}}を更新する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function run\PYGZus{}simulation(imgs, num\PYGZus{}iter, nt\PYGZus{}max, batch\PYGZus{}size, sz, num\PYGZus{}units, eps)
    H, W, num\PYGZus{}images = size(imgs)
    num\PYGZus{}inputs = sz\PYGZca{}2

    model = OlshausenField1996Model\PYGZob{}Float32\PYGZcb{}(num\PYGZus{}inputs=num\PYGZus{}inputs, num\PYGZus{}units=num\PYGZus{}units, batch\PYGZus{}size=batch\PYGZus{}size)
    errorarr = zeros(num\PYGZus{}iter) \PYGZsh{} Vector to save errors    
    
    \PYGZsh{} Run simulation
    @showprogress \PYGZdq{}Computing...\PYGZdq{} for iter in 1:num\PYGZus{}iter
        \PYGZsh{} Get the coordinates of the upper left corner of clopping image randomly.
        beginx = rand(1:W\PYGZhy{}sz, batch\PYGZus{}size)
        beginy = rand(1:H\PYGZhy{}sz, batch\PYGZus{}size)

        inputs = zeros(batch\PYGZus{}size, num\PYGZus{}inputs)  \PYGZsh{} Input image patches

        \PYGZsh{} Get images randomly
        for i in 1:batch\PYGZus{}size        
            idx = rand(1:num\PYGZus{}images)
            img = imgs[:, :, idx]
            clop = img[beginy[i]:beginy[i]+sz\PYGZhy{}1, beginx[i]:beginx[i]+sz\PYGZhy{}1][:]
            \PYGZsh{}clop = collect(flatten(img[beginy[i]:beginy[i]+sz\PYGZhy{}1, beginx[i]:beginx[i]+sz\PYGZhy{}1]))
            inputs[i, :] = clop .\PYGZhy{} mean(clop)
        end

        model.r = zeros(batch\PYGZus{}size, num\PYGZus{}units) \PYGZsh{} Reset r states
        model.Phi = normalize\PYGZus{}rows(model.Phi) \PYGZsh{} Normalize weights
        \PYGZsh{} Input image patches until latent variables are converged 
        r\PYGZus{}tm1 = zeros(batch\PYGZus{}size, num\PYGZus{}units)  \PYGZsh{} set previous r (t minus 1)

        for t in 1:nt\PYGZus{}max
            \PYGZsh{} Update r without update weights 
            error = updateOF!(model, model.param, inputs, false)

            dr = model.r \PYGZhy{} r\PYGZus{}tm1 

            \PYGZsh{} Compute norm of r
            dr\PYGZus{}norm = sqrt(sum(dr.\PYGZca{}2)) / sqrt(sum(r\PYGZus{}tm1.\PYGZca{}2) + 1e\PYGZhy{}8)
            r\PYGZus{}tm1 = copy(model.r) \PYGZsh{} update r\PYGZus{}tm1

            \PYGZsh{} Check convergence of r, then update weights
            if dr\PYGZus{}norm \PYGZlt{} eps
                error = updateOF!(model, model.param, inputs, true)
                errorarr[iter] = calculate\PYGZus{}total\PYGZus{}error(error, model.r, model.param.lmda) \PYGZsh{} Append errors
                break
            end

            \PYGZsh{} If failure to convergence, break and print error
            if t \PYGZgt{}= nt\PYGZus{}max\PYGZhy{}1
                print(\PYGZdq{}Error at patch:\PYGZdq{}, iter\PYGZus{}, dr\PYGZus{}norm)
                errorarr[iter] = calculate\PYGZus{}total\PYGZus{}error(error, model.r, model.param.lmda) \PYGZsh{} Append errors
                break
            end
        end
        \PYGZdq{}\PYGZdq{}\PYGZdq{}
        \PYGZsh{} Print moving average error
        if iter \PYGZpc{} 100 == 0
            moving\PYGZus{}average\PYGZus{}error = mean(errorarr[iter\PYGZhy{}99:iter])
            println(\PYGZdq{}iter: \PYGZdq{}, iter, \PYGZdq{}/\PYGZdq{}, num\PYGZus{}iter, \PYGZdq{}, Moving average error:\PYGZdq{}, moving\PYGZus{}average\PYGZus{}error)
        end
        \PYGZdq{}\PYGZdq{}\PYGZdq{}
    end
    return model, errorarr
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
run\PYGZus{}simulation (generic function with 1 method)
\end{sphinxVerbatim}


\paragraph{シミュレーションの実行}
\label{\detokenize{11-2_sparse-coding:id10}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Simulation constants}
\PYG{n}{num\PYGZus{}iter} \PYG{o}{=} \PYG{l+m+mi}{500} \PYG{c+c1}{\PYGZsh{} number of iterations}
\PYG{n}{nt\PYGZus{}max} \PYG{o}{=} \PYG{l+m+mi}{1000} \PYG{c+c1}{\PYGZsh{} Maximum number of simulation time}
\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{250} \PYG{c+c1}{\PYGZsh{} Batch size}

\PYG{n}{sz} \PYG{o}{=} \PYG{l+m+mi}{16} \PYG{c+c1}{\PYGZsh{} image patch size}
\PYG{n}{num\PYGZus{}units} \PYG{o}{=} \PYG{l+m+mi}{100} \PYG{c+c1}{\PYGZsh{} number of neurons (units)}
\PYG{n}{eps} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}2} \PYG{c+c1}{\PYGZsh{} small value which determines convergence}

\PYG{n}{model}\PYG{p}{,} \PYG{n}{errorarr} \PYG{o}{=} \PYG{n}{run\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{imgs}\PYG{p}{,} \PYG{n}{num\PYGZus{}iter}\PYG{p}{,} \PYG{n}{nt\PYGZus{}max}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{sz}\PYG{p}{,} \PYG{n}{num\PYGZus{}units}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{訓練中の損失の描画}
\label{\detokenize{11-2_sparse-coding:id11}}
訓練中の損失の変化を描画してみよう。損失が低下し、学習が進行したことが分かる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot error}
\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Iterations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{num\PYGZus{}iter}\PYG{p}{,} \PYG{n}{errorarr}\PYG{p}{)}
\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{11-2_sparse-coding_36_0}.png}


\paragraph{重み行列 (受容野)の描画}
\label{\detokenize{11-2_sparse-coding:id12}}
学習後の重み行列 \sphinxcode{\sphinxupquote{Phi}} (\(\mathbf{\Phi}\))を可視化してみよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot Receptive fields}
\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.2}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{subplots\PYGZus{}adjust}\PYG{p}{(}\PYG{n}{hspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{wspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{num\PYGZus{}units}
    \PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}
    \PYG{n}{imshow}\PYG{p}{(}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{Phi}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{p}{(}\PYG{n}{sz}\PYG{p}{,} \PYG{n}{sz}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gray}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{end}
\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Receptive fields}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{)}
\PYG{n}{subplots\PYGZus{}adjust}\PYG{p}{(}\PYG{n}{top}\PYG{o}{=}\PYG{l+m+mf}{0.925}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{11-2_sparse-coding_38_0}.png}

白色が\sphinxstylestrong{ON領域}(興奮)、黒色が\sphinxstylestrong{OFF領域}(抑制)を表す。Gaborフィルタ様の局所受容野が得られており、これは一次視覚野(V1)における単純型細胞(simple cells)の受容野に類似している。

\begin{sphinxadmonition}{note}{論文以外の参考資料}
\begin{itemize}
\item {} 
\sphinxurl{http://www.scholarpedia.org/article/Sparse\_coding}

\item {} 
Bruno Olshausen: “Sparse coding in brains and machines”(\sphinxhref{https://talks.stanford.edu/bruno-olshausen-sparse-coding-in-brains-and-machines/}{Stanford talks}), \sphinxhref{http://www.rctn.org/bruno/public/Simons-sparse-coding.pdf}{Slide}

\item {} 
\sphinxurl{https://redwood.berkeley.edu/wp-content/uploads/2018/08/sparse-coding-ICA.pdf}

\item {} 
\sphinxurl{https://redwood.berkeley.edu/wp-content/uploads/2018/08/sparse-coding-LCA.pdf}

\item {} 
\sphinxurl{https://redwood.berkeley.edu/wp-content/uploads/2018/08/Dylan-lca\_overcompleteness\_09-27-2018.pdf}

\end{itemize}
\end{sphinxadmonition}


\bigskip\hrule\bigskip



\subsection{11.3 Predictive coding (Rao \& Ballard, 1999) モデル}
\label{\detokenize{11-3_predictive-coding-rao:predictive-coding-rao-ballard-1999}}\label{\detokenize{11-3_predictive-coding-rao::doc}}
Predictive codingの初めの数理的モデルとなる (\sphinxhref{https://www.nature.com/articles/nn0199\_79}{Rao \& Ballard, \sphinxstyleemphasis{Nat. Neurosci}. 1999})を実装する。


\subsubsection{11.3.1 観測世界の階層的予測}
\label{\detokenize{11-3_predictive-coding-rao:id1}}
構築するネットワークは入力層を含め、3層のネットワークとする。網膜への入力として画像 \(\boldsymbol{I} \in \mathbb{R}^{n_0}\)を考える。画像 \(\boldsymbol{I}\) の観測世界における隠れ変数、すなわち\sphinxstylestrong{潜在変数} (latent variable)を\(\boldsymbol{r} \in \mathbb{R}^{n_1}\)とし、ニューロン群によって発火率で表現されているとする (真の変数と \(\boldsymbol{r}\)は異なるので文字を分けるべきだが簡単のためにこう表す)。このとき、
\begin{equation*}
\begin{split}
\boldsymbol{I} = f(U\boldsymbol{r}) + \boldsymbol{n} \tag{1}
\end{split}
\end{equation*}
が成立しているとする。ただし、\(f(\cdot)\)は活性化関数 (activation function)、\(U \in \mathbb{R}^{n_0 \times n_1}\)は重み行列である。\$\textbackslash{}boldsymbol\{n\} \textbackslash{}in \textbackslash{}mathbb\{R\}\textasciicircum{}\{n\_0\} \$は平均0, 分散 \(\sigma^2\) のGaussian ノイズ項とする。

潜在変数 \(\boldsymbol{r}\)はさらに高次 (higher\sphinxhyphen{}level)の潜在変数 \(\boldsymbol{r}^h\)により、次式で表現される。
\begin{equation*}
\begin{split}
\boldsymbol{r} = \boldsymbol{r}^{td}+\boldsymbol{n}^{td}=f(U^h \boldsymbol{r}^h)+\boldsymbol{n}^{td} \tag{2}
\end{split}
\end{equation*}
ただし、Top\sphinxhyphen{}downの予測信号を \(\boldsymbol{r}^{td}:=f(U^h \boldsymbol{r}^h)\)とした。また、\$\textbackslash{}boldsymbol\{r\}\textasciicircum{}\{td\} \textbackslash{}in \textbackslash{}mathbb\{R\}\textasciicircum{}\{n\_1\}, \textbackslash{}boldsymbol\{r\}\textasciicircum{}\{h\} \textbackslash{}in \textbackslash{}mathbb\{R\}\textasciicircum{}\{n\_2\}, U\textasciicircum{}h \textbackslash{}in \textbackslash{}mathbb\{R\}\textasciicircum{}\{n\_1 \textbackslash{}times n\_2\} \(である。\)\textbackslash{}boldsymbol\{n\}\textasciicircum{}\{td\} \textbackslash{}in \textbackslash{}mathbb\{R\}\textasciicircum{}\{n\_1\} \$は平均0, 分散 \(\sigma_{td}^2\) のGaussian ノイズ項とする。

話は飛ぶが、Predictive codingのネットワークの特徴は
\begin{itemize}
\item {} 
階層的な構造

\item {} 
高次による低次の予測 (Feedback or Top\sphinxhyphen{}down信号)

\item {} 
低次から高次への誤差信号の伝搬 (Feedforward or Bottom\sphinxhyphen{}up 信号)

\end{itemize}

である。ここまでは高次表現による低次表現の予測、というFeedback信号について説明してきたが、この部分はSparse codingでも同じである。それではPredictive codingのもう一つの要となる、低次から高次への予測誤差の伝搬というFeedforward信号はどのように導かれるのだろうか。結論から言えば、これは\sphinxstylestrong{復元誤差 (reconstruction error)の最小化を行う再帰的ネットワーク (recurrent network)を考慮することで自然に導かれる}。


\subsubsection{損失関数と学習則}
\label{\detokenize{11-3_predictive-coding-rao:id2}}

\paragraph{損失関数の設定}
\label{\detokenize{11-3_predictive-coding-rao:id3}}
前節では2層までのパラメータを最適化することを考えました、高次の活動も考慮して損失関数 \(E\)を次のように再定義する。
\begin{equation*}
\begin{split}
\begin{align}
E=\underbrace{\frac{1}{\sigma^{2}}\|\boldsymbol{I}-f(U \boldsymbol{r})\|^2+\frac{1}{\sigma_{t d}^{2}}\left\|\boldsymbol{r}-f(U^h \boldsymbol{r}^h)\right\|^2}_{\text{reconstruction error}}+\underbrace{g(\boldsymbol{r})+g(\boldsymbol{r}^{h})+h(U)+h(U^h)}_{\text{sparsity penalty}}\tag{8}
\end{align}
\end{split}
\end{equation*}

\paragraph{再帰ネットワークの更新則}
\label{\detokenize{11-3_predictive-coding-rao:id4}}
簡単のために\(\boldsymbol{x}:=U\boldsymbol{r}, \boldsymbol{x}^h:=U^h\boldsymbol{r}^h\)とする。
\begin{equation*}
\begin{split}
\begin{align}
\frac{d \boldsymbol{r}}{d t}&=-\frac{k_{1}}{2} \frac{\partial E}{\partial \boldsymbol{r}}=k_{1}\cdot\Bigg(\frac{1}{\sigma^{2}} U^{T}\bigg[\frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}}\odot\underbrace{(\boldsymbol{I}-f(\boldsymbol{x}))}_{\text{bottom-up error}}\bigg]-\frac{1}{\sigma_{t d}^{2}}\underbrace{\left(\boldsymbol{r}-f(\boldsymbol{x}^h)\right)}_{\text{top-down error}}-\frac{1}{2}g'(\boldsymbol{r})\Bigg)\tag{9}\\
\frac{d \boldsymbol{r}^h}{d t}&=-\frac{k_{1}}{2} \frac{\partial E}{\partial \boldsymbol{r}^h}=k_{1}\cdot\Bigg(\frac{1}{\sigma_{t d}^{2}}(U^h)^T\bigg[\frac{\partial f(\boldsymbol{x}^h)}{\partial \boldsymbol{x}^h}\odot\underbrace{\left(\boldsymbol{r}-f(\boldsymbol{x}^h)\right)}_{\text{bottom-up error}}\bigg]-\frac{1}{2}g'(\boldsymbol{r}^h)\Bigg)\tag{10}
\end{align}
\end{split}
\end{equation*}
ただし、\(k_1\)は更新率 (updating rate)である。または、発火率の時定数を\(\tau:=1/k_1\)として、\(k_1\)は発火率の時定数\(\tau\)の逆数であると捉えることもできる。ここで(9)式において、中間表現 \(\boldsymbol{r}\) のダイナミクスはbottom\sphinxhyphen{}up errorとtop\sphinxhyphen{}down errorで記述されている。このようにbottom\sphinxhyphen{}up errorが \(\boldsymbol{r}\) への入力となることは自然に導出される。なお、top\sphinxhyphen{}down errorに関しては高次からの予測 (prediction)の項 \(f(\boldsymbol{x}^h)\)とleaky\sphinxhyphen{}integratorとしての項 \(-\boldsymbol{r}\)に分割することができる。また\(U^T, (U^h)^T\)は重み行列の転置となっており、bottom\sphinxhyphen{}upとtop\sphinxhyphen{}downの投射において対称な重み行列を用いることを意味している。\(-g'(\boldsymbol{r})\)は発火率を抑制してスパースにすることを目的とする項だが、無理やり解釈をすると自己再帰的な抑制と言える。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{PyPlot}
\end{sphinxVerbatim}


\paragraph{画像データの読み込み}
\label{\detokenize{11-3_predictive-coding-rao:id5}}
データは\sphinxurl{http://www.rctn.org/bruno/sparsenet/}からダウンロードできる。\sphinxcode{\sphinxupquote{IMAGES\_RAW.mat}}は10枚の自然画像で、\sphinxcode{\sphinxupquote{IMAGES.mat}}はそれを白色化したものである。\sphinxcode{\sphinxupquote{mat}}ファイルの読み込みには\sphinxhref{https://github.com/JuliaIO/MAT.jl}{MAT.jl}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{MAT}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} datasets from http://www.rctn.org/bruno/sparsenet/}
\PYG{n}{mat\PYGZus{}images} \PYG{o}{=} \PYG{n}{matopen}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}static/datasets/IMAGES.mat}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{imgs} \PYG{o}{=} \PYG{n}{read}\PYG{p}{(}\PYG{n}{mat\PYGZus{}images}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{IMAGES}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{close}\PYG{p}{(}\PYG{n}{mat\PYGZus{}images}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{モデルの定義}
\label{\detokenize{11-3_predictive-coding-rao:id6}}
必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\PYG{n}{using} \PYG{n}{LinearAlgebra}
\PYG{n}{using} \PYG{n}{Random}
\PYG{n}{using} \PYG{n}{Statistics}
\PYG{n}{using} \PYG{n}{ProgressMeter}
\end{sphinxVerbatim}

モデルを定義する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{RBParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{α}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0}
    \PYG{n}{αh}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.05}
    \PYG{n}{var}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0}
    \PYG{n}{vartd}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mi}{10}
    \PYG{n}{inv\PYGZus{}var}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{var}       
    \PYG{n}{inv\PYGZus{}vartd}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{vartd}
    \PYG{n}{k1}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.3} \PYG{c+c1}{\PYGZsh{} k\PYGZus{}1: update rate}
    \PYG{n}{λ}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.02} \PYG{c+c1}{\PYGZsh{} regularization parameter}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{RaoBallard1999Model}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{RBParameter} \PYG{o}{=} \PYG{n}{RBParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{num\PYGZus{}units\PYGZus{}lv0}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16} \PYG{o}{=} \PYG{l+m+mi}{256} \PYG{c+c1}{\PYGZsh{} number of units of level0}
    \PYG{n}{num\PYGZus{}units\PYGZus{}lv1}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16} \PYG{o}{=} \PYG{l+m+mi}{32}
    \PYG{n}{num\PYGZus{}units\PYGZus{}lv2}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16} \PYG{o}{=} \PYG{l+m+mi}{128}
    \PYG{n}{num\PYGZus{}lv1}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16} \PYG{o}{=} \PYG{l+m+mi}{3}
    \PYG{n}{k2}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.2} \PYG{c+c1}{\PYGZsh{} k\PYGZus{}2: learning rate}
    \PYG{n}{r}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Array}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{num\PYGZus{}lv1}\PYG{p}{,} \PYG{n}{num\PYGZus{}units\PYGZus{}lv1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} activity of neurons}
    \PYG{n}{rh}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Array}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{num\PYGZus{}units\PYGZus{}lv2}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} activity of neurons}
    \PYG{n}{U}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Array}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}units\PYGZus{}lv0}\PYG{p}{,} \PYG{n}{num\PYGZus{}units\PYGZus{}lv1}\PYG{p}{)} \PYG{o}{.}\PYG{o}{*} \PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mf}{2.0} \PYG{o}{/} \PYG{p}{(}\PYG{n}{num\PYGZus{}units\PYGZus{}lv0}\PYG{o}{+}\PYG{n}{num\PYGZus{}units\PYGZus{}lv1}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{Uh}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Array}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}lv1}\PYG{o}{*}\PYG{n}{num\PYGZus{}units\PYGZus{}lv1}\PYG{p}{,} \PYG{n}{num\PYGZus{}units\PYGZus{}lv2}\PYG{p}{)} \PYG{o}{.}\PYG{o}{*} \PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mf}{2.0} \PYG{o}{/} \PYG{p}{(}\PYG{n}{num\PYGZus{}lv1}\PYG{o}{*}\PYG{n}{num\PYGZus{}units\PYGZus{}lv1}\PYG{o}{+}\PYG{n}{num\PYGZus{}units\PYGZus{}lv2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

パラメータを更新する関数を定義する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function update!(variable::RaoBallard1999Model, param::RBParameter, inputs::Array, training::Bool)
    @unpack num\PYGZus{}units\PYGZus{}lv0, num\PYGZus{}units\PYGZus{}lv1, num\PYGZus{}units\PYGZus{}lv2, num\PYGZus{}lv1, k2, r, rh, U, Uh = variable
    @unpack α, αh, var, vartd, inv\PYGZus{}var, inv\PYGZus{}vartd, k1, λ = param

    r\PYGZus{}reshaped = r[:] \PYGZsh{} (96)

    fx = r * U\PYGZsq{} \PYGZsh{} (3, 256)
    fxh = Uh * rh \PYGZsh{} (96, )

    \PYGZsh{} Calculate errors
    error = inputs \PYGZhy{} fx \PYGZsh{} (3, 256)
    errorh = r\PYGZus{}reshaped \PYGZhy{} fxh \PYGZsh{} (96, ) 
    errorh\PYGZus{}reshaped = reshape(errorh, (num\PYGZus{}lv1, num\PYGZus{}units\PYGZus{}lv1)) \PYGZsh{} (3, 32)

    g\PYGZus{}r = α * r ./ (1.0 .+ r .\PYGZca{} 2) \PYGZsh{} (3, 32)
    g\PYGZus{}rh = αh * rh ./ (1.0 .+ rh .\PYGZca{} 2) \PYGZsh{} (64, )

    \PYGZsh{} Update r and rh
    dr = k1 * (inv\PYGZus{}var * error * U \PYGZhy{} inv\PYGZus{}vartd * errorh\PYGZus{}reshaped \PYGZhy{} g\PYGZus{}r)
    drh = k1 * (inv\PYGZus{}vartd * Uh\PYGZsq{} * errorh \PYGZhy{} g\PYGZus{}rh)
    
    r[:, :] += dr
    rh[:] += drh
    
    if training 
        U[:, :] += k2 * (inv\PYGZus{}var * error\PYGZsq{} * r \PYGZhy{} num\PYGZus{}lv1 * λ * U)
        Uh[:, :] += k2 * (inv\PYGZus{}vartd * errorh * rh\PYGZsq{} \PYGZhy{} λ * Uh)
    end

    return error, errorh, dr, drh
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
update! (generic function with 1 method)
\end{sphinxVerbatim}

入力に乗じるGaussianフィルタを定義する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Gaussian mask for inputs}
\PYG{n}{function} \PYG{n}{GaussianMask}\PYG{p}{(}\PYG{n}{sizex}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{n}{sizey}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{:}\PYG{n}{sizex}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}
    \PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{:}\PYG{n}{sizey}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}
    \PYG{n}{X} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{x}\PYG{p}{,} \PYG{n}{j} \PYG{o+ow}{in} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{length}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{]}
    \PYG{n}{Y} \PYG{o}{=} \PYG{p}{[}\PYG{n}{j} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{length}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{,} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n}{y}\PYG{p}{]}
    
    \PYG{n}{x0} \PYG{o}{=} \PYG{p}{(}\PYG{n}{sizex}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{/} \PYG{l+m+mi}{2}
    \PYG{n}{y0} \PYG{o}{=} \PYG{p}{(}\PYG{n}{sizey}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{/} \PYG{l+m+mi}{2}
    \PYG{n}{mask} \PYG{o}{=} \PYG{n}{exp}\PYG{o}{.}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{p}{(}\PYG{n}{X} \PYG{o}{.}\PYG{o}{\PYGZhy{}} \PYG{n}{x0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{p}{(}\PYG{n}{Y} \PYG{o}{.}\PYG{o}{\PYGZhy{}} \PYG{n}{y0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{l+m+mf}{2.0}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sigma}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{mask} \PYG{o}{.}\PYG{o}{/} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{mask}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
GaussianMask (generic function with 4 methods)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{gau} \PYG{o}{=} \PYG{n}{GaussianMask}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gaussian mask}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{gau}\PYG{p}{)}
\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{11-3_predictive-coding-rao_15_0}.png}

損失関数を定義する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{function} \PYG{n}{calculate\PYGZus{}total\PYGZus{}error}\PYG{p}{(}\PYG{n}{error}\PYG{p}{,} \PYG{n}{errorh}\PYG{p}{,} \PYG{n}{variable}\PYG{p}{:}\PYG{p}{:}\PYG{n}{RaoBallard1999Model}\PYG{p}{,} \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{RBParameter}\PYG{p}{)}
    \PYG{n+nd}{@unpack} \PYG{n}{num\PYGZus{}units\PYGZus{}lv0}\PYG{p}{,} \PYG{n}{num\PYGZus{}units\PYGZus{}lv1}\PYG{p}{,} \PYG{n}{num\PYGZus{}units\PYGZus{}lv2}\PYG{p}{,} \PYG{n}{num\PYGZus{}lv1}\PYG{p}{,} \PYG{n}{k2}\PYG{p}{,} \PYG{n}{r}\PYG{p}{,} \PYG{n}{rh}\PYG{p}{,} \PYG{n}{U}\PYG{p}{,} \PYG{n}{Uh} \PYG{o}{=} \PYG{n}{variable}
    \PYG{n+nd}{@unpack} \PYG{n}{α}\PYG{p}{,} \PYG{n}{αh}\PYG{p}{,} \PYG{n}{var}\PYG{p}{,} \PYG{n}{vartd}\PYG{p}{,} \PYG{n}{inv\PYGZus{}var}\PYG{p}{,} \PYG{n}{inv\PYGZus{}vartd}\PYG{p}{,} \PYG{n}{k1}\PYG{p}{,} \PYG{n}{λ} \PYG{o}{=} \PYG{n}{param}
    \PYG{n}{recon\PYGZus{}error} \PYG{o}{=} \PYG{n}{inv\PYGZus{}var} \PYG{o}{*} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{error}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{+} \PYG{n}{inv\PYGZus{}vartd} \PYG{o}{*} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{errorh}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{sparsity\PYGZus{}r} \PYG{o}{=} \PYG{n}{α} \PYG{o}{*} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{r}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{+} \PYG{n}{αh} \PYG{o}{*} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{rh}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{sparsity\PYGZus{}U} \PYG{o}{=} \PYG{n}{λ} \PYG{o}{*} \PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{U}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{+} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{Uh}\PYG{o}{.}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{recon\PYGZus{}error} \PYG{o}{+} \PYG{n}{sparsity\PYGZus{}r} \PYG{o}{+} \PYG{n}{sparsity\PYGZus{}U}
\PYG{n}{end}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
calculate\PYGZus{}total\PYGZus{}error (generic function with 1 method)
\end{sphinxVerbatim}

シミュレーションを実行する関数を定義する。外側の\sphinxcode{\sphinxupquote{for loop}}では画像パッチの作成と\sphinxcode{\sphinxupquote{r}}の初期化を行う。内側の\sphinxcode{\sphinxupquote{for loop}}では\sphinxcode{\sphinxupquote{r}}が収束するまで更新を行い、収束したときに重み行列\sphinxcode{\sphinxupquote{Phi}}を更新する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function run\PYGZus{}simulation(imgs, num\PYGZus{}iter, nt\PYGZus{}max, eps)
    \PYGZsh{} Define model
    model = RaoBallard1999Model\PYGZob{}Float32\PYGZcb{}()
    
    \PYGZsh{} Simulation constants
    H, W, num\PYGZus{}images = size(imgs)
    input\PYGZus{}scale = 40 \PYGZsh{} scale factor of inputs
    gmask = GaussianMask() \PYGZsh{} Gaussian mask
    errorarr = zeros(num\PYGZus{}iter) \PYGZsh{} Vector to save errors    
    
    \PYGZsh{} Run simulation
    @showprogress \PYGZdq{}Computing...\PYGZdq{} for iter in 1:num\PYGZus{}iter
        \PYGZsh{} Get images randomly
        idx = rand(1:num\PYGZus{}images)
        img = imgs[:, :, idx]

        \PYGZsh{} Get the coordinates of the upper left corner of clopping image randomly.
        beginx = rand(1:W\PYGZhy{}27)
        beginy = rand(1:H\PYGZhy{}17)
        img\PYGZus{}clopped = img[beginy:beginy+15, beginx:beginx+25]

        \PYGZsh{} Clop three patches
        inputs = hcat([(gmask .* img\PYGZus{}clopped[:, 1+i*5:i*5+16])[:] for i = 0:2]...)\PYGZsq{}
        inputs = (inputs .\PYGZhy{} mean(inputs)) .* input\PYGZus{}scale

        \PYGZsh{} Reset states
        model.r = inputs * model.U 
        model.rh = model.Uh\PYGZsq{} * model.r[:]

        \PYGZsh{} Input an image patch until latent variables are converged 
        for i in 1:nt\PYGZus{}max
            \PYGZsh{} Update r and rh without update weights 
            error, errorh, dr, drh = update!(model, model.param, inputs, false)

            \PYGZsh{} Compute norm of r and rh
            dr\PYGZus{}norm = sqrt(sum(dr.\PYGZca{}2))
            drh\PYGZus{}norm = sqrt(sum(drh.\PYGZca{}2))

            \PYGZsh{} Check convergence of r and rh, then update weights
            if dr\PYGZus{}norm \PYGZlt{} eps \PYGZam{}\PYGZam{} drh\PYGZus{}norm \PYGZlt{} eps
                error, errorh, dr, drh = update!(model, model.param, inputs, true)
                errorarr[iter] = calculate\PYGZus{}total\PYGZus{}error(error, errorh, model, model.param) \PYGZsh{} Append errors
                break
            end

            \PYGZsh{} If failure to convergence, break and print error
            if i \PYGZgt{}= nt\PYGZus{}max\PYGZhy{}2
                println(\PYGZdq{}Error at patch:\PYGZdq{}, iter)
                println(dr\PYGZus{}norm, drh\PYGZus{}norm)
                break
            end
        end


        \PYGZsh{} Decay learning rate         
        if iter \PYGZpc{} 40 == 39
            model.k2 /= 1.015
        end

        \PYGZsh{} Print moving average error
        if iter \PYGZpc{} 1000 == 0
            moving\PYGZus{}average\PYGZus{}error = mean(errorarr[iter\PYGZhy{}999:iter])
            println(\PYGZdq{}iter: \PYGZdq{}, iter, \PYGZdq{}/\PYGZdq{}, num\PYGZus{}iter, \PYGZdq{}, Moving average error:\PYGZdq{}, moving\PYGZus{}average\PYGZus{}error)
        end
    end
    return model, errorarr
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
run\PYGZus{}simulation (generic function with 1 method)
\end{sphinxVerbatim}


\paragraph{シミュレーションの実行}
\label{\detokenize{11-3_predictive-coding-rao:id7}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Simulation constants}
\PYG{n}{num\PYGZus{}iter} \PYG{o}{=} \PYG{l+m+mi}{5000} \PYG{c+c1}{\PYGZsh{} number of iterations}
\PYG{n}{nt\PYGZus{}max} \PYG{o}{=} \PYG{l+m+mi}{1000} \PYG{c+c1}{\PYGZsh{} Maximum number of simulation time}
\PYG{n}{eps} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}3} \PYG{c+c1}{\PYGZsh{} small value which determines convergence}

\PYG{n}{model}\PYG{p}{,} \PYG{n}{errorarr} \PYG{o}{=} \PYG{n}{run\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{imgs}\PYG{p}{,} \PYG{n}{num\PYGZus{}iter}\PYG{p}{,} \PYG{n}{nt\PYGZus{}max}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{訓練中の損失の描画}
\label{\detokenize{11-3_predictive-coding-rao:id8}}
訓練中の損失の変化を描画してみよう。損失が低下し、学習が進行したことが分かる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{function} \PYG{n}{moving\PYGZus{}average}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
    \PYG{n}{ret} \PYG{o}{=} \PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{ret}\PYG{p}{[}\PYG{n}{n}\PYG{p}{:}\PYG{n}{end}\PYG{p}{]} \PYG{o}{=} \PYG{n}{ret}\PYG{p}{[}\PYG{n}{n}\PYG{p}{:}\PYG{n}{end}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{ret}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{end}\PYG{o}{\PYGZhy{}}\PYG{n}{n}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{k}{return} \PYG{n}{ret}\PYG{p}{[}\PYG{n}{n} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{end}\PYG{p}{]} \PYG{o}{/} \PYG{n}{n}
\PYG{n}{end}

\PYG{c+c1}{\PYGZsh{} Plot error}
\PYG{n}{moving\PYGZus{}average\PYGZus{}error} \PYG{o}{=} \PYG{n}{moving\PYGZus{}average}\PYG{p}{(}\PYG{n}{errorarr}\PYG{p}{)}
\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moving error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Iterations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{size}\PYG{p}{(}\PYG{n}{moving\PYGZus{}average\PYGZus{}error}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{moving\PYGZus{}average\PYGZus{}error}\PYG{p}{)}
\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{11-3_predictive-coding-rao_23_0}.png}


\paragraph{重み行列 (受容野)の描画}
\label{\detokenize{11-3_predictive-coding-rao:id9}}
学習後の重み行列 \sphinxcode{\sphinxupquote{Phi}} (\(\Phi\))を可視化してみよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot Receptive fields}
\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{subplots\PYGZus{}adjust}\PYG{p}{(}\PYG{n}{hspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{wspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{32}
    \PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}
    \PYG{n}{imshow}\PYG{p}{(}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{U}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{16}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gray}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{end}
\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Receptive fields}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{)}
\PYG{n}{subplots\PYGZus{}adjust}\PYG{p}{(}\PYG{n}{top}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{11-3_predictive-coding-rao_25_0}.png}

白色が\sphinxstylestrong{ON領域}(興奮)、黒色が\sphinxstylestrong{OFF領域}(抑制)を表す。Gaborフィルタ様の局所受容野が得られており、これは一次視覚野(V1)における単純型細胞(simple cells)の受容野に類似している。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot Receptive fields of level 2}
\PYG{n}{zero\PYGZus{}padding} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{80}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{)}
\PYG{n}{U0} \PYG{o}{=} \PYG{p}{[}\PYG{n}{model}\PYG{o}{.}\PYG{n}{U}\PYG{p}{;} \PYG{n}{zero\PYGZus{}padding}\PYG{p}{;} \PYG{n}{zero\PYGZus{}padding}\PYG{p}{]}
\PYG{n}{U1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{zero\PYGZus{}padding}\PYG{p}{;} \PYG{n}{model}\PYG{o}{.}\PYG{n}{U}\PYG{p}{;} \PYG{n}{zero\PYGZus{}padding}\PYG{p}{]}
\PYG{n}{U2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{zero\PYGZus{}padding}\PYG{p}{;} \PYG{n}{zero\PYGZus{}padding}\PYG{p}{;} \PYG{n}{model}\PYG{o}{.}\PYG{n}{U}\PYG{p}{]}
\PYG{n}{U\PYGZus{}} \PYG{o}{=} \PYG{p}{[}\PYG{n}{U0} \PYG{n}{U1} \PYG{n}{U2}\PYG{p}{]}
\PYG{n}{Uh\PYGZus{}} \PYG{o}{=} \PYG{n}{U\PYGZus{}} \PYG{o}{*} \PYG{n}{model}\PYG{o}{.}\PYG{n}{Uh} 

\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{subplots\PYGZus{}adjust}\PYG{p}{(}\PYG{n}{hspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{wspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{24}
    \PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}
    \PYG{n}{imshow}\PYG{p}{(}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{Uh\PYGZus{}}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{26}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gray}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{end}

\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Receptive fields of level 2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{)}
\PYG{n}{subplots\PYGZus{}adjust}\PYG{p}{(}\PYG{n}{top}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{11-3_predictive-coding-rao_27_0}.png}


\section{付録}
\label{\detokenize{appendix_intro:id1}}\label{\detokenize{appendix_intro::doc}}

\subsection{まえがき}
\label{\detokenize{preface:id1}}\label{\detokenize{preface::doc}}
何故このサイトを作ろうと思ったか、というと鬱憤が溜まっていたためである。\sphinxstylestrong{物理シミュレーションの本はあるのに神経科学のシミュレーションの本は何故無いのか？} もちろん、洋書を探せば何冊かあるが、ブラックボックス的なライブラリを用いたり、幅広い分野の内容を扱っていなかったりする。数理モデルを扱う洋書・和書の名著は多数あるが、実装まで記述してくれない本がほとんどである。そのくせ、関連分野のニューラルネットワークの本は腐るほど出版されている (というと著者の方々に大変失礼だが)。有名な論文でも再現するためのパラメータの記載が不足していたり、著者実装がなかったり、あっても古いMATLAB実装は動かない。シミュレーションを実施する講義を受けたいと思っても医学部のカリキュラムで試験と被る。潜りも難しい。所属している研究室は理論や数値計算がメインではないので研究室の「秘伝のタレ」があるわけでもない。Twitterで流れてくる講義の情報を見ても「どうして資料を公開してくれないんだ」と嘆きながら枕を濡らすしかない。

「本が無いなら本を書けばいいじゃない」という言葉は…今作った言葉だが、一先ずこれまでに自分が書いてきたコードやノート (\sphinxhref{https://salad-bowl-of-knowledge.github.io/hp/}{ブログ}、\sphinxhref{https://compneuro-julia.github.io/\#spiking-neural-networks}{同人誌 (SNN本)}などを含む)をまとめるだけでも良い資料になるのではないか、という中々思い上がった考えからこのサイトを始めた (目次を見るとかなり無謀なことが察せられる。何事も勢いで初めてはいけない)。このサイトの元となったSNN本が「SNNを如何に学習させるか」ということに重きを置いた同人誌であったため、このサイトでも「学習」に重きを置く予定である。

目標とするのは\sphinxstylestrong{幅広い分野を取り扱い、数式や理論も書いていて、実装を掲載しているサイト}である。実装も線形代数の演算は許容するが、できるだけブラックボックスな演算を無くし、「手を動かして学ぶ」を基本とする。また、可読性が高く、高速に実行出来て研究にも使える実装が望ましい (もちろん、現時点でこれが実現できているとは思わない)。

問題はサイトで使用するプログラミング言語であった。一体何を使えばいいだろうか？神経科学ではMATLABがよく使われているし、関連分野の機械学習ではPythonがよく使われる。ただ、MATLABはライセンスが必要だし、Pythonが数値計算に向いているとは思えない。高速に実行したいし、これらのユーザーが簡単に移行できる言語を考えたとき、\sphinxhref{https://julialang.org/}{Julia}が望ましいという結論に至った。

\sphinxhref{https://www.geidai.ac.jp/~marui/julialang/why\_we\_created\_julia/index.html}{欲張りな言語であるJulia}は欲張りなこのサイトの理念に合致している。Juliaはまだ発展途上ではあるが、ユーザーも増えており、このサイトが一通り完成するころ (3～5年後?) にはかなり良い環境が整っているはずである。

サイトの作成に関しては\sphinxhref{https://jupyterbook.org/intro.html}{Jupyter book}を用いている。Jupyter bookはJupyter notebookやmarkdownをちゃんとしたサイトの形式に変換してくれ、大変便利である。

内容に関しての疑問点や指摘に関しては、各記事末尾のコメント欄からお願いしたい (\sphinxhref{https://utteranc.es/}{utterances.es}を用いているためGitHubアカウントが必要である)。他に、もしこのサイトに協力してくれるという方がいれば、Twitter(\sphinxhref{https://twitter.com/tak\_yamm}{@tak\_yamm})でDMをしていただければ幸いである。

なお、知り合いの方向けに言っておくと、このサイトはあくまで趣味であって、勉強や研究や仕事の方を優先するので安心してほしい。などという言葉を試験3日前に書いている私であった。

2020年7月24日山本 拓都


\subsection{1.2 記号の表記}
\label{\detokenize{notation:id1}}\label{\detokenize{notation::doc}}
このサイトは次のような記号表記を用いている。
\begin{itemize}
\item {} 
実数全体を\(\mathbb{R}\), 複素数全体は\(\mathbb{C}\)と表記する。

\item {} 
スカラーは小文字・斜体で \(x\) のように表記する。

\item {} 
ベクトルは小文字・立体・太字で \(\mathbf{x}\) のように表記し、列ベクトル (縦ベクトル) として扱う。

\item {} 
行列やテンソルは大文字・立体・太字で \(\mathbf{X}\) のように表記する。

\item {} 
\(n\times 1\)の実ベクトルの集合を \(\mathbb{R}^n\), \(n\times m\) の実行列の集合を \(\mathbb{R}^{n\times m}\)と表記する。

\item {} 
行列 \(\mathbf{X}\) の置換は \(\mathbf{X}^T\)と表記する。ベクトルの要素を表す場合は \(\mathbf{x} = (x_1, x_2,\cdots, x_n)^T\)のように表記する。

\item {} 
単位行列を \(\mathbf{I}\) と表記する。

\item {} 
ゼロベクトルは \(\mathbf{0}\) , 要素が全て1のベクトルは \(\mathbf{1}\) と表記する。

\item {} 
\(e\)を自然対数の底とし、指数関数を \(e^x=\exp(x)\)と表記する。また、自然対数を \(\ln(x)\)と表記する。

\item {} 
平均 \(\mu\), 標準偏差 \(\sigma\) の正規分布を \(\mathcal{N}(\mu, \sigma^2)\) と表記する。

\end{itemize}


\subsection{2.2 Hodgkin\sphinxhyphen{}Huxleyモデル}
\label{\detokenize{2-2_hodgkinhuxley:hodgkin-huxley}}\label{\detokenize{2-2_hodgkinhuxley::doc}}

\subsubsection{2.2.1 Hodgkin\sphinxhyphen{}Huxleyモデルにおける膜の等価回路モデル}
\label{\detokenize{2-2_hodgkinhuxley:id1}}
\sphinxstylestrong{Hodgkin\sphinxhyphen{}Huxleyモデル} (HH モデル)は, A.L. HodgkinとA.F. Huxleyによって1952年に考案されたニューロンの膜興奮を表すモデルである (\sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/}{Hodgkin \& Huxley, 1952})。Hodgkinらはヤリイカの巨大神経軸索に対する\sphinxstylestrong{電位固定法}(voltage\sphinxhyphen{}clamp)を用いた実験を行い, 実験から得られた観測結果を元にモデルを構築した。

HHモデルには等価な電気回路モデルがあり, \sphinxstylestrong{膜の並列等価回路モデル} (parallel conductance model)と呼ばれている。膜の並列等価回路モデルでは, ニューロンの細胞膜をコンデンサ, 細胞膜に埋まっているイオンチャネルを可変抵抗 (動的に変化する抵抗) として置き換える。

\sphinxstylestrong{イオンチャネル} (ion channel)は特定のイオン(例えばナトリウムイオンやカリウムイオンなど)を選択的に通す膜輸送体の一種である。それぞれのイオンの種類において, 異なるイオンチャネルがある (同じイオンでも複数の種類のイオンチャネルがある)。また, イオンチャネルにはイオンの種類に応じて異なる\sphinxstylestrong{コンダクタンス}(抵抗の逆数で電流の「流れやすさ」を意味する)と\sphinxstylestrong{平衡電位}(equilibrium potential)がある。HHモデルでは, ナトリウム(Na\(^{+}\))チャネル, カリウム(K\(^{+}\))チャネル, 漏れ電流(leak current)のイオンチャネルを仮定する。漏れ電流のイオンチャネルは当時特定できなかったチャネルで, 膜から電流が漏れ出すチャネルを意味する。なお, 現在では漏れ電流の多くはCl\(^{-}\)イオン(chloride ion)によることが分かっている。

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{parallel_conductance_model}.JPG}
\caption{Hodgkin\sphinxhyphen{}Huxleyモデルの膜の等価回路モデル}\label{\detokenize{2-2_hodgkinhuxley:parallel-conductance-model}}\end{figure}

それでは, 等価回路モデルを用いて電位変化の式を立ててみよう。上図において, \(C_m\)は細胞膜のキャパシタンス(膜容量), \(I_{m}(t)\)は細胞膜を流れる電流(外部からの入力電流), \(I_\text{Cap}(t)\)は膜のコンデンサを流れる電流, \(I_\text{Na}(t)\)及び \(I_K(t)\)はそれぞれナトリウムチャネルとカリウムチャネルを通って膜から流出する電流, \(I_\text{L}(t)\)は漏れ電流である。このとき,
\begin{equation*}
\begin{split}
I_{m}(t)=I_\text{Cap}(t)+I_\text{Na}(t)+I_\text{K}(t)+I_\text{L}(t)    
\end{split}
\end{equation*}
という仮定をしている。

膜電位を\(V(t)\)とすると, Kirchhoffの第二法則 (Kirchhoff’s Voltage Law)より,
\begin{equation*}
\begin{split}
\underbrace{C_m\frac {dV(t)}{dt}}_{I_\text{Cap} (t)}=I_{m}(t)-I_\text{Na}(t)-I_\text{K}(t)-I_\text{L}(t)
\end{split}
\end{equation*}
となる。Hodgkinらはチャネル電流\(I_\text{Na}, I_K, I_\text{L}\)が従う式を実験的に求めた。
\begin{equation*}
\begin{split}
\begin{aligned}
I_\text{Na}(t) &= g_{\text{Na}}\cdot m^{3}h(V-E_{\text{Na}})\\
I_\text{K}(t) &= g_{\text{K}}\cdot n^{4}(V-E_{\text{K}})\\
I_\text{L}(t) &= g_{\text{L}}(V-E_{\text{L}})
\end{aligned}
\end{split}
\end{equation*}
ただし, \(g_{\text{Na}}, g_{\text{K}}\)はそれぞれNa\(^+\), K\(^+\)の最大コンダクタンスである。\(g_{\text{L}}\)はオームの法則に従うコンダクタンスで, Lコンダクタンスは時間的に変化はしないと仮定する。また, \(m\)はNa\(^+\)コンダクタンスの活性化パラメータ, \(h\)はNa\(^+\)コンダクタンスの不活性化パラメータ, \(n\)はK\(^+\)コンダクタンスの活性化パラメータであり, ゲートの開閉確率を表している。よって, HHモデルの状態は\(V, m, h, n\)の4変数で表される。これらの変数は以下の\(x\)を\(m, n, h\)に置き換えた3つの微分方程式に従う。
\begin{equation*}
\begin{split}
\frac{dx}{dt}=\alpha_{x}(V)(1-x)-\beta_{x}(V)x
\end{split}
\end{equation*}
ただし, \(V\)の関数である\(\alpha_{x}(V),\ \beta_{x}(V)\)は\(m, h, n\)によって異なり, 次の6つの式に従う。
\begin{equation*}
\begin{split}
\begin{array}{ll}
\alpha_{m}(V)=\dfrac {0.1(25-V)}{\exp \left[(25-V)/10\right]-1}, &\beta_{m}(V)=4\exp {(-V/18)}\\
\alpha_{h}(V)=0.07\exp {(-V/20)}, & \beta_{h}(V)={\dfrac{1}{\exp {\left[(30-V)/10 \right]}+1}}\\
\alpha_{n}(V)={\dfrac {0.01(10-V)}{\exp {\left[(10-V)/10\right]}-1}},& \beta_{n}(V)=0.125\exp {(-V/80)} 
\end{array}
\end{split}
\end{equation*}
なお、この式は6.3℃の条件下においてイカの巨大軸索の活動から得たデータを用いて導かれたものであることに注意しよう。


\subsubsection{2.2.2 Hodgkin\sphinxhyphen{}Huxley モデルの定義}
\label{\detokenize{2-2_hodgkinhuxley:id2}}
これまでに説明した式を用いてHHモデルを実装する。まず必要なパッケージを読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Base}\PYG{p}{:} \PYG{n+nd}{@kwdef}
\PYG{n}{using} \PYG{n}{Parameters}\PYG{p}{:} \PYG{n+nd}{@unpack} \PYG{c+c1}{\PYGZsh{} or using UnPack}
\end{sphinxVerbatim}

変更しない定数を保持する \sphinxcode{\sphinxupquote{struct}} の \sphinxcode{\sphinxupquote{HHParameter}} と, 変数を保持する \sphinxcode{\sphinxupquote{mutable struct}} の \sphinxcode{\sphinxupquote{HH}} を作成する。\sphinxcode{\sphinxupquote{v, m, h, n}} はHH modelの4変数だが, \sphinxcode{\sphinxupquote{r}} はpre\sphinxhyphen{}synaptic dynamicsを表す変数である。詳細は3章で解説する。 定数は次のように設定する。

\textbackslash{}begin\{align*\}
C\_m=1.0, g\_\{\textbackslash{}text\{Na\}\}=120, g\_\{\textbackslash{}text\{K\}\}=36, g\_\{\textbackslash{}text\{L\}\}=0.3\textbackslash{}
E\_\{\textbackslash{}text\{Na\}\}=50.0, E\_\{\textbackslash{}text\{K\}\}=\sphinxhyphen{}77, E\_\{\textbackslash{}text\{L\}\}=\sphinxhyphen{}54.387
\textbackslash{}end\{align*\}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@kwdef} \PYG{n}{struct} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{Cm}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{c+c1}{\PYGZsh{} 膜容量(uF/cm\PYGZca{}2)}
    \PYG{n}{gNa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{120.0} \PYG{c+c1}{\PYGZsh{} Na+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{36.0} \PYG{c+c1}{\PYGZsh{} K+ の最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{gL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.3} \PYG{c+c1}{\PYGZsh{} 漏れイオンの最大コンダクタンス(mS/cm\PYGZca{}2)}
    \PYG{n}{ENa}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{50.0} \PYG{c+c1}{\PYGZsh{} Na+ の平衡電位(mV)}
    \PYG{n}{EK}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{77.0} \PYG{c+c1}{\PYGZsh{} K+ の平衡電位(mV)}
    \PYG{n}{EL}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{54.387} \PYG{c+c1}{\PYGZsh{}漏れイオンの平衡電位(mV)}
    \PYG{n}{tr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{0.5} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{td}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{8.0} \PYG{c+c1}{\PYGZsh{} ms}
    \PYG{n}{invtr}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{tr}
    \PYG{n}{invtd}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{l+m+mf}{1.0} \PYG{o}{/} \PYG{n}{td}
    \PYG{n}{v0}\PYG{p}{:}\PYG{p}{:}\PYG{n}{FT} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{20.0} \PYG{c+c1}{\PYGZsh{} mV}
\PYG{n}{end}

\PYG{n+nd}{@kwdef} \PYG{n}{mutable} \PYG{n}{struct} \PYG{n}{HH}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}
    \PYG{n}{param}\PYG{p}{:}\PYG{p}{:}\PYG{n}{HHParameter} \PYG{o}{=} \PYG{n}{HHParameter}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{N}\PYG{p}{:}\PYG{p}{:}\PYG{n}{UInt16}
    \PYG{n}{v}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{65.0}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{m}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{h}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{n}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{fill}\PYG{p}{(}\PYG{l+m+mf}{0.32}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}
    \PYG{n}{r}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Vector}\PYG{p}{\PYGZob{}}\PYG{n}{FT}\PYG{p}{\PYGZcb{}} \PYG{o}{=} \PYG{n}{zeros}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}
\PYG{n}{end}
\end{sphinxVerbatim}

次に変数を更新する関数\sphinxcode{\sphinxupquote{update!}}を書く。ソルバーとしては陽的Euler法または4次のRunge\sphinxhyphen{}Kutta法を用いる。以下ではEuler法を用いている。Juliaではforループを用いて1つのニューロンごとにパラメータを更新する方がベクトルを用いるよりも高速である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function update!(variable::HH, param::HHParameter, I::Vector, dt)
    @unpack N, v, m, h, n, r = variable
    @unpack Cm, gNa, gK, gL, ENa, EK, EL, tr, td, invtr, invtd, v0 = param
    @inbounds for i = 1:N
        m[i] += dt * ((0.1(v[i]+40.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+40.0))))*(1.0 \PYGZhy{} m[i]) \PYGZhy{} 4.0exp(\PYGZhy{}(v[i]+65.0) / 18.0)*m[i])
        h[i] += dt * ((0.07exp(\PYGZhy{}0.05(v[i]+65.0)))*(1.0 \PYGZhy{} h[i]) \PYGZhy{} 1.0/(1.0 + exp(\PYGZhy{}0.1(v[i]+35.0)))*h[i])
        n[i] += dt * ((0.01(v[i]+55.0)/(1.0 \PYGZhy{} exp(\PYGZhy{}0.1(v[i]+55.0))))*(1.0 \PYGZhy{} n[i]) \PYGZhy{} (0.125exp(\PYGZhy{}0.0125(v[i]+65)))*n[i])
        v[i] += dt / Cm * (I[i] \PYGZhy{} gNa * m[i]\PYGZca{}3 * h[i] * (v[i] \PYGZhy{} ENa) \PYGZhy{} gK * n[i]\PYGZca{}4 * (v[i] \PYGZhy{} EK) \PYGZhy{} gL * (v[i] \PYGZhy{} EL))
        r[i] += dt * ((invtr \PYGZhy{} invtd) * (1.0 \PYGZhy{} r[i])/(1.0 + exp(\PYGZhy{}v[i] + v0)) \PYGZhy{} r[i] * invtd)
    end
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
update! (generic function with 1 method)
\end{sphinxVerbatim}


\subsubsection{2.2.3 Hodgkin\sphinxhyphen{}Huxleyモデルのシミュレーションの実行}
\label{\detokenize{2-2_hodgkinhuxley:id3}}
いくつかの定数を設定してシミュレーションを実行する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * ((t .\PYGZgt{} 50) \PYGZhy{} (t .\PYGZgt{} 200)) + 35f0 * ((t .\PYGZgt{} 250) \PYGZhy{} (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} 記録用
varr = zeros(Float32, nt, N)
gatearr = zeros(Float32, nt, 3, N)

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr[i, :] = neurons.v
    gatearr[i, 1, :] = neurons.m
    gatearr[i, 2, :] = neurons.h
    gatearr[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.217901 seconds (1.36 M allocations: 46.979 MiB, 3.69\PYGZpc{} gc time)
\end{sphinxVerbatim}

結果を表示するために \sphinxcode{\sphinxupquote{Plots}}を読み込む。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{using} \PYG{n}{Plots}
\end{sphinxVerbatim}

ニューロンの膜電位 \sphinxcode{\sphinxupquote{v}}, ゲート変数 \sphinxcode{\sphinxupquote{m, h, n}}, 刺激電流 \sphinxcode{\sphinxupquote{I}}の描画をする。入力電流の単位は \(\mu\text{A/cm}^2\)である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

次項で用いるために発火回数を求める。\sphinxcode{\sphinxupquote{bitwise and}}を用いると楽である。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Num. of spikes : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}spikes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Num. of spikes : 27
\end{sphinxVerbatim}

50msから200msまでで11回, 250msから400msまでで16回発火しているので発火回数は計27回であり、この結果は正しい。


\subsubsection{2.2.4 Frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve}
\label{\detokenize{2-2_hodgkinhuxley:frequency-current-f-i-curve}}
この項ではHodgkin\sphinxhyphen{}Huxleyモデルにおいて、入力電流に対する発火率がどのように変化するかを調べる。次のコードのように入力電流を徐々に増加させたときの発火率を見てみよう。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 1000 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps

N = 100 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
maxcurrent = 30
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = Array\PYGZob{}Float32\PYGZcb{}(range(1,maxcurrent,length=N)) \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr\PYGZus{}fi = zeros(Float32, nt, N)

\PYGZsh{} simulation
for i = 1:nt
    update!(neurons, neurons.param, I[:], dt)
    varr\PYGZus{}fi[i, :] = neurons.v
end
\end{sphinxVerbatim}

発火率を計算して結果を描画する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spike} \PYG{o}{=} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{nt}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{.}\PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{varr\PYGZus{}fi}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{nt}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{.}\PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{num\PYGZus{}spikes} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{spike}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{rate} \PYG{o}{=} \PYG{n}{num\PYGZus{}spikes}\PYG{o}{/}\PYG{n}{T}\PYG{o}{*}\PYG{l+m+mf}{1e3}

\PYG{n}{plot}\PYG{p}{(}\PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{rate}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Input current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Firing rate (Hz)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n}{false}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{400}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

このような曲線を\sphinxstylestrong{frequency\sphinxhyphen{}current (F\sphinxhyphen{}I) curve} (または neuronal input/output (I/O) curve)と呼ぶ。


\subsubsection{2.2.5  抑制後リバウンド (Postinhibitory rebound; PIR)}
\label{\detokenize{2-2_hodgkinhuxley:postinhibitory-rebound-pir}}
ニューロンは電流が流入することで膜電位が変化し, 膜電位がある一定の閾値を超えると発火が起こる, というのはニューロンの活動電位発生についての典型的な説明である。それではHHモデルの膜電位閾値はどのくらいの値になるのだろうか。答えは「\sphinxstylestrong{膜電位閾値は一定ではない}」である。それを示す現象として \sphinxstylestrong{抑制後リバウンド} (Postinhibitory rebound; PIR)がある。この時生じる発火を\sphinxstylestrong{リバウンド発火} (rebound spikes)
と呼ぶ。抑制後リバウンドは過分極性の電流の印加を止めた際に膜電位が静止膜電位に回復するのみならず, さらに脱分極をして発火をするという現象である。この現象が生じる要因として
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{アノーダルブレイク} (anodal break, またはanode break excitation; ABE)

\item {} 
遅いT型カルシウム電流 (slow T\sphinxhyphen{}type calcium current)

\end{enumerate}

がある (\sphinxhref{https://pubmed.ncbi.nlm.nih.gov/15324089/}{Chik et al., 2004})。HH モデルはこのうちアノーダルブレイクを再現できるため, シミュレーションによりどのような現象か確認してみよう。これは入力電流を変更するだけで行える。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
T = 450 \PYGZsh{} ms
dt = 0.01f0 \PYGZsh{} ms
nt = UInt32(T/dt) \PYGZsh{} number of timesteps
N = 1 \PYGZsh{} ニューロンの数

\PYGZsh{} 入力刺激
t = Array\PYGZob{}Float32\PYGZcb{}(1:nt)*dt
I = repeat(10f0 * (\PYGZhy{}(t .\PYGZgt{} 50) + (t .\PYGZgt{} 200)) + 20f0 * (\PYGZhy{}(t .\PYGZgt{} 250) + (t .\PYGZgt{} 400)), 1, N)  \PYGZsh{} injection current

\PYGZsh{} modelの定義
neurons = HH\PYGZob{}Float32\PYGZcb{}(N=N)

\PYGZsh{} 記録用
varr2 = zeros(Float32, nt, N)
gatearr2 = zeros(Float32, nt, 3, N)

\PYGZsh{} simulation
@time for i = 1:nt
    update!(neurons, neurons.param, I[i, :], dt)
    varr2[i, :] = neurons.v
    gatearr2[i, 1, :] = neurons.m
    gatearr2[i, 2, :] = neurons.h
    gatearr2[i, 3, :] = neurons.n
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  0.074358 seconds (848.89 k allocations: 19.134 MiB)
\end{sphinxVerbatim}

結果は次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p1} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{varr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p2} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{gatearr2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{h}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{p3} \PYG{o}{=} \PYG{n}{plot}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n}{I}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} 
    \PYG{n}{xlabel} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Times (ms)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
    \PYG{n}{ylabel}\PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{V (mV)}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gating Value}\PYG{l+s+s2}{\PYGZdq{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Injection}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ current}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{layout} \PYG{o}{=} \PYG{n}{grid}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{heights}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.25}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{,}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

なぜこのようなことが起こるか, というと過分極の状態から静止膜電位へと戻る際にNa\(^+\)チャネルが活性化 (Na\(^+\)チャネルの活性化パラメータ\(m\)が増加し, 不活性化パラメータ\(h\)が減少)し, 膜電位が脱分極することで再度Na\(^+\)チャネルが活性化する, というポジティブフィードバック過程(\sphinxstylestrong{自己再生的過程})に突入するためである (もちろん, この過程は通常の活動電位発生のメカニズムである)。 この際, 発火に必要な閾値が膜電位の低下に応じて下がった, ということもできる。

このように膜電位閾値は一定ではない。しかし, この後の節で紹介するLIFモデルなどでは簡略化のためにif文を用い, 膜電位閾値を超えたから発火, というものもある。実際には違うということを頭の片隅に残しながら読み進めることを推奨する。

\begin{sphinxadmonition}{note}{Note:}
PIRに関連する現象として抑制後促通 (Postinhibitory facilitation; PIF)がある。これは抑制入力の後に興奮入力がある一定の時間内で入ると発火が起こるという現象である (\sphinxhref{http://www.brain.riken.jp/en/summer/prev/2006/files/j\_rinzel04.pdf}{Dolda et al., 2006}, \sphinxhref{https://link.springer.com/referenceworkentry/10.1007\%2F978-1-4614-7320-6\_152-1}{Dodla, 2014})。
\end{sphinxadmonition}


\subsection{3.1 シナプス伝達}
\label{\detokenize{3-1_synapse:id1}}\label{\detokenize{3-1_synapse::doc}}
\begin{sphinxadmonition}{note}{Note:}
内容は大幅に追記が必要。
\end{sphinxadmonition}

スパイクが生じたことによる膜電位変化は軸索を伝播し, \sphinxstylestrong{シナプス}という構造により, 次のニューロンへと興奮が伝わる. このときの伝達の仕組みとして, シナプスには\sphinxstylestrong{化学シナプス}(chemical synapse)とGap junctionによる\sphinxstylestrong{電気シナプス}(electrical synapse)がある。

化学シナプスの場合, シナプス前膜からの\sphinxstylestrong{神経伝達物質}の放出, シナプス後膜の受容体への神経伝達物質の結合, イオンチャネル開口による\sphinxstylestrong{シナプス後電流}(postsynaptic current; PSC)の発生, という過程が起こる。かなり簡略化して書いたが, 実際にはかなりの過程を含くむ。しかし, これらの過程を全てモデル化するのは計算量がかなり大きくなるので, 基本的には簡易的な現象論的なモデルを用いる。

このように, シナプス前細胞のスパイク列(spike train)は次のニューロンにそのまま伝わるのではなく, ある種の時間的フィルターをかけられて伝わる。このフィルターを\sphinxstylestrong{シナプスフィルター}(synaptic filter)と呼ぶ。3章では, このようにシナプス前細胞で生じた発火が, シナプス後細胞の膜電位に与える過程のモデルについて説明する。


\subsection{11.1 ベイズ統計の基礎}
\label{\detokenize{11-1_bayes_statistics:id1}}\label{\detokenize{11-1_bayes_statistics::doc}}
この節では本章で用いるベイズ統計の基礎的概念の説明を行う。

\begin{sphinxadmonition}{note}{Note:}
悪いこと言わないので\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/index-j.html}{渡辺澄夫先生のHP}の講義録、特に\sphinxhref{http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/joho-gakushu6.html}{ベイズ統計入門}を読もう。
\end{sphinxadmonition}


\subsection{JuliaのTips集}
\label{\detokenize{tips:juliatips}}\label{\detokenize{tips::doc}}
このページはJuliaでの実装におけるTips (詰まったところの解決策)をまとめたものである。体系的にまとまってはいない。


\subsubsection{1. 関数名の!記号}
\label{\detokenize{tips:id1}}
単なる\sphinxstylestrong{慣習}として関数への入力を変更する場合に!を付ける。

関数内で配列を変更する場合には注意が必要である。以下に入力された配列を同じサイズの要素1の配列で置き換える、ということを目的として書かれた2つの関数がある。違いは\sphinxcode{\sphinxupquote{v}}の後に\sphinxcode{\sphinxupquote{{[}:{]}}}としているかどうかである。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function wrong!(A::Array)
    a = ones(size(a))
end

function right!(a::Array)
    a[:] = ones(size(a))
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
right! (generic function with 2 methods)
\end{sphinxVerbatim}

実行すると\sphinxcode{\sphinxupquote{wrong!}}の場合には入力された配列が変更されていないことがわかる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
using Random
v = rand(2, 2)
print(\PYGZdq{}v : \PYGZdq{}, v)

wrong!(v)
print(\PYGZdq{}\PYGZbs{}nwrong : \PYGZdq{}, v)

right!(v)
print(\PYGZdq{}\PYGZbs{}nright : \PYGZdq{}, v)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
v : [0.42692589476552056 0.34730711983824536; 0.9208034639794132 0.41699915079158134]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+ne}{UndefVarError}: wrong! not defined

\PYG{n+ne}{Stacktrace}:
 \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{n}{top}\PYG{o}{\PYGZhy{}}\PYG{n}{level} \PYG{n}{scope} \PYG{n}{at} \PYG{n}{In}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}\PYG{l+m+mi}{5}
 \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{n}{include\PYGZus{}string}\PYG{p}{(}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Function}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{Module}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{)} \PYG{n}{at} \PYG{o}{.}\PYGZbs{}\PYG{n}{loading}\PYG{o}{.}\PYG{n}{jl}\PYG{p}{:}\PYG{l+m+mi}{1091}
\end{sphinxVerbatim}


\subsubsection{2. 配列の1次元化}
\label{\detokenize{tips:id2}}
配列を一次元化(flatten)する方法。まずは3次元配列を作成する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2×2 Array\PYGZob{}Float64,3\PYGZcb{}:
[:, :, 1] =
 0.997107  0.0827965
 0.394606  0.847174

[:, :, 2] =
 0.0305417  0.448929
 0.270553   0.601419
\end{sphinxVerbatim}

用意されている\sphinxcode{\sphinxupquote{flatten}}を素直に用いると次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{Base}\PYG{n+nn}{.}\PYG{n+nn}{Iterators}\PYG{p}{:} \PYG{n}{flatten}
\PYG{n}{collect}\PYG{p}{(}\PYG{n}{flatten}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}

ただし、単に\sphinxcode{\sphinxupquote{B{[}:{]}}}とするだけでもよい。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}


\subsubsection{3. 行列の行・列ごとの正規化}
\label{\detokenize{tips:id3}}
シミュレーションにおいてニューロン間の重み行列を行あるいは列ごとに正規化 (weight normalization)する場合がある。これは各ニューロンへの入力の大きさを同じにする働きや重みの発散を防ぐ役割がある。以下では行ごとの和を1にする。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.166781  0.0552745  0.973622
 0.100336  0.753766   0.370005
 0.378222  0.165159   0.471515
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Wnormed} \PYG{o}{=} \PYG{n}{W} \PYG{o}{.}\PYG{o}{/} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{W}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.258439  0.0567384  0.536389
 0.155479  0.773729   0.203843
 0.586082  0.169533   0.259767
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{Wnormed}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1.0 1.0 1.0]
\end{sphinxVerbatim}


\subsubsection{4. 行列の結合 (concatenate)}
\label{\detokenize{tips:concatenate}}
行列の結合はMATLABに近い形式で行うことができる。まず、2つの行列A, Bを用意する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{l+m+mi}{3} \PYG{l+m+mi}{4}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{4} \PYG{l+m+mi}{5} \PYG{l+m+mi}{6}\PYG{p}{;} \PYG{l+m+mi}{7} \PYG{l+m+mi}{8} \PYG{l+m+mi}{9}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×3 Array\PYGZob{}Int64,2\PYGZcb{}:
 4  5  6
 7  8  9
\end{sphinxVerbatim}


\paragraph{4.1 水平結合 (Horizontal concatenation)}
\label{\detokenize{tips:horizontal-concatenation}}
\sphinxcode{\sphinxupquote{hcat}}を使うやり方と、\sphinxcode{\sphinxupquote{{[} {]}}}を使うやり方がある。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H1} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,}\PYG{n}{B}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

なお、MATLABのように次のようにすると正しく結合はされない。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H3} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{,} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2\PYGZhy{}element Array\PYGZob{}Array\PYGZob{}Int64,2\PYGZcb{},1\PYGZcb{}:
 [1 2; 3 4]
 [4 5 6; 7 8 9]
\end{sphinxVerbatim}


\paragraph{4.2 垂直結合 (Vertical concatenation)}
\label{\detokenize{tips:vertical-concatenation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V1} \PYG{o}{=} \PYG{n}{vcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{V2} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;}\PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×4 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  1  2
 3  4  3  4
 4  7  4  7
 5  8  5  8
 6  9  6  9
\end{sphinxVerbatim}


\subsubsection{5. 配列に新しい軸を追加}
\label{\detokenize{tips:id4}}
要はnumpyでの\sphinxcode{\sphinxupquote{A{[}None, :{]}}}や\sphinxcode{\sphinxupquote{A{[}np.newaxis, :{]}}}のようなことがしたい場合。やや面倒だが、\sphinxcode{\sphinxupquote{reshape}}を使うか、\sphinxcode{\sphinxupquote{{[}CartesianIndex(){]}}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.3487808667518093
 0.717776723811032
 0.3369916406749496
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{newaxis} \PYG{o}{=} \PYG{p}{[}\PYG{n}{CartesianIndex}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{v1} \PYG{o}{=} \PYG{n}{v}\PYG{p}{[}\PYG{n}{newaxis}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.348781  0.717777  0.336992
\end{sphinxVerbatim}


\subsubsection{6. Array\{Array\{Float64, x\},1\}をArray\{Float64, x+1\}に変換}
\label{\detokenize{tips:array-array-float64-x-1-array-float64-x-1}}
numpyでは\sphinxcode{\sphinxupquote{array({[}matrix for i in range(){]})}}などを用いると、1次元配列のリストを2次元配列に変換できた。Juliaでも同様にする場合は\sphinxcode{\sphinxupquote{hcat(...)}}や\sphinxcode{\sphinxupquote{cat(...)}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,1\PYGZcb{},1\PYGZcb{}
Size : (5,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A2} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : LinearAlgebra.Adjoint\PYGZob{}Float64,Array\PYGZob{}Float64,2\PYGZcb{}\PYGZcb{}
Size : (5, 3)
\end{sphinxVerbatim}

以下は多次元配列の場合。\sphinxcode{\sphinxupquote{cat(...)}}で配列を結合し、\sphinxcode{\sphinxupquote{permitedims}}で転置する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,3\PYGZcb{},1\PYGZcb{}
Size : (6,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B2} \PYG{o}{=} \PYG{n}{permutedims}\PYG{p}{(}\PYG{n}{cat}\PYG{p}{(}\PYG{n}{B1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Float64,4\PYGZcb{}
Size : (6, 3, 4, 5)
\end{sphinxVerbatim}


\subsection{JuliaのTips集}
\label{\detokenize{tips:juliatips}}\label{\detokenize{tips::doc}}
このページはJuliaでの実装におけるTips (詰まったところの解決策)をまとめたものである。体系的にまとまってはいない。


\subsubsection{1. 関数名の!記号}
\label{\detokenize{tips:id1}}
単なる\sphinxstylestrong{慣習}として関数への入力を変更する場合に!を付ける。

関数内で配列を変更する場合には注意が必要である。以下に入力された配列を同じサイズの要素1の配列で置き換える、ということを目的として書かれた2つの関数がある。違いは\sphinxcode{\sphinxupquote{v}}の後に\sphinxcode{\sphinxupquote{{[}:{]}}}としているかどうかである。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
function wrong!(A::Array)
    a = ones(size(a))
end

function right!(a::Array)
    a[:] = ones(size(a))
end
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
right! (generic function with 2 methods)
\end{sphinxVerbatim}

実行すると\sphinxcode{\sphinxupquote{wrong!}}の場合には入力された配列が変更されていないことがわかる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
using Random
v = rand(2, 2)
print(\PYGZdq{}v : \PYGZdq{}, v)

wrong!(v)
print(\PYGZdq{}\PYGZbs{}nwrong : \PYGZdq{}, v)

right!(v)
print(\PYGZdq{}\PYGZbs{}nright : \PYGZdq{}, v)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
v : [0.42692589476552056 0.34730711983824536; 0.9208034639794132 0.41699915079158134]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+ne}{UndefVarError}: wrong! not defined

\PYG{n+ne}{Stacktrace}:
 \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{n}{top}\PYG{o}{\PYGZhy{}}\PYG{n}{level} \PYG{n}{scope} \PYG{n}{at} \PYG{n}{In}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}\PYG{l+m+mi}{5}
 \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{n}{include\PYGZus{}string}\PYG{p}{(}\PYG{p}{:}\PYG{p}{:}\PYG{n}{Function}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{Module}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{,} \PYG{p}{:}\PYG{p}{:}\PYG{n}{String}\PYG{p}{)} \PYG{n}{at} \PYG{o}{.}\PYGZbs{}\PYG{n}{loading}\PYG{o}{.}\PYG{n}{jl}\PYG{p}{:}\PYG{l+m+mi}{1091}
\end{sphinxVerbatim}


\subsubsection{2. 配列の1次元化}
\label{\detokenize{tips:id2}}
配列を一次元化(flatten)する方法。まずは3次元配列を作成する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2×2 Array\PYGZob{}Float64,3\PYGZcb{}:
[:, :, 1] =
 0.997107  0.0827965
 0.394606  0.847174

[:, :, 2] =
 0.0305417  0.448929
 0.270553   0.601419
\end{sphinxVerbatim}

用意されている\sphinxcode{\sphinxupquote{flatten}}を素直に用いると次のようになる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{Base}\PYG{n+nn}{.}\PYG{n+nn}{Iterators}\PYG{p}{:} \PYG{n}{flatten}
\PYG{n}{collect}\PYG{p}{(}\PYG{n}{flatten}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}

ただし、単に\sphinxcode{\sphinxupquote{B{[}:{]}}}とするだけでもよい。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B}\PYG{p}{[}\PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.25323335359382226
 0.8475284309218816
 0.46170218524594486
 0.3908851101558619
 0.2049406129688014
 0.20068478557126213
 0.29072826846396627
 0.5193322837086369
\end{sphinxVerbatim}


\subsubsection{3. 行列の行・列ごとの正規化}
\label{\detokenize{tips:id3}}
シミュレーションにおいてニューロン間の重み行列を行あるいは列ごとに正規化 (weight normalization)する場合がある。これは各ニューロンへの入力の大きさを同じにする働きや重みの発散を防ぐ役割がある。以下では行ごとの和を1にする。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.166781  0.0552745  0.973622
 0.100336  0.753766   0.370005
 0.378222  0.165159   0.471515
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Wnormed} \PYG{o}{=} \PYG{n}{W} \PYG{o}{.}\PYG{o}{/} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{W}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.258439  0.0567384  0.536389
 0.155479  0.773729   0.203843
 0.586082  0.169533   0.259767
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{Wnormed}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1.0 1.0 1.0]
\end{sphinxVerbatim}


\subsubsection{4. 行列の結合 (concatenate)}
\label{\detokenize{tips:concatenate}}
行列の結合はMATLABに近い形式で行うことができる。まず、2つの行列A, Bを用意する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{l+m+mi}{3} \PYG{l+m+mi}{4}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{4} \PYG{l+m+mi}{5} \PYG{l+m+mi}{6}\PYG{p}{;} \PYG{l+m+mi}{7} \PYG{l+m+mi}{8} \PYG{l+m+mi}{9}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×3 Array\PYGZob{}Int64,2\PYGZcb{}:
 4  5  6
 7  8  9
\end{sphinxVerbatim}


\paragraph{4.1 水平結合 (Horizontal concatenation)}
\label{\detokenize{tips:horizontal-concatenation}}
\sphinxcode{\sphinxupquote{hcat}}を使うやり方と、\sphinxcode{\sphinxupquote{{[} {]}}}を使うやり方がある。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H1} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,}\PYG{n}{B}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2×5 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  4  5  6
 3  4  7  8  9
\end{sphinxVerbatim}

なお、MATLABのように次のようにすると正しく結合はされない。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{H3} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{,} \PYG{n}{B}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2\PYGZhy{}element Array\PYGZob{}Array\PYGZob{}Int64,2\PYGZcb{},1\PYGZcb{}:
 [1 2; 3 4]
 [4 5 6; 7 8 9]
\end{sphinxVerbatim}


\paragraph{4.2 垂直結合 (Vertical concatenation)}
\label{\detokenize{tips:vertical-concatenation}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V1} \PYG{o}{=} \PYG{n}{vcat}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V2} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;} \PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×2 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2
 3  4
 4  7
 5  8
 6  9
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{V2} \PYG{p}{[}\PYG{n}{A}\PYG{p}{;}\PYG{l+s+sa}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{]]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
5×4 Array\PYGZob{}Int64,2\PYGZcb{}:
 1  2  1  2
 3  4  3  4
 4  7  4  7
 5  8  5  8
 6  9  6  9
\end{sphinxVerbatim}


\subsubsection{5. 配列に新しい軸を追加}
\label{\detokenize{tips:id4}}
要はnumpyでの\sphinxcode{\sphinxupquote{A{[}None, :{]}}}や\sphinxcode{\sphinxupquote{A{[}np.newaxis, :{]}}}のようなことがしたい場合。やや面倒だが、\sphinxcode{\sphinxupquote{reshape}}を使うか、\sphinxcode{\sphinxupquote{{[}CartesianIndex(){]}}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3\PYGZhy{}element Array\PYGZob{}Float64,1\PYGZcb{}:
 0.3487808667518093
 0.717776723811032
 0.3369916406749496
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{newaxis} \PYG{o}{=} \PYG{p}{[}\PYG{n}{CartesianIndex}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{v1} \PYG{o}{=} \PYG{n}{v}\PYG{p}{[}\PYG{n}{newaxis}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1×3 Array\PYGZob{}Float64,2\PYGZcb{}:
 0.348781  0.717777  0.336992
\end{sphinxVerbatim}


\subsubsection{6. Array\{Array\{Float64, x\},1\}をArray\{Float64, x+1\}に変換}
\label{\detokenize{tips:array-array-float64-x-1-array-float64-x-1}}
numpyでは\sphinxcode{\sphinxupquote{array({[}matrix for i in range(){]})}}などを用いると、1次元配列のリストを2次元配列に変換できた。Juliaでも同様にする場合は\sphinxcode{\sphinxupquote{hcat(...)}}や\sphinxcode{\sphinxupquote{cat(...)}}を用いる。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,1\PYGZcb{},1\PYGZcb{}
Size : (5,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A2} \PYG{o}{=} \PYG{n}{hcat}\PYG{p}{(}\PYG{n}{A1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : LinearAlgebra.Adjoint\PYGZob{}Float64,Array\PYGZob{}Float64,2\PYGZcb{}\PYGZcb{}
Size : (5, 3)
\end{sphinxVerbatim}

以下は多次元配列の場合。\sphinxcode{\sphinxupquote{cat(...)}}で配列を結合し、\sphinxcode{\sphinxupquote{permitedims}}で転置する。

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{i}\PYG{o}{*}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Array\PYGZob{}Float64,3\PYGZcb{},1\PYGZcb{}
Size : (6,)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B2} \PYG{o}{=} \PYG{n}{permutedims}\PYG{p}{(}\PYG{n}{cat}\PYG{p}{(}\PYG{n}{B1}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{n}{dims}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{typeof}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{println}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Size : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{p}{(}\PYG{n}{B2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Type : Array\PYGZob{}Float64,4\PYGZcb{}
Size : (6, 3, 4, 5)
\end{sphinxVerbatim}


\subsection{有用なリンク集}
\label{\detokenize{useful_links:id1}}\label{\detokenize{useful_links::doc}}
この説では計算論的神経科学を学ぶのに有用なWeb上の資料のリンクをまとめた。随時追加。


\subsubsection{書籍}
\label{\detokenize{useful_links:id2}}\begin{itemize}
\item {} 
\sphinxhref{http://www.gatsby.ucl.ac.uk/~dayan/book/}{Theoretical Neuroscience}：理論神経科学の教科書の中で一番有名な本。pdfとexercises (MATLAB)などが公開されている。

\item {} 
\sphinxhref{https://neuronaldynamics.epfl.ch/index.html}{Neuronal Dynamics}：こちらも計算論的神経科学の教科書。書籍の内容がすべてブラウザで見れるようになっている。Pythonのコードも少しついている。また、\sphinxhref{https://www.classcentral.com/course/edx-neuronal-dynamics-2685}{edXの講義}もある。

\item {} 
\sphinxhref{https://groups.oist.jp/sites/default/files/imce/u194/Books/Doya2007icns.pdf}{「計算神経科学への招待」脳の学習機構の理解を目指して}：OISTの\sphinxhref{https://groups.oist.jp/ja/ncu}{銅谷先生}の書かれた「数理科学」の別冊のpdf版（銅谷先生が公開されている）

\end{itemize}


\subsubsection{講義資料}
\label{\detokenize{useful_links:id3}}\begin{itemize}
\item {} 
\sphinxhref{https://warwick.ac.uk/fac/sci/systemsbiology/staff/richardson/teaching/ma4g4/}{Introduction to Theoretical Neuroscience}：M. Richardson先生の講義資料

\item {} 
\sphinxhref{https://www.cns.nyu.edu/~david/}{David Heeger}：D. Heeger先生の講義資料（ホームページの”Teaching”を参照）

\end{itemize}


\subsubsection{オンライン講義}
\label{\detokenize{useful_links:id4}}\begin{itemize}
\item {} 
\sphinxhref{https://www.coursera.org/learn/computational-neuroscience/}{Computational Neuroscience}：Courseraの講義。

\item {} 
\sphinxhref{https://www.classcentral.com/course/edx-computational-neuroscience-neuronal-dynamics-of-cognition-104230}{Computational Neuroscience: Neuronal Dynamics of Cognition}：edXの講義。

\end{itemize}


\subsubsection{ワークショップの資料}
\label{\detokenize{useful_links:id5}}\begin{itemize}
\item {} 
\sphinxhref{https://github.com/NeuromatchAcademy/course-content/tree/master/tutorials}{Neuromatch Academy Tutorial}：Pythonのnotebookと講義動画が大変よくまとまっている (このサイトを作るのを辞めようと思ったほど)。

\item {} 
\sphinxhref{https://github.com/RainerEngelken/neurotheory-seminar-2019}{neurotheory\sphinxhyphen{}seminar\sphinxhyphen{}2019}：一部、スライドとコードが公開されている。

\end{itemize}


\subsubsection{その他}
\label{\detokenize{useful_links:id6}}\begin{itemize}
\item {} 
\sphinxhref{https://www.insightsfromthebrain.com/}{Insights from the brain: The road towards Machine Intelligence}：神経科学と人工知能の関連についてまとまった資料。

\end{itemize}







\renewcommand{\indexname}{Index}
\printindex
\end{document}