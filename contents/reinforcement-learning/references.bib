---
---

@ARTICLE{Dabney2020-uj,
  title     = "A distributional code for value in dopamine-based reinforcement
               learning",
  author    = "Dabney, Will and Kurth-Nelson, Zeb and Uchida, Naoshige and
               Starkweather, Clara Kwon and Hassabis, Demis and Munos, RÃ©mi and
               Botvinick, Matthew",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  577,
  number    =  7792,
  pages     = "671--675",
  abstract  = "Since its introduction, the reward prediction error theory of
               dopamine has explained a wealth of empirical phenomena, providing
               a unifying framework for understanding the representation of
               reward and value in the brain1-3. According to the now canonical
               theory, reward predictions are represented as a single scalar
               quantity, which supports learning about the expectation, or mean,
               of stochastic outcomes. Here we propose an account of
               dopamine-based reinforcement learning inspired by recent
               artificial intelligence research on distributional reinforcement
               learning4-6. We hypothesized that the brain represents possible
               future rewards not as a single mean, but instead as a probability
               distribution, effectively representing multiple future outcomes
               simultaneously and in parallel. This idea implies a set of
               empirical predictions, which we tested using single-unit
               recordings from mouse ventral tegmental area. Our findings
               provide strong evidence for a neural realization of
               distributional reinforcement learning.",
  month     =  jan,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Jensen2023-lo,
  title         = "An introduction to reinforcement learning for neuroscience",
  author        = "Jensen, Kristopher T",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Reinforcement learning has a rich history in neuroscience,
                   from early work on dopamine as a reward prediction error
                   signal for temporal difference learning (Schultz et al.,
                   1997) to recent work suggesting that dopamine could implement
                   a form of 'distributional reinforcement learning' popularized
                   in deep learning (Dabney et al., 2020). Throughout this
                   literature, there has been a tight link between theoretical
                   advances in reinforcement learning and neuroscientific
                   experiments and findings. As a result, the theories
                   describing our experimental data have become increasingly
                   complex and difficult to navigate. In this review, we cover
                   the basic theory underlying classical work in reinforcement
                   learning and build up to an introductory overview of methods
                   in modern deep reinforcement learning that have found
                   applications in systems neuroscience. We start with an
                   overview of the reinforcement learning problem and classical
                   temporal difference algorithms, followed by a discussion of
                   'model-free' and 'model-based' reinforcement learning
                   together with methods such as DYNA and successor
                   representations that fall in between these two extremes.
                   Throughout these sections, we highlight the close parallels
                   between such machine learning methods and related work in
                   both experimental and theoretical neuroscience. We then
                   provide an introduction to deep reinforcement learning with
                   examples of how these methods have been used to model
                   different learning phenomena in systems neuroscience, such as
                   meta-reinforcement learning (Wang et al., 2018) and
                   distributional reinforcement learning (Dabney et al., 2020).
                   Code that implements the methods discussed in this work and
                   generates the figures is also provided.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC"
}

@ARTICLE{Subramanian2020-tx,
  title         = "Reinforcement Learning and its Connections with Neuroscience
                   and Psychology",
  author        = "Subramanian, Ajay and Chitlangia, Sharad and Baths, Veeky",
  journal       = "arXiv [cs.LG]",
  abstract      = "Reinforcement learning methods have recently been very
                   successful at performing complex sequential tasks like
                   playing Atari games, Go and Poker. These algorithms have
                   outperformed humans in several tasks by learning from
                   scratch, using only scalar rewards obtained through
                   interaction with their environment. While there certainly has
                   been considerable independent innovation to produce such
                   results, many core ideas in reinforcement learning are
                   inspired by phenomena in animal learning, psychology and
                   neuroscience. In this paper, we comprehensively review a
                   large number of findings in both neuroscience and psychology
                   that evidence reinforcement learning as a promising candidate
                   for modeling learning and decision making in the brain. In
                   doing so, we construct a mapping between various classes of
                   modern RL algorithms and specific findings in both
                   neurophysiological and behavioral literature. We then discuss
                   the implications of this observed relationship between RL,
                   neuroscience and psychology and its role in advancing
                   research in both AI and brain science.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Lowet2020-hi,
  title     = "Distributional reinforcement learning in the brain",
  author    = "Lowet, Adam S and Zheng, Qiao and Matias, Sara and Drugowitsch,
               Jan and Uchida, Naoshige",
  journal   = "Trends Neurosci.",
  publisher = "Elsevier BV",
  volume    =  43,
  number    =  12,
  pages     = "980--997",
  abstract  = "Learning about rewards and punishments is critical for survival.
               Classical studies have demonstrated an impressive correspondence
               between the firing of dopamine neurons in the mammalian midbrain
               and the reward prediction errors of reinforcement learning
               algorithms, which express the difference between actual reward
               and predicted mean reward. However, it may be advantageous to
               learn not only the mean but also the complete distribution of
               potential rewards. Recent advances in machine learning have
               revealed a biologically plausible set of algorithms for
               reconstructing this reward distribution from experience. Here, we
               review the mathematical foundations of these algorithms as well
               as initial evidence for their neurobiological implementation. We
               conclude by highlighting outstanding questions regarding the
               circuit computation and behavioral readout of these
               distributional codes.",
  month     =  dec,
  year      =  2020,
  keywords  = "artificial intelligence; deep neural networks; dopamine; machine
               learning; population coding; reward",
  language  = "en"
}

@ARTICLE{Matteucci2023-na,
  title    = "Unsupervised learning of mid-level visual representations",
  author   = "Matteucci, Giulio and Piasini, Eugenio and Zoccolan, Davide",
  journal  = "Curr. Opin. Neurobiol.",
  volume   =  84,
  pages    =  102834,
  abstract = "Recently, a confluence between trends in neuroscience and machine
              learning has brought a renewed focus on unsupervised learning,
              where sensory processing systems learn to exploit the statistical
              structure of their inputs in the absence of explicit training
              targets or rewards. Sophisticated experimental approaches have
              enabled the investigation of the influence of sensory experience
              on neural self-organization and its synaptic bases. Meanwhile,
              novel algorithms for unsupervised and self-supervised learning
              have become increasingly popular both as inspiration for theories
              of the brain, particularly for the function of intermediate visual
              cortical areas, and as building blocks of real-world learning
              machines. Here we review some of these recent developments,
              placing them in historical context and highlighting some research
              lines that promise exciting breakthroughs in the near future.",
  month    =  dec,
  year     =  2023,
  language = "en"
}

@article{rescorla1972theory,
  title={A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and non-reinforcement},
  author={Rescorla, Robert A},
  journal={Classical conditioning, Current research and theory},
  volume={2},
  pages={64--69},
  year={1972},
  publisher={Appleton-Century-Crofts}
}
