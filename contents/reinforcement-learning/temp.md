#### 環境とエージェントの定義
環境 (environment) とは，エージェント (agent) が相互作用する対象であり，エージェントの行動によってその状態が変化するものである．一方，エージェントは環境内で行動を選択し，学習を行う主体（例えば生物やロボットなど）を指す．  
エージェントと環境の状態 (state) を $s \in \mathcal{S}$，エージェントの行動 (action) を $a \in \mathcal{A}$ とする．ここで，$\mathcal{S}$ および $\mathcal{A}$ は，それぞれ環境のあらゆる可能な状態の集合，およびエージェントが選択できる行動の集合である．状態や行動は離散的または連続的であり得る．

例えば，グリッド状の迷路を探索する場合，環境は迷路全体を指し，状態集合 $\mathcal{S}$ は迷路内の各セル（位置）からなる．行動集合 $\mathcal{A}$ は，{上, 下, 左, 右} の4つの移動方向からなると考えられる．

#### 報酬と学習の目的
エージェントは環境内で行動を選択し，その結果として**報酬** (reward) を得る．報酬は，エージェントが望ましい行動をとった場合に正の値を持ち，望ましくない行動をとった場合には負の値（罰; punishment）を持つ．  
強化学習の目的は，エージェントが得た報酬に基づいて行動の選択を調整し，**累積報酬の最大化** を図ることである．報酬は，即時に得られることもあれば，長期的な影響を通じて評価されることもある．

#### マルコフ決定過程 (MDP)
エージェントが状態 $s_t$ で行動 $a_t$ をとると，次の状態 $s_{t+1}$ に遷移し，報酬 $r_{t+1}$ を受け取る\footnote{状態 $s_t$ において行動 $a_t$ を行った後に受け取る報酬を $r_t$ と定義する流派もある．}．このとき，状態 $s_{t+1}$ と報酬 $r_{t+1}$ は直前の状態 $s_t$ と行動 $a_t$ のみに依存し，過去の状態や行動の履歴には依存しない場合，この過程は**マルコフ性**を持つ．  
このとき，環境の状態遷移確率は  
$$
p(s_{t+1}, r_{t+1} \mid s_t, a_t)
$$
で表される．この分布は，「状態 $s_t$ で行動 $a_t$ を選択した際に，次の状態が $s_{t+1}$ になり，報酬 $r_{t+1}$ を得る確率」を示している．このような環境の記述を**マルコフ決定過程** (Markov Decision Process, MDP) と呼ぶ．MDP が成立するためには，状態 $s_t$ が環境とエージェントの相互作用に関する十分な情報を持つ必要がある．

#### 部分観測マルコフ決定過程 (POMDP)
エージェントは環境の状態 $s_t$ を直接観測できるとは限らない．エージェントが受け取る情報を観測 (observation) $o_t$ とすると，$o_t = s_t$ の場合は MDP が成立する．しかし，現実の多くの問題では，エージェントは $s_t$ の一部しか観測できないことがある．この場合，環境は**部分観測マルコフ決定過程** (Partially Observable Markov Decision Process, POMDP) で記述される．例えば，ロボットがカメラを用いて環境を観測する場合，センサーの精度や障害物の影響で環境の完全な状態を取得できないことがある．このような状況では，エージェントは観測の不確実性を考慮しながら意思決定を行う必要がある．