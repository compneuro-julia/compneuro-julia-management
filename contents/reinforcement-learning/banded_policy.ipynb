{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6699cbd-9f23-4f88-8c3e-35e335bb843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "# 簡単なバンディット環境\n",
    "struct Bandit\n",
    "    true_probs::Vector{Float64}  # 各アクションの真の報酬確率\n",
    "end\n",
    "\n",
    "function step(bandit::Bandit, action::Int)\n",
    "    reward = rand() < bandit.true_probs[action] ? 1.0 : 0.0\n",
    "    return reward\n",
    "end\n",
    "\n",
    "# 方策ネットワーク（ソフトマックス方策）\n",
    "mutable struct PolicyNetwork\n",
    "    params::Vector{Float64}  # 行動のパラメータ（スコア）\n",
    "end\n",
    "\n",
    "function softmax(logits::Vector{Float64})\n",
    "    exp_logits = exp.(logits .- maximum(logits))\n",
    "    return exp_logits / sum(exp_logits)\n",
    "end\n",
    "\n",
    "function select_action(policy::PolicyNetwork)\n",
    "    probs = softmax(policy.params)\n",
    "    return rand(Categorical(probs)), probs\n",
    "end\n",
    "\n",
    "# REINFORCEアルゴリズム\n",
    "function reinforce()\n",
    "    Random.seed!(42)\n",
    "    bandit = Bandit([0.3, 0.7])  # 2つのアクションの報酬確率\n",
    "    policy = PolicyNetwork([0.0, 0.0])  # 初期パラメータ\n",
    "    learning_rate = 0.1\n",
    "    num_episodes = 1000\n",
    "    \n",
    "    for episode in 1:num_episodes\n",
    "        action, probs = select_action(policy)\n",
    "        reward = step(bandit, action)\n",
    "        \n",
    "        # 方策の勾配更新\n",
    "        policy.params[action] += learning_rate * (reward - 0.5) * (1 - probs[action])\n",
    "        for i in 1:length(policy.params)\n",
    "            if i != action\n",
    "                policy.params[i] -= learning_rate * (reward - 0.5) * probs[i]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return softmax(policy.params)\n",
    "end\n",
    "\n",
    "# 学習後の方策を確認\n",
    "learned_policy = reinforce()\n",
    "println(\"Learned Policy Probabilities: \", learned_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ca967-c402-4c31-9ae6-b5618f9f8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "# 迷路環境\n",
    "mutable struct MazeEnv\n",
    "    grid::Array{Int,2}  # 迷路のマップ (0: 通路, 1: 壁, 2: ゴール)\n",
    "    start::Tuple{Int,Int}  # 開始位置\n",
    "    goal::Tuple{Int,Int}  # ゴール位置\n",
    "    agent_pos::Tuple{Int,Int}  # エージェントの現在位置\n",
    "end\n",
    "\n",
    "function reset!(env::MazeEnv)\n",
    "    env.agent_pos = env.start\n",
    "    return env.agent_pos\n",
    "end\n",
    "\n",
    "function step!(env::MazeEnv, action::Int)\n",
    "    moves = [(0,-1), (0,1), (-1,0), (1,0)]  # 左, 右, 上, 下\n",
    "    new_pos = (env.agent_pos[1] + moves[action][1], env.agent_pos[2] + moves[action][2])\n",
    "    \n",
    "    if env.grid[new_pos...] != 1  # 壁でないなら移動\n",
    "        env.agent_pos = new_pos\n",
    "    end\n",
    "    \n",
    "    reward = env.agent_pos == env.goal ? 1.0 : -0.01\n",
    "    done = env.agent_pos == env.goal\n",
    "    return env.agent_pos, reward, done\n",
    "end\n",
    "\n",
    "# 方策ネットワーク (簡易的なテーブル)\n",
    "mutable struct PolicyNetwork\n",
    "    params::Dict{Tuple{Int,Int}, Vector{Float64}}  # 各状態のアクション確率\n",
    "end\n",
    "\n",
    "function softmax(logits::Vector{Float64})\n",
    "    exp_logits = exp.(logits .- maximum(logits))\n",
    "    return exp_logits / sum(exp_logits)\n",
    "end\n",
    "\n",
    "function select_action(policy::PolicyNetwork, state::Tuple{Int,Int})\n",
    "    probs = softmax(policy.params[state])\n",
    "    return rand(Categorical(probs)), probs\n",
    "end\n",
    "\n",
    "# REINFORCEアルゴリズム\n",
    "function reinforce_maze()\n",
    "    Random.seed!(42)\n",
    "    grid = [0 0 1 0 0; 0 1 1 0 2; 0 0 0 0 1; 1 0 1 0 0; 0 0 0 1 0]\n",
    "    env = MazeEnv(grid, (1,1), (2,5), (1,1))\n",
    "    policy = PolicyNetwork(Dict((i,j) => [0.0, 0.0, 0.0, 0.0] for i in 1:5, j in 1:5 if grid[i,j] != 1))\n",
    "    learning_rate = 0.1\n",
    "    num_episodes = 500\n",
    "    \n",
    "    for episode in 1:num_episodes\n",
    "        state = reset!(env)\n",
    "        trajectory = []\n",
    "        done = false\n",
    "        \n",
    "        while !done\n",
    "            action, probs = select_action(policy, state)\n",
    "            new_state, reward, done = step!(env, action)\n",
    "            push!(trajectory, (state, action, reward, probs))\n",
    "            state = new_state\n",
    "        end\n",
    "        \n",
    "        G = 0\n",
    "        for (state, action, reward, probs) in reverse(trajectory)\n",
    "            G += reward\n",
    "            policy.params[state][action] += learning_rate * G * (1 - probs[action])\n",
    "            for i in 1:4\n",
    "                if i != action\n",
    "                    policy.params[state][i] -= learning_rate * G * probs[i]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return policy\n",
    "end\n",
    "\n",
    "# 学習後の方策を確認\n",
    "learned_policy = reinforce_maze()\n",
    "println(\"Learned Policy: \", learned_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
