\documentclass[titlepage]{ltjsbook}
\usepackage[
  paperheight=232truemm, paperwidth=182truemm,
  top=20truemm, bottom=15truemm, inner=15truemm, outer=15truemm
  ]{geometry}

%\documentclass[tombow, paper={182truemm, 232truemm}, titlepage]{ltjsbook}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{mathrsfs}

\usepackage{textgreek}
\usepackage[luatex]{graphicx} 
\usepackage[svgnames]{xcolor}
\usepackage{sty/julia-syntax-highlighting} % 
\usepackage{sty/indexing} % 

\usepackage[export]{sty/adjustbox} % added

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyfoot{}
\fancyhead[RO, LE]{\thepage}
\fancyhead[LO]{\nouppercase{\leftmark}}
\fancyhead[RE]{\nouppercase{\rightmark}}

%\renewcommand{\chaptermark}[1]{\markboth{#1}{} }
\renewcommand{\chaptermark}[1]{\markboth{第\ \thechapter\ 章. ~#1}{}}
% \renewcommand{\chaptermark}[1]{\markboth{\MakeUppercase{第\chaptername \thechapter 章.\ #1}}{}}
% \renewcommand{\headrulewidth}{0pt}

\usepackage{hyperref}

% https://ja.overleaf.com/learn/latex/Bibliography_management_with_bibtex
\usepackage[
    backend=biber,
    bibencoding=utf8,
    style=authoryear-comp, 
    url=false,
    isbn=true,
    doi=true,
    natbib=true, 
    alldates=year,
    maxcitenames=2,
    uniquelist=false, 
    sorting=nty,
    sortcites=true,
    giveninits=true,
    terseinits=false,
    refsegment=chapter
]{biblatex}

\addbibresource{../references/02_local-learning-rule.bib}
% \addbibresource{bibfiles/appendix-references.bib}
% \addbibresource{bibfiles/bayesian-brain-references.bib}
% \addbibresource{bibfiles/energy-based-model-references.bib}
% \addbibresource{bibfiles/introduction-references.bib}
% \addbibresource{bibfiles/local-learning-rule-references.bib}
% \addbibresource{bibfiles/motor-learning-references.bib}
% \addbibresource{bibfiles/neuron-model-references.bib}
% \addbibresource{bibfiles/neuronal-computation-references.bib}
% \addbibresource{bibfiles/reinforcement-learning-references.bib}
% \addbibresource{bibfiles/solve-credit-assignment-problem-references.bib}
% \addbibresource{bibfiles/synapse-model-references.bib}

\DeclareNameAlias{author}{last-first}
\AtEveryBibitem{\clearlist{language}}
\renewbibmacro{in:}{}

% https://stackoverflow.com/questions/69682457/extended-links-in-citations
% \makeatletter
% \renewbibmacro*{cite}{%
%   \printtext[bibhyperref]{\iffieldundef{shorthand}
%     {\ifthenelse{\ifnameundef{labelname}\OR\iffieldundef{labelyear}}
%        {\usebibmacro{cite:label}%
%         \setunit{\printdelim{nonameyeardelim}}}
%        {\printnames{labelname}%
%         \setunit{\printdelim{nameyeardelim}}}%
%      \usebibmacro{cite:labeldate+extradate}}
%     {\usebibmacro{cite:shorthand}}}}
% \makeatother

\newcommand{\jl}{\lstinline[language=julia]}

\title{\Huge \textbf{Juliaで作って学ぶ計算論的神経科学}}
\author{\huge 山本 拓都}
\date{\huge \today} 

\begin{document}
%\maketitle
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\chapter{発火率モデルと局所学習則}
\section{神経細胞の概要}
\subsection{脳における細胞の種類と数}
脳は膨大な数の細胞によって構成されており，主に神経細胞 (neuron) とグリア細胞 (glial cells)の二種類に分類される．ヒトの脳には約860億個の神経細胞が存在し\footnote{Azevedoらは4人の成人男性の死後脳に対して，等方性分画法 (isotropic fractionator, \citep{herculano2005isotropic}) を用い，神経細胞の数を推定した．等方性分画法ではまず，固定した脳組織を解離および懸濁し，核が同濃度になるように撹拌する．その後，核を染色した後に血球計算盤で核の濃度を数え，濃度に懸濁液の総量をかけて全体の核の数，すなわち細胞数を概算する．懸濁液中の一部を抗NeuN抗体で免疫染色し，NeuNを発現している核の数を数える．そして，NeuN陽性細胞の割合と全体の細胞数から，神経細胞と非神経細胞の数が概算できる．結果として，神経細胞は大脳に約163億個，小脳に約690億個，その他の脳領域 (大脳基底核・間脳・脳幹等) に約69億個，脳全体に約860億個あると推定された \citep{azevedo2009equal}．しかし，この約860億個という数字は絶対的なものではないことに注意が必要である \citep{goriely2024eighty}．推定値の標準偏差は約81億個であり，各被験者間では約730億個から約990億個とばらつきがあった．また神経細胞のマーカーとしてNeuNが使用されたが，NeuN陰性の神経細胞も存在するため，大脳皮質以外の神経細胞数は過小評価されている．}，グリア細胞も同数またはそれ以上の数が含まれると推定されている \citep{azevedo2009equal, von2016search}．これらの細胞は非常に高密度に詰まっており，例えばヒト大脳皮質の一次視覚野には1mm$^3$あたりで約7.9万個の神経細胞が存在する \citep{garcia2024neuronal}．

\subsection{神経細胞の形態と構造}
神経細胞の形態は他の細胞と大きく異なり，細胞体 (soma, cell body)，樹状突起(dendrite)，軸索 (axon) という三つの主要構造からなる．細胞体には細胞核があり，タンパク質合成やエネルギー代謝など基本的な細胞機能が行われる．樹状突起は木の枝のように複雑に分岐した構造で，他の神経細胞からの入力 (シナプス入力) を受け取る部位である．軸索は通常1本の細長い突起であり，細胞体で統合された情報を他の神経細胞へと送る電気信号を伝導する．軸索の起始部には，細胞体との接合部である軸索小丘 (あるいは軸索起始円錐; axon hillock) が存在し，それより遠位の領域は軸索初節 (axon initial segment, AIS)  と呼ばれる．AISにはトリガー帯 (trigger zone)と呼ばれる，電気信号の発生，すなわち活動電位 (詳細は後述) の出発点として重要な部位が存在する．AISには電位依存性ナトリウムチャネルが高密度に存在し，膜電位が閾値を超えると活動電位がここで生成される．

軸索の先端には軸索終末 (nerve terminal) があり，シナプスを形成している膨大部はシナプス前終末あるいはブトン (synaptic bouton) と呼ばれる．ブトンは樹状突起スパイン (dendritic spine) と結合している．

\subsection{神経細胞の電気的活動}
神経細胞は，主に電気的活動によって情報を処理・伝達する．この活動は，細胞膜を挟んだイオンの移動に基づいており，特にイオンチャネルとイオントランスポータの働きが重要である．神経細胞の膜は静止時に内側が負に帯電しており，この状態は主にナトリウム・カリウムポンプ (Na$^+$/K$^+$ ATPase) によって維持される．外部からの入力によって膜電位が上昇し，ある閾値 (ただし一定ではない) を超えると，AISに存在する電位依存性ナトリウムチャネルが開き，ナトリウムイオンの流入によって膜が急激に脱分極する．この過程で生じる電位変化が活動電位 (action potential) あるいは スパイク (spike) と呼ばれる信号であり，軸索を伝導して末端まで到達する．活動電位が発生することを発火 (firing) とも呼ぶ．スパイクの後には，一時的に再発火が困難となる不応期 (refractory period) が存在し，これにより信号の一方向性が保たれ，連続的なスパイクの発生頻度が制御される．

活動電位は最終的にシナプス (synapse) に到達し，次の細胞に情報を伝える．この伝達には大きく分けて二種類のシナプスがある．化学シナプスでは，活動電位の到達によりシナプス小胞が開口放出 (exocytosis) し，内部に蓄えられた神経伝達物質 (neurotransmitter) が細胞間隙に放出される．この物質は次の細胞の受容体 (receptor) に結合し，膜電位を変化させる．膜電位が脱分極方向に変化する場合は興奮性シナプス後電位 (excitatory postsynaptic potential; EPSP)，過分極方向であれば抑制性シナプス後電位 (inhibitory postsynaptic potential; IPSP) と呼ばれる．一方，電気シナプスではギャップ結合を通じてイオン電流が直接隣の細胞に流れ，より高速で同期的な通信が可能である．なお，シナプスで繋がる2つの細胞を，伝達の流れに即してシナプス前細胞 (pre-synaptic cell)およびシナプス後細胞 (post-synaptic cell) と呼ぶ．

\subsection{神経細胞の種類}
神経細胞はその形態や機能，伝達物質の種類により多くのサブタイプ (subtype) に分類されるが，最も基本的な区別は興奮性ニューロン (excitatory neuron) と抑制性ニューロン (inhibitory neuron) である．興奮性ニューロンは主にグルタミン酸 (glutamate) を放出し，標的細胞を脱分極に導いて興奮させる．抑制性ニューロンは主にGABA ($\gamma$-アミノ酪酸, gamma-aminobutyric acid) あるいはグリシン (glycine) を放出し，標的の膜電位を過分極させて抑制する．皮質においては，神経細胞の約80\%が興奮性，約20\%が抑制性とされる．

特に大脳皮質や海馬において，興奮性ニューロンの代表的な形態として知られるのが錐体細胞 (pyramidal neuron) である．錐体細胞は三角形に近い細胞体を持ち，1本の長い太い尖端樹状突起 (apical dendrite) \footnote{尖端樹状突起の先端はいくつもの枝を持ち，房状分枝 (tuft) と呼ばれる．}と複数の基底樹状突起 (basal dendrites) を持つのが特徴である．これにより空間的に広く分布した入力を統合でき，かつ軸索はしばしば長距離にわたって他の皮質領域や皮質下構造に投射する．これらの細胞は大脳皮質では第5層や第3層に多く存在し，皮質内外の広範な情報伝達を担う．皮質回路において中心的な情報出力の担い手として，認知・運動・記憶などの高次機能に不可欠である．

神経細胞の伝達物質の一貫性に関しては，Daleの法則 (Dale's principle) が古くから知られている．この法則は，「一つの神経細胞はその全ての出力部位で同一の神経伝達物質を放出する」という原則である．たとえば錐体細胞はどのシナプスでもグルタミン酸を放出し，同様に抑制性ニューロンであればGABAを一貫して使用する．現在では，補助的な神経ペプチドや共放出物質の存在が知られており，Daleの法則は厳密には修正されているものの，「主たる伝達物質の一貫性」という点では今も有効な原理とされている．

このように，神経細胞はその構造，電気的性質，機能的分類において精緻な多様性と秩序を持ち，脳回路全体の動的バランスと情報処理を支えている．

\subsection{グリア細胞の種類}
神経細胞が情報の受容・統合・出力といった処理の中心を担うのに対し，グリア細胞はかつて単なる支持組織 (糊, glia) として捉えられていたが，現在では神経細胞と並んで神経系の恒常性維持・可塑性制御・免疫応答において不可欠な役割を担う能動的な細胞群であると認識されている．

まずアストロサイト (astrocyte) は，中枢神経系において最も豊富なグリア細胞であり，星状の形態を持つ．アストロサイトは血管と神経細胞の間を仲介し，血液脳関門 (blood-brain barrier; BBB) の形成，イオン濃度の調節，神経伝達物質の再取り込み，さらにシナプスの形成と除去の調整に関与する．神経回路の機能に対して能動的に影響を与える点で，単なる支持細胞という枠を超えた存在である．

次に，オリゴデンドロサイト (oligodendrocyte) は中枢神経系においてミエリン鞘 (myelin sheath) を形成する細胞である．1個のオリゴデンドロサイトは複数の軸索に分岐を伸ばし，それぞれにミエリンを巻き付ける．ミエリンは絶縁体として機能し，跳躍伝導を可能にすることでスパイクの伝導速度を著しく高める．オリゴデンドロサイトは伝達速度の調整も行っており，スパイクタイミングの調節等に寄与している．これに対し，シュワン細胞 (Schwann cell) は末梢神経系 (peripheral nervous system) に存在し，オリゴデンドロサイトと類似の役割を果たす．ただし，シュワン細胞は1つの細胞が1つの軸索の1セグメントのみにミエリンを形成するという点でオリゴデンドロサイトとは異なる．また，シュワン細胞は神経損傷後の再生過程の促進にも関与する．

最後に，ミクログリア (microglia) は中枢神経系内に存在する免疫細胞であり，発生学的には他のグリア細胞とは起源が異なる (造血系由来) ．ミクログリアは脳内の異物の貪食 (ファゴサイトーシス) やアポトーシス細胞の除去を担い，また炎症性サイトカインの分泌を通じて神経炎症応答を調節する．またミクログリアはシナプスの刈り込み (synaptic pruning) にも関与し，神経回路の発達と可塑性にも寄与している．

\section{神経細胞の発火率モデル}
神経細胞は複雑な構造と機能を有する特殊な細胞であるが，その基本的な働きを理解するためには，ある程度抽象化された単純なモデルの導入が有用である．本章から第5章までは，この目的のもとに形式ニューロンや発火率モデルといった抽象的な数理モデルを用いることとし，チャネル動態などを含む詳細な生物物理モデルについては第6章で扱う\footnote{神経細胞のモデル化においては，詳細なモデルから抽象モデルを導出する記述順も考えられるが，本書では，コードのまとまりを考慮して，先に単純なモデルから入り，徐々に生物物理学的に忠実なモデルへと発展させる構成とする．}．本章で取り上げる発火率モデル（firing rate model）は，神経細胞の発火活動を平均的な頻度，すなわち発火率（firing rate）として記述する枠組みであり，この連続的な発火率により情報を表現する形式を発火率による符号化（rate coding）と呼ぶ．

\subsection{静的離散時間モデル}
発火率モデルでは，シナプス前細胞の活動の重みづけ和に基づき，シナプス後細胞の出力が決定される．まず，時間を離散的に扱い，神経細胞の内部状態を考慮しない静的なモデルを導入しよう．$n$ 個のシナプス前細胞の活動をベクトル $\mathbf{x} = [x_1, x_2, \dots, x_n]^\top \in \mathbb{R}^n$，シナプス重みを $\mathbf{w} = [w_1, w_2, \dots, w_n]^\top \in \mathbb{R}^n$，定常項（バイアス項）を $b \in \mathbb{R}$，シナプス後細胞の出力を $y \in \mathbb{R}$ とすると，静的離散時間発火率モデルは以下のように定式化される：

\begin{equation}
y = f(\mathbf{w}^\top \mathbf{x} + b) = f\left(\sum_{i=1}^n w_i x_i + b\right)
\end{equation}

ここで，$f(\cdot)$ は入出力関係を表す関数であり，活性化関数（activation function）または伝達関数（transfer function）と呼ばれる．

この静的離散時間モデルは，McCullochとPittsによって1943年に提案された形式ニューロン（formal neuron, McCulloch–Pitts neuron）に起源を持つ \citep{mcculloch1943logical}．形式ニューロンでは，活性化関数としてHeaviside関数が用いられる．このモデルは出力が0か1のいずれかであり，発火率ではなくスパイクの発火の有無を二値的に表現するものである．したがって，形式ニューロンは「入力の重みづけ和がある閾値 $\theta \ (=-b)$ を超えるかどうか」によって出力を決定する閾値判定器として機能する．

このような形式ニューロンに学習機構を導入したモデルが，1958年にRosenblattによって提案されたパーセプトロン（perceptron）である \citep{rosenblatt1958perceptron}．ただし，Rosenblattは形式ニューロンをただ使用するのではなく，形式ニューロンも含めた神経回路網のモデルを提案した．例えばRosenblattによって単純パーセプトロン (simple perceptron, Mark I perceptron)と呼ばれたモデルは，3つのユニット群（感覚ユニット，連想ユニット，応答ユニット）から構成されていた．感覚ユニットと連想ユニットはランダムなシナプス重みで結合\footnote{入力信号のランダム重みによる投射は第8章で触れるリザバーコンピューティングと同様の形態である．}しており，連想ユニットと応答ユニット間には双方向の結合が存在していた．これに対して，後に提案された簡略化されたモデルでは，連想ユニットと双方向の結合を排除し，感覚ユニットと応答ユニットを直接結合する形となっており，これが現在一般的に用いられている現代的パーセプトロン（modern perceptron）あるいは単純パーセプトロン (simple perceptron) である．したがって，本節で紹介した離散時間の発火率モデルは，広義にはこの単純化されたパーセプトロンに対応する．なお，パーセプトロンの学習則に関しては次々項で詳解を行う．

\subsubsection{多出力形式への拡張}
前項での静的発火率モデルは1つのシナプス後細胞を対象としたものであったが，容易に多出力形式へと拡張可能である．複数のシナプス後細胞が同一のシナプス前細胞群から投射を受けるとし，$m$ 個のシナプス後細胞を $\mathbf{y} \in \mathbb{R}^m$，重み行列を $\mathbf{W} \in \mathbb{R}^{m \times n}$，定常項 (bias) を $\mathbf{b} \in \mathbb{R}^m$ とすれば，多出力形式のモデルは次のように表される：

\begin{equation}
\mathbf{y} = f(\mathbf{W} \mathbf{x} + \mathbf{b})
\end{equation}

この形式では，入力ベクトル $\mathbf{x}$ から出力ベクトル $\mathbf{y}$ への変換が一組の線形変換 $\mathbf{W}\mathbf{x} + \mathbf{b}$ と非線形変換 $f(\cdot)$ からなる操作で構成されており，機械学習においてはこの一連の変換過程を層（layer）と呼ぶ．さらに層を区分化し，線形変換部分を線形層（linear layer）あるいは全結合層（fully connected layer），非線形変換 $f(\cdot)$ を活性化層（activation layer）と呼ぶこともある．このような層を複数連結し，ある層の出力が次の層の入力となるようにしたモデルが多層パーセプトロン（multilayer perceptron; MLP）である．MLPはニューラルネットワーク (neural network; NN) の基本的な構造であり，第4章で詳解する．

\subsubsection{活性化関数}
生理学的観点からこの式を解釈すると，$\mathbf{x}$ はシナプス前細胞から送られる発火率（に比例する量），$\mathbf{w}$ は各シナプス結合の強度を反映した重みであり，その内積 $\mathbf{w}^\top \mathbf{x}$ はシナプス後細胞に流入する総電流に相当する\footnote{2つの神経細胞間は1つのシナプス結合しか繋がっていないわけではなく，冗長な結合が存在する．ここではそうした複数のシナプス結合によるシナプス後細胞への影響の総和を取ってシナプス重みとしている．}．バイアス項 $b$ は，発火閾値（神経細胞の興奮性などの電気的特性）や定常的な興奮性入力などを含む項として解釈される．出力 $y$ は，神経細胞の平均的な発火率（に比例する量）とみなすことができ，活性化関数 $f(\cdot)$ はこの電流入力に応じた発火頻度の変化を表す関数，すなわち周波数–電流曲線 (F-I曲線, frequency-current curve) に対応する．このF-I曲線の具体的形状と導出については，第6章で詳しく扱う．

本書では、活性化関数にベクトルを入力する場合、原則として各要素ごと (element-wise, point-wise) に計算が行われるものとする。ただし、Softmax関数のようにベクトル全体を参照して出力を決定する例外もある。

活性化関数には線形関数と非線形関数の両方が用いられる．活性化関数を恒等写像 $f(x) = x$ とした場合，このモデルは線形回帰と同型になり，こうしたモデルを線形ニューロンモデル (linear neuron model) と呼ぶ．非線形な活性化関数は、線形関数に比べて実際の神経活動の電気的性質をより適切に反映し、その特性に応じて様々な関数が用途別に用いられる。

\paragraph{Heavisideの階段関数・符号関数}
まず、Heavisideの階段関数 (Heaviside step function) あるいは単にHeaviside関数 $H(\cdot)$ は、入力が0以上であれば1を出力し、それ以外は0を出力する不連続な関数であり、次式で定義される：

\begin{equation}
f(x) = H(x):=
\begin{cases}
1 & (x \geq 0) \\
0 & (x < 0)
\end{cases}
\end{equation}

Heaviside関数は、閾値を境に出力が離散的に変化するという 全か無かの法則 (all-or-none principle) を表現するための最も基本的な関数として位置づけられる。なお，$H(0)$ の値には複数の定義が存在し、主に $0$, $\frac{1}{2}$, $1$ のいずれかを取ることがあるが、ここでは $H(0) = 1$ を採用する。次に、符号関数（sign function）$\mathrm{sgn}(\cdot)$は、入力の正負に応じて $+1$ または $-1$ を出力し、ゼロの場合には出力0を与える：

\begin{equation}
f(x) = \mathrm{sgn}(x):=
\begin{cases}
1 & (x > 0) \\
0 & (x = 0) \\
-1 & (x < 0)
\end{cases}
\end{equation}

符号関数を用いれば、次の細胞に与える影響が興奮性であれば正、抑制性であれば負として表現できる。これは、Daleの法則を無視するか、あるいは他の神経細胞を介した間接的な効果として解釈する場合に限られるが、神経活動における興奮と抑制という対照的な作用を簡潔に記述する手法となる。

\paragraph{シグモイド関数・tanh関数}
Heaviside関数と符号関数は、いずれも $x=0$ に不連続点をもち、通常の解析においては微分不可能であるため、理論的に扱いにくい場合がある。これらの関数を連続的かつ滑らかな非線形関数で近似する関数として、シグモイド関数 (sigmoid function, logistic function) およびtanh関数 (双曲線正接関数) がある。

まず、シグモイド関数 $\mathrm{sigmoid}(\cdot)$ はS字型の形状を持ち、実数値の入力を $[0, 1]$ の範囲に滑らかに写像する関数であり、次の式で定義される：

\begin{equation}
f(x) = \mathrm{sigmoid}(x):= \frac{1}{1 + e^{-x}}
\end{equation}

活性化関数としてシグモイド関数を用いた場合，このモデルはロジスティック回帰と同型になる \footnote{パーセプトロンとロジスティック回帰は同年（1958年）に提案された．}．この場合の出力は「スパイク発生の確率」と「発火率」の双方の解釈が可能である．すなわち，出力が $[0, 1]$ の範囲に正規化されているため，「ある入力に対して神経細胞が発火する確率」として解釈することも，「ある入力に対する平均的な発火頻度を正規化した値」として解釈することもできる．

一方、tanh関数 $\tanh(\cdot)$ も類似したS字型の形状を持つが、その出力は $[-1, 1]$ の範囲にわたり、より対称的な性質を持つ。定義は以下の通りである：

\begin{equation}
f(x) = \tanh(x) := \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = \frac{1 - e^{-2x}}{1 + e^{-2x}}
\end{equation}

また、両者の間には次のような明確な関係がある：

\begin{equation}
\tanh(x) = 2\cdot \mathrm{sigmoid}(2x) - 1
\end{equation}

このように、tanh関数はシグモイド関数に線形変換を施すことで得ることができる。さらに、シグモイド関数およびtanh関数に逆温度（inverse temperature）$\beta\ (>0)$ を導入することで、関数の遷移の鋭さ（sharpness）を調整することが可能である。逆温度付きのシグモイド関数およびtanh関数は、以下のように定義される：

\begin{align}
\mathrm{sigmoid}_\beta(x; \beta)&:= \frac{1}{1 + e^{-\beta x}}\\
\tanh_\beta(x; \beta)&:= \frac{e^{\beta x} - e^{-\beta x}}{e^{\beta x} + e^{-\beta x}}
\end{align}

ここで、$\beta \to \infty$ の極限を取ると、$\mathrm{sigmoid}_\beta(x; \beta) \to H(x)$、および $\tanh_\beta(x; \beta) \to \mathrm{sgn}(x)$ となる．すなわち，シグモイド関数およびtanh関数は，それぞれHeaviside関数と符号関数に近づく。

$\beta \to \infty$

具体的には、$x > 0$ では $\mathrm{sigmoid}_\beta(x)\to \frac{1}{1+0}=1$ 
に、$x < 0$ では $\mathrm{sigmoid}_\beta(x)\to \frac{1}{1+\infty}=0$ に、$x = 0$ では $\mathrm{sigmoid}_\beta(x) \to \frac{1}{1+1} = \frac{1}{2}$ に近づくため、これは $H(0) = \frac{1}{2}$ とするHeaviside関数に一致する。

同様に、$\beta \to \infty$ のときに符号関数 $\mathrm{sgn}(x)$ に収束する。すなわち、正の入力では 1、負の入力では -1、ゼロでは 0 に対応する。

なお、これらの逆温度付き関数は、関数の定義そのものを新たに与える必要はなく、元の関数の引数を単に $x \to \beta x$ と置換することで導入することができる。

\paragraph{ReLU関数・ソフトプラス関数}
シグモイド関数やtanh関数は、神経細胞が強い入力に対して発火率を飽和させるという生理的特性を捉えているが、その出力範囲（dynamic range, ダイナミックレンジ）は限られており、極端な入力に対しては出力がほぼ一定となる。このとき出力の分布は0または1（あるいは $\pm 1$）付近に偏るため、エントロピー、すなわち出力の不確実性や多様性が低下する。情報理論的に見れば、出力のエントロピーが低いということは、活性化関数が伝達できる情報量が減少していることを意味し、結果として表現力が制限される。このように、飽和を起こす関数では入力に応じた情報の分解能が失われやすく、信号伝達の効率という観点から不利である。こうした問題を避けるため、非線形だが出力が飽和しない関数として、ランプ関数 (ramp function) が用いられる \footnote{なお，機械学習にランプ関数（ReLU関数）が導入された理由はダイナミックレンジの問題だけでなく，勾配消失問題に対処できるという性質もあるが，これに関しては第4章で触れる．}．ランプ関数はHouseholder により神経細胞のモデルに導入され \citep{householder1941theory}，後にReLU関数 (rectified linear unit function, 正規化線形関数) と呼ばれるようになり \citep{nair2010rectified}、現在では後者の名称の方が広く一般に定着している。本書では、機械学習の事項も扱うため，統一的に「ReLU関数」という名称を用いる。ReLU関数 $\textrm{ReLU}(\cdot)$ は、入力が負のときに0を出力し、正のときはそのままの値を出力する関数であり，

\begin{equation}
f(x) = \textrm{ReLU}(x):=\max(0, x)=
\begin{cases}
x & (x > 0) \\
0 & (x \leq 0)
\end{cases}
\end{equation}

と表される．ただし，$\max(a, b)$ は, $a$ と $b$ のうち，大きい値を返す関数である．ReLU関数はHeaviside関数や符号関数などのように不連続点はないが，$x=0$ において微分不可能である．ReLU関数は非線形関数であるが，区間ごと ($x\geq 0$ および $x<0$) に見ると線形であるため，区分線形関数 (piecewise linear function) に含まれる．

ReLU関数を滑らかに近似する関数がソフトプラス関数 (Softplus function) $\textrm{Softplus}(\cdot)$ であり，

\begin{equation}
f(x) = \textrm{Softplus}(x):=\log(1+e^x)
\end{equation}

と表される．シグモイド関数などと同様に逆温度で関数をスケーリングでき，逆温度付きソフトプラス関数は次のように表される：

\begin{equation}
\textrm{Softplus}_\beta(x; \beta):=\frac{1}{\beta}\log(1+e^{\beta x})
\end{equation}

$\beta \to \infty$ とする場合，$x>0$ に対しては $\mathrm{Softplus}_\beta(x)\to \frac{1}{\beta} \log(e^{\beta x}) = x$ となり，$x<0$ では $\mathrm{Softplus}_\beta(x)\to \frac{1}{\beta} \log(1+0) = 0$ となる．また，$x=0$ では $\mathrm{Softplus}_\beta(x)=\frac{1}{\beta}\log(1+1)\to 0$ に漸近するため，逆温度付きソフトプラス関数は $\beta \to \infty$ でReLU関数 $\max(0, x)$ に収束する。

\paragraph{Naka–Rushton関数}
F-I曲線の形状としては，シグモイド関数のような飽和関数（saturated function）が用いられることが多いが，実際の神経細胞では多くの場合完全な飽和には至らず，部分的な飽和挙動を示す\footnote{ただし，シグモイド関数に渡す入力を適切にスケーリングすることで飽和を防ぐことは可能である．}．そのような特性を表現する関数として，以下のようなNaka–Rushton関数 $\textrm{NR}(\cdot)$ が用いられることもある\citep{naka1966s,sclar1990coding,wilson1999spikes}：

\begin{equation}
f(x) = \textrm{NR}(x; a, s, m):=\frac{m\cdot x^a}{s^a + x^a} \cdot H(x)
=\begin{cases}
\frac{m\cdot x^a}{s^a + x^a} & (x > 0) \\
0 & (x \leq 0)
\end{cases}
\end{equation}

ここで，$m\;(>0)$ は最大応答，$s\;(>0)$ は感度定数，$a\;(>0)$ は指数，$H(x)$ はHeaviside関数である．この関数は，入力に対して初期は急峻に応答し，その後徐々に応答が飽和する非線形性を持つ点で，生理的F-I曲線により近い特性を示す．

\subsubsection{神経集団モデルと神経場モデル}
Wilson–Cowanモデルと密接に関連し，より大域的・集団的な神経活動を記述する枠組みとして，神経集団モデル（neural mass model, neural population model）および神経場モデル（neural field model）があり，これらのモデルに関して簡単に触れておく．いずれも個々のニューロンの詳細な活動ではなく，神経細胞集団の平均的な膜電位や発火率の時間変化を対象とし，脳波などのマクロな神経活動を記述するための理論的枠組みを提供する．

神経集団モデルでは，皮質のマイクロカラムや局所回路といった小規模な神経集団を1ユニットとしてモデル化し，Wilson–Cowanモデルと同様に，平均発火率や膜電位のダイナミクスを扱う．神経集団モデルの例としては局所神経回路をモデル化したJansen-Ritモデル \citep{jansen1995electroencephalogram, david2003neural} や，てんかん活動の動態を記述するWendlingモデル \citep{wendling2002epileptic} などがある．

一方，神経場モデル（neural field model）では，神経活動を空間的に連続な関数として記述し，広範囲における神経活動の時空間的なダイナミクスを扱う \citep{coombes2014tutorial, cook2022neural}．神経場モデルはWilsonおよびCowan \citep{wilson1973mathematical}, Nunez \citep{nunez1974brain}, 甘利 \citep{amari1975homogeneous, amari1977dynamics} らの研究に基づいており，ここでは甘利による定式化（甘利モデル, Amari model）を簡単に説明する．甘利モデルでは，まず神経場の定義域 $\Omega$（一次元の皮質断面や二次元の皮質平面など）を考える．$\Omega$ における神経活動は以下のような積分–微分方程式によって与えられる：

\begin{equation}
\tau \frac{\partial u(x,t)}{\partial t} = -u(x,t) + \int_{\Omega} w(x, x') f(u(x', t))\,\mathrm{d}x' + I(x,t)
\end{equation}

ここで，$x, x' \in \Omega$ は神経場における位置を表す．$u(x,t)$ は位置 $x$ における時刻 $t$ の発火率，$w(x,x')$ は位置 $x'$ から $x$ への結合重み，$f(\cdot)$ は活性化関数，$I(x,t)$ は外部入力を表す．神経場モデルは，皮質進行波 (cortical travelling waves) \citep{muller2018cortical}等の現象を理論的に説明する手段となる．

\section{Hebb則とシナプス可塑性}
\subsection{Hebb則}
神経回路はどのようにして自己組織化するのだろうか．1940年代にHebbにより提案された学習則は「細胞Aが反復的または持続的に細胞Bの発火に関与すると，細胞Aが細胞Bを発火させる効率が向上するような成長過程または代謝変化が一方または両方の細胞に起こる」というものであった \citep{Hebb1949-iv}．すなわち，発火に時間的相関のある細胞間のシナプス結合を強化するという学習則である．これをHebbの学習則 (Hebbian learning rule) あるいはHebb則 (Hebb's rule) という．Hebb則は（Hebb自身ではなく）Shatzにより"cells that fire together wire together"（共に活動する細胞は共に結合する）と韻を踏みながら短く言い換えられている \citep{Shatz1992-he}．

数式を用いてHebb則を表現してみよう。まず，発火率モデルを定義する．$n$個のシナプス前細胞と$m$個のシナプス後細胞の発火率をそれぞれ $\mathbf{x} \in \mathbb{R}^n$，$\mathbf{y} \in \mathbb{R}^m$ とし，シナプス前細胞と後細胞のあいだのシナプス結合強度を $\mathbf{W} \in \mathbb{R}^{m \times n}$ とすると，前細胞と後細胞の活動の関係は活性化関数 $f(\cdot)$ を用いて $\mathbf{y} = f(\mathbf{W}\mathbf{x})$ と表現できる。このとき，連続時間の形式において，一般化されたHebb則は次のように表される：

\begin{equation}
\tau \frac{\mathrm{d}\mathbf{W}}{\mathrm{d}t} = \phi(\mathbf{y}) \varphi(\mathbf{x})^\top
\end{equation}

ここで，$\tau$ は学習の時定数であり，その逆数 $\eta := 1/\tau$ は学習率（learning rate）と呼ばれ，学習の速さを決定するパラメータである。関数 $\varphi(\cdot)$ および $\phi(\cdot)$ は，それぞれシナプス前細胞および後細胞の活動に応じてシナプス重みの変化を決定する変換関数である。特に $\varphi(\cdot)$，$\phi(\cdot)$ を恒等写像（恒等関数）とした場合，Hebb則は次のように簡潔な形で書ける：

\begin{equation}
\tau \dfrac{\mathrm{d}\mathbf{W}}{\mathrm{d}t} = \mathbf{y} \mathbf{x}^\top\quad \left(= (\textrm{後細胞の活動}) \cdot (\textrm{前細胞の活動})^\top\right)
\end{equation}

このような単純な形式のHebb則を線形Hebb則と呼び，狭義にはこれをもってHebb則とすることが多い。一方で，$\varphi(\cdot)$，$\phi(\cdot)$ を非線形関数とした拡張形式は非線形Hebb則と呼ばれ，本章で取り扱う。

\subsection{シナプス可塑性とLTP・LTD}
Hebb則の神経生理学的な基盤を裏付けるものとして，1973年にBlissとLømoによってウサギの海馬において長期増強（Long-Term Potentiation, LTP）が発見された \citep{Bliss1973-vj}．彼らの実験では，海馬のSchaffer側枝からCA1錐体細胞への経路に高頻度の電気刺激を加えることで，その後のシナプス応答が長時間にわたって増強される現象が観察された．この持続的なシナプス強度の増加は，まさにHebb則に対応する生理的現象と見なされ，Hebbian plasticityの実体と考えられるようになった．LTPはグルタミン酸作動性シナプスで観察されることが多く，特にNMDA受容体が関与することで知られている．この受容体は膜電位依存的にMg²⁺ブロックが外れることにより，カルシウムイオン（Ca²⁺）の流入を許し，それが下流のシグナル伝達を活性化してシナプス後部のAMPA受容体の増加や活性化を引き起こす．

一方，1980年代には長期抑圧（Long-Term Depression, LTD）という現象も発見された \citep{Dudek1992-nz}．これは，シナプス前ニューロンとシナプス後ニューロンが低頻度で同時活動した場合に，シナプスの伝達効率が長期にわたって減少する現象である．LTDもまた海馬や小脳などの領域で観察されており，この減弱はHebb則の反対の効果を示すものとして位置づけられる．特に，小脳における登上線維と平行線維の同時活動により引き起こされるLTDは，運動学習のモデルとして重要視されている．LTPと同様に，LTDにおいてもCa²⁺シグナリングが重要な役割を果たすが，その振幅や時間的プロファイルが異なっていることが，シナプス強化と抑圧の分岐をもたらすと考えられている．

これらの発見を通じて，Hebb則は単なる理論的仮説にとどまらず，シナプス可塑性という具体的な細胞メカニズムを通して，神経回路における学習と記憶の基盤であることが明らかにされた．

\subsection{Hebb則の不安定性と修正Hebb則}
Hebb則には問題点があり，シナプス結合強度が際限なく増大するか，あるいは消失するかという不安定性がある．これを数式で確認しておこう．前細胞と後細胞がそれぞれ1つの場合を考える．2細胞間の結合強度を $w\ (>0)$ とし，線形ニューロンを仮定，すなわち $y=wx$ が成り立つとすると，Hebb則は $\dfrac{dw}{\mathrm{d}t}=\eta yx=\eta x^2w$ となる．この場合，$\eta x^2>1$ なら $\lim_{t\to\infty} w= \infty$, $\eta x^2<1$ なら $\lim_{t\to\infty} w= 0$ となる．当然，生理的にシナプス結合強度が無限大となることはあり得ないが，不安定なほど大きくなってしまう可能性があることに違いはない．このため，Hebb則を安定化させるための修正が必要とされた．

この問題に対して、さまざまな修正Hebb則 (modified hebbian rule) が提案されている．ここでは代表的な学習則である CLO則，BCM則，Oja則について説明する．

以下では 入力 $\mathbf{x}\in \mathbb{R}^n$ $y\in \mathbb{R}$ とし，シナプス結合 $\mathbf{w}\in \mathbb{R}^n$ を持つ，単一の神経細胞モデル $y = f(\mathbf{w}^\top \mathbf{x})$ を仮定する．

\subsection{CLO則}
視覚野のニューロンが経験により方向選択性を獲得し、かつ視覚刺激の遮断によってその選択性を失うといった生理的実験の結果を説明する理論として，Cooper, Liberman, Ojaにより閾値制約付き受動的可塑性 (threshold passive modification) と呼ばれる形式の学習則が提案された \citep{Cooper1979-wz}．この学習則は提案者の頭文字を取って，CLO則 (CLO rule) と呼ばれる．CLO則は、Hebb則に対して出力の大きさに応じた閾値的な修正と重みの減衰項（忘却）を加えることにより、選択性の獲得と重みの安定性の両立を目指した学習則である。

CLO則は、出力 $y$ の値と修正閾値（modification threshold） $\theta_m$ および出力飽和値 $\theta_\mathrm{max}$ によって区分される3つの範囲で異なる更新を行う。CLO則の元の形式は離散時間での更新則であるが，表記を統一するため，連続時間でのCLO則を示す：

\begin{align}
\frac{\mathrm{d}\mathbf{w}}{\mathrm{d}t} =
\begin{cases}
- \lambda \mathbf{w} & (y \geq \theta_{\max}) \\
- \lambda \mathbf{w} + \eta_+ (\theta_{\max} - y) \mathbf{x} & (\theta_m \leq y < \theta_{\max}) \\
- \lambda \mathbf{w} - \eta_- y \mathbf{x} & (y < \theta_m)
\end{cases}
\end{align}

ここで，$\lambda\ (\geq 0)$ は重みの減衰（leak）の度合いを決める定数であり，$\eta_+, \eta_-\ (> 0)$ はそれぞれ増強・抑圧に対応する学習率である．なお，第2項はHebb則であるが，第3項は反Hebb則 (anti-Hebbian rule) と呼ばれる．

このように、適度な出力のときにのみ強化が起こり、過剰な出力では学習が停止し、出力が小さすぎる場合には抑制が起こるという、三相性の学習則が構築される。これにより、各ニューロンは特定の入力パターンに対してのみ強い応答を示すようになり、他のパターンには反応しなくなる。これは方向選択性や空間選択性のような感覚特異性（specificity）の獲得を数理的に説明する。CLO則はLTPに加えてLTDも組み込み，重みの減衰項も加えているため，不安定性はHebb則よりも低減されている．一方で，複雑で不連続な三相性の学習則を持ち，修正閾値 $\theta_m$ も固定値であるという欠点があった．$\theta_m$ が固定されていると，$\theta_m$ が大きければLTDしか生じず，小さければLTPのみが生じる．

\subsection{BCM則}
CLO則を踏まえて，Bienenstock, Cooper, Munroにより提案されたBCM則（Bienenstock–Cooper–Munro則）ではLTPとLTDを連続的に記述し，修正閾値 $\theta_m$ は出力活動の履歴に応じて変化するように修正された \citep{Bienenstock1982-km,Cooper2012-ec}．BCM則は次のように表される：

\begin{equation}
\frac{\mathrm{d}\mathbf{w}}{\mathrm{d}t} = \eta_w \, \mathbf{x} \, y (y - \theta_m)
\end{equation}

関数$\phi$は$\phi(y, \theta_m)=y(y-\theta_m)$などとする．非線形Hebb則の一種である．また $\theta_m:=\mathbb{E}[y^2]$は閾値を決定するパラメータ，修正閾値 (modification threshold) である．$\theta_m$ は活動履歴に基づいて動的に変化し、たとえば以下のように定義される：

\begin{equation}
\frac{\mathrm{d}\theta_m}{\mathrm{d}t} = \eta_\theta (y^2 - \theta_m)
\end{equation}

この構造により、出力 $y$ が $\theta_m$ を超えるときにはシナプスが強化され（LTP）、逆に $y < \theta_m$ のときには弱化（LTD）される。このように、BCM則は同一の数式の中でHebbian強化とAnti-Hebbian抑制を両立させている。また、この動的閾値 $\theta_m$ は、ニューロンが自らの「活動水準の平均」を内部的に学習していく仕組みであり、これにより入力空間に対する選択的な応答性が獲得される。これは、視覚野ニューロンの方位選択性など、実際の神経生理学的観測とも整合する

\subsection{Oja則}
Hebb則を安定化させる別のアプローチとして，結合強度を正規化するという手法が考えられる．学習率を $\eta$ とすると，$\mathbf{w}\leftarrow\dfrac{\mathbf{w}+\eta \mathbf{x}y}{\|\mathbf{w}+\eta \mathbf{x}y\|}$とすれば正規化できる．ここで，$h(\eta):=\dfrac{\mathbf{w}+\eta \mathbf{x}y}{\|\mathbf{w}+\eta \mathbf{x}y\|}$とし，$\eta=0$においてTaylor展開を行うと，

\begin{align}
h(\eta)&\approx h(0) + \eta \left.\frac{dh(\eta^*)}{\mathrm{d}\eta^*}\right|_{\eta^*=0} + \mathcal{O}(\eta^2)\\
&=\frac{\mathbf{w}}{\|\mathbf{w}\|} + \eta \left(\frac{\mathbf{x}y}{\|\mathbf{w}\|}-\frac{y^2\mathbf{w}}{\|\mathbf{w}\|^3}\right)+ \mathcal{O}(\eta^2)
\end{align}

ここで $\|\mathbf{w}\|=1$ として，1次近似すれば $h(\eta)\approx \mathbf{w} + \eta \left(\mathbf{x}y-y^2 \mathbf{w}\right)$ となる．重みの変化が連続的であるとすると，

\begin{equation}
\frac{\mathrm{d}\mathbf{w}}{\mathrm{d}t} = \eta \left(\mathbf{x}y-y^2 \mathbf{w}\right)
\end{equation}

として重みの更新則が得られる．これをOja則 (Oja's rule) と呼ぶ \citep{Oja1982-yd}．こうして得られた学習則において$\|\mathbf{w}\|\to 1$となることを確認しよう．

\begin{equation}
\frac{\mathrm{d}\|\mathbf{w}\|^2}{\mathrm{d}t}=2\mathbf{w}^\top\frac{\mathrm{d}\mathbf{w}}{\mathrm{d}t}= 2\eta y^2\left(1-\|\mathbf{w}\|^2\right)
\end{equation}

より，平衡状態 $\dfrac{\mathrm{d}\|\mathbf{w}\|^2}{\mathrm{d}t}=0$ において，$\|\mathbf{w}\|= 1$となる．

\subsection{Hebb則の変分原理的導出}
Hebb則は数学的に導出されたものではないが，神経回路のダイナミクスがある目的関数（エネルギー関数）を最適化するように設計されていると仮定すれば、Hebb則に対応する重み更新則が自然に導出される。このようなネットワークをエネルギーベースモデル (energy-based models) といい，次章で扱う．

エネルギーベースモデルでは、神経活動 $\mathbf{z}$ およびシナプス結合 $\mathbf{W}$ に対して、あるエネルギー関数 $E(\mathbf{z}, \mathbf{W})$ を定義し、ダイナミクスがそのエネルギーを減少させるように構成される。すなわち、神経状態と重みの時間変化は、それぞれ次のようにエネルギーの勾配に基づいて与えられる：

\begin{equation}
\frac{\mathrm{d}\mathbf{z}}{\mathrm{d}t}\propto-\left(\frac{\partial E}{\partial \mathbf{z}}\right)^\top,\quad \frac{\mathrm{d} \mathbf{W}}{\mathrm{d}t}\propto-\left(\frac{\partial E}{\partial \mathbf{W}}\right)^\top
\end{equation}

このとき、逆に神経活動のダイナミクスのみが先に与えられている場合でも、それに整合するエネルギー関数を定義すれば、重みの更新則（すなわち学習則）を変分原理的に導出することができる。具体的には，神経細胞の活動ダイナミクスを積分することで神経回路のエネルギー関数 $E$ を導出し，さらに $E$ を重み行列で微分することでHebb則が導出できる \citep{Isomura2020-sn}．Hebb則の導出を連続時間線形ニューロンモデル $\dfrac{\mathrm{d}\mathbf{y}}{\mathrm{d}t}=-\mathbf{y}+\mathbf{W}\mathbf{x}$ を例にして考えよう（簡単のため $\tau=1$ とした）．ここで $\dfrac{\partial E}{\partial\mathbf{y}}:=-\left(\dfrac{\mathrm{d}\mathbf{y}}{\mathrm{d}t}\right)^\top$ となるようなエネルギー関数 $E(\mathbf{x}, \mathbf{y}, \mathbf{W})$ を仮定すると，

\begin{equation}
E(\mathbf{x}, \mathbf{y}, \mathbf{W})=-\left(\int -\mathbf{y}+\mathbf{W}\mathbf{x}\,\mathrm{d}\mathbf{y}\right)\propto\|\mathbf{y}\|^2-\mathbf{y}^\top \mathbf{W}\mathbf{x} \in \mathbb{R}
\end{equation}

となる．これをさらに$\mathbf{W}$で微分すると，

\begin{equation}
\dfrac{\partial E}{\partial\mathbf{W}}=-\mathbf{x}\mathbf{y}^\top\Rightarrow
\frac{\mathrm{d}\mathbf{W}}{\mathrm{d}t}=-\left(\frac{\partial E}{\partial \mathbf{W}}\right)^\top=\mathbf{y}\mathbf{x}^\top
\end{equation}

となり，Hebb則が導出できる．

このような導出の仕方は、物理学の解析力学における変分原理（variational principle）あるいは 最小作用の原理（principle of least action）の発想に基づいている．これらの原理においては、ある経路に沿って定義される作用（action）と呼ばれる量が極値（多くの場合、極小値）を取るような経路が、実際に物理系が取る経路として実現されるとされる。この考え方と同様に、神経活動やシナプス結合の変化も、あるエネルギー関数（または汎関数）の極値（神経回路が安定している場合には極小値）を実現するようなダイナミクスとして捉えることができる。もちろん、神経活動とシナプス結合が同一のエネルギー関数を最小化しているというのは大きな仮定である。しかし、このように神経回路の時間発展を最適化の視点から記述する立場は、神経可塑性を理論的に理解するための一つの有力な枠組みを提供し得る \citep{isomura2023experimental}．

\section{自己組織化マップ}
\subsection{自己組織化マップと視覚野の構造}
自己組織化マップ（Self-Organizing Map; SOM）は，Kohonenによって提案された教師なし学習アルゴリズムであり，高次元データを低次元（通常は2次元）の格子状マップに写像することにより，データのトポロジ的構造を保ちながら可視化する手法である．SOMは，競合学習（competitive learning）と呼ばれる学習規則に基づいており，入力パターンに最も近い出力ユニット（ニューロン）が「勝者」となり，その近傍のユニットとともに重みが更新される．競合学習はSOMに限らず，出力ニューロンが互いに競い合い，最も適合するものだけが活性化されるような学習機構を指す．SOMではこの競合に加えて，空間的な隣接性を重視した協調的な重み更新が行われる点が特徴的である．これにより，類似した入力はマップ上の近い位置に投影されるようになり，結果としてトポグラフィックマッピング (topographic mapping) が実現される．

視覚野にはコラム構造が存在する．こうした構造は神経活動依存的な発生  (activity dependent development) により獲得される．本節では視覚野のコラム構造を生み出す数理モデルの中で，自己組織化マップ (self-organizing map) \citep{Kohonen1982-mn, Kohonen2013-yt}を取り上げる．

自己組織化マップを視覚野の構造に適応したのは\citep{Obermayer1990-gq, N_V_Swindale1998-ri} などの研究である．視覚野マップの数理モデルとして自己組織化マップは受容野を考慮しないなどの簡略化がなされているが，単純な手法にして視覚野の構造に関する良い予測を与える．他の数理モデルとしては自己組織化マップと発想が類似している Elastic net  \citep{Durbin1987-bp, Durbin1990-xx, Carreira-Perpinan2005-gy} \footnote{ここでのElastic netは正則化手法としてのElastic net regularizationとは異なる．Elastic netは両者を明示的に計算し，線形結合で表されるエネルギー関数を最小化する．}や受容野を明示的に設定した \citep{Tanaka2004-vz, Ringach2007-oe}などのモデルがある．総説としては\citep{Das2005-mq, Goodhill2007-va}，数理モデル同士の関係については\citep{2002-nm}が詳しい．

自己組織化マップでは「抹消から中枢への伝達過程で損失される情報量」，および「近い性質を持ったニューロン同士が結合するような配線長」の両者を最小化するような学習が行われる．包括性 (coverage) と連続性 (continuity) のトレードオフとも呼ばれる \citep{Carreira-Perpinan2005-gy}． 連続性と関連する事項として，近い性質を持つ細胞が脳内で近傍に存在するような発生/発達過程をトポグラフィックマッピング (topographic mapping) と呼ぶ．トポグラフィックマッピングの数理モデルの初期の研究としては\citep{Von_der_Malsburg1973-bz,Willshaw1976-zo, Takeuchi1979-mi}などがある．

発生の数理モデルに関する総説 \citep{Van_Ooyen2011-fz, Goodhill2018-ho}

\subsubsection{単純なデータセット}
SOMにおける $n$ 番目の入力を $\mathbf{v}(t)=\mathbf{v}_n\in \mathbb{R}^{D} (n=1, \ldots, N)$，$m$番目のニューロン $(m=1, \ldots, M)$ の重みベクトル（または活動ベクトル, 参照ベクトル）を $\mathbf{w}_m(t)\in \mathbb{R}^{D}$ とする \citep{Kohonen2013-yt}．また，各ニューロンの物理的な位置を $\mathbf{x}_m$ とする．このとき，$\mathbf{v}(t)$ に対して $\mathbf{w}_m(t)$ を次のように更新する．

まず，$\mathbf{v}(t)$ と $\mathbf{w}_m(t)$ の間の距離が最も小さい (類似度が最も大きい) ニューロンを見つける．距離や類似度としてはユークリッド距離やコサイン類似度などが考えられる．

\begin{align}
&[\text{ユークリッド距離}]: c = \underset{m}{\operatorname{argmin}}\left[\|\mathbf{v}(t)-\mathbf{w}_m(t)\|^2\right]\\
&[\text{コサイン類似度}]: c  = \underset{m}{\operatorname{argmax}}\left[\frac{\mathbf{w}_m(t)^\top\mathbf{v}(t)}{\|\mathbf{w}_m(t)\|\|\mathbf{v}(t)\|}\right]
\end{align}

この，$c$ 番目のニューロンを 勝者ユニット (best matching unit; BMU) と呼ぶ．コサイン類似度において，$\mathbf{w}_m(t)^\top\mathbf{v}(t)$ は線形ニューロンモデルの出力となる．このため，コサイン距離を採用する方が生理学的に妥当でありSOMの初期の研究ではコサイン類似度が用いられている \citep{Kohonen1982-mn}．しかし，コサイン類似度を用いる場合は $\mathbf{w}_m$ および $\mathbf{v}$ を正規化する必要がある．ユークリッド距離を用いると正規化なしでも学習できるため，SOMを応用する上ではユークリッド距離が採用される事が多い．ユークリッド距離を用いる場合，$\mathbf{w}_m$ は重みベクトルではなくなるため，活動ベクトルや参照ベクトルと呼ばれる．ここでは結果の安定性を優先してユークリッド距離を用いることとする．

こうして得られた $c$ を用いて $\mathbf{w}_m$ を次のように更新する．

\begin{equation}
\mathbf{w}_m(t+1)=\mathbf{w}_m(t)+h_{cm}(t)[\mathbf{v}(t)-\mathbf{w}_m(t)]
\end{equation}

ここで$h_{cm}(t)$は近傍関数 (neighborhood function) と呼ばれ，$c$番目と$m$番目のニューロンの距離が近いほど大きな値を取る．ガウス関数を用いるのが一般的である．

\begin{equation}
h_{cm}(t)=\alpha(t)\exp\left(-\frac{\|\mathbf{x}_c-\mathbf{x}_m\|^2}{2\sigma^2(t)}\right)
\end{equation}

ここで$\mathbf{x}$はニューロンの位置を表すベクトルである．また，$\alpha(t), \sigma(t)$は単調に減少するように設定する．\footnote{Generative topographic map (GTM)を用いれば$\alpha(t), \sigma(t)$の縮小は必要ない．また，SOMとGTMの間を取ったモデルとしてS-mapがある．}

\printbibliography[segment=\therefsegment,heading=subbibliography,title={参考文献}]
\addcontentsline{toc}{section}{参考文献}
\end{document}