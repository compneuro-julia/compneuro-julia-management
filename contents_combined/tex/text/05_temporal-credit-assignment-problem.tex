\section{経時的貢献度分配問題}
\textbf{時間的貢献度分配問題 (Temporal Credit Assignment Problem)}\index{じかんてきこうけんどぶんぱいもんだい (Temporal Credit Assignment Problem)@時間的貢献度分配問題 (Temporal Credit Assignment Problem)} は、強化学習やリカレントニューラルネットワーク（RNN）のような動的システムにおいて、時間的に遅延のある報酬に対して、どのタイミングでどの行動がどれだけ貢献したのかを特定する問題です。具体的には、ある行動が取られた後、その結果として報酬が遅れて現れる場合、報酬がどの行動に対してどれだけ寄与したのかを明確に評価する必要があります。このような問題は、時間的遅延のある状況において、個々の行動の貢献度を割り当てることができなければ、エージェントが適切に学習することは難しくなります。
この時間的貢献度分配問題を解決するための手法の一つが、\textbf{バックプロパゲーション・スルー・タイム（BPTT: Backpropagation Through Time）}\index{ばっくぷろぱげーしょん・するー・たいむ（BPTT: Backpropagation Through Time）@バックプロパゲーション・スルー・タイム（BPTT: Backpropagation Through Time）} です。BPTTは、リカレントニューラルネットワーク（RNN）において、時間的依存関係を学習するためのアルゴリズムであり、通常のバックプロパゲーションと同様に、ネットワークの誤差を逆方向に伝播させる方法ですが、時間的に連続する情報を考慮して誤差を伝播させます。BPTTのプロセスは、まずネットワークを順方向に実行して各時刻での出力を計算し、その後、出力層から誤差を計算し、逆方向に誤差を伝播させて、全てのパラメータに対する勾配を求めます。これにより、時間的に関連する状態やアクションがどのように出力に影響を与えたかを学習することができます。
RNNは、出力が次の入力に影響を与えるという再帰的な構造を持つため、BPTTを使用することで、過去の入力やアクションが現在の出力にどれだけ影響を与えているかを適切に学習することができます。しかし、BPTTには計算コストが高くなるという欠点や、長期的な依存関係において勾配消失問題や勾配爆発問題が発生することがあるため、これらの問題を解決するために、長短期記憶（LSTM）やゲート付き再帰ユニット（GRU）のような改良型RNNが使用されることが多いです。
時間的貢献度分配問題とBPTTは、特に強化学習において重要です。強化学習では、エージェントが環境とインタラクションを行い、遅延した報酬を得ることがあります。このような環境では、エージェントがどの行動に対して報酬を得たのか、またその行動が全体の目標達成にどれだけ貢献したのかを評価する必要があります。BPTTを使用することで、時間的な依存関係を適切に学習し、遅延した報酬を適切に割り当てることが可能となり、エージェントは長期的な報酬を最大化するために必要な行動を学習することができます。
\section{経時的誤差逆伝播法 (BPTT)}
Backpropagation through time and the brain
https://www.sciencedirect.com/science/article/pii/S0959438818302009
RNN
状態
\mathbf{h}(t+1)=\left(1-\frac{1}{\tau}\right)\mathbf{h}(t)+\frac{1}{\tau}f(\mathbf{W}\mathbf{h}(t)+\mathbf{W}_{in}\mathbf{x}(t+1)+\mathbf{b})
出力は
\mathbf{y}(t)=\mathbf{W}\mathbf{h}(t)
BPTT (Backpropagation through time)
backpropagation through time (BPTT) (Rumelhart et al., 1985) in order to compare it with the learning rules presented above. The derivation here follows Lecun (1988).
RTRL (Real-time recurrent learning)
Williams RJ, Zipser D. 1989. A learning algorithm for continually running fully recurrent neural networks. Neural Computation 1:270–280. DOI: https://doi.org/10.1162/neco.1989.1.2.270
RFRO (Random feedback local online learning)
Check the implementation
\frac{\partial h_{j} (t )}{\partial W_{a b}} = (1 - \frac{1}{\tau} ) \frac{\partial h_{j} (t - 1 )}{\partial W_{a b}} + \frac{1}{\tau} \delta_{j a} \phi^{\prime} (u_{a} (t ) ) h_{b} (t - 1 ) + \frac{1}{\tau} \underset{k}{ \left (\sum \right ) } \phi^{\prime} (u_{j} (t ) ) W_{j k} \frac{\partial h_{k} (t - 1 )}{\partial W_{a b}} ,
Output error
\begin{align}
\epsilon (t)=\mathbf{y}(t)-\hat{\mathbf{y}}(t)\\
\mathcal{L}=\frac{1}{2T}\sum_{t=1}^T \|\epsilon (t)\|^2
\end{align}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}}=-\frac{1}{T}\sum_{t=1}^T \mathbf{W}_{out}^\top \epsilon (t)\frac{\partial \mathbf{h}(t)}{\partial \mathbf{W}}
Update rule
\begin{align}
\Delta \mathbf{W}^{out}_t&=\eta \epsilon_{t} \mathbf{h}_t\\
\Delta \mathbf{W}_{rec}(t)&=\eta \mathbf{B}\epsilon(t) \mathbf{P}(t)\\
\Delta \mathbf{W}_{in}(t)&=\eta \mathbf{B}\epsilon (t) \mathbf{Q}(t)\\
\end{align}
Eligibility trace $\mathbf{P}\in \mathbb{R}^{N_{rec}\times N_{rec}}, \mathbf{Q}\in \mathbb{R}^{N_{rec}\times N_{in}}$
\begin{align}
\mathbf{P}_t&=\frac{1}{\tau}f'(\mathbf{u}_t)\mathbf{h}_{t-1}^\top+\left(1-\frac{1}{\tau}\right)\mathbf{P}_{t-1}\\
\mathbf{Q}_t&=\frac{1}{\tau}f'(\mathbf{u}_t)\mathbf{x}_{t-1}^\top+\left(1-\frac{1}{\tau}\right)\mathbf{Q}_{t-1}
\end{align}
----
\section{\textbf{BPTT（Backpropagation Through Time）とRTRL（Real-Time Recurrent Learning）の数式表現}\index{BPTT（Backpropagation Through Time）とRTRL（Real-Time Recurrent Learning）のすうしきひょうげん@BPTT（Backpropagation Through Time）とRTRL（Real-Time Recurrent Learning）の数式表現}}
\subsection{\textbf{1. BPTT（Backpropagation Through Time）}\index{1. BPTT（Backpropagation Through Time）}}
BPTTはRNN（Recurrent Neural Network）の学習において誤差逆伝播法（Backpropagation）を時間方向に適用する手法である．時系列データに対する損失関数の勾配を効率的に計算するために，時間展開したネットワーク上で逆伝播を行う．
\subsubsection{\textbf{(1) 順伝播}\index{(1) じゅんでんぱ@(1) 順伝播}}
考えるRNNの状態更新と出力の式は、次のように書けます。
\mathbf{h}_t = f(\mathbf{W}_h \mathbf{h}_{t-1} + \mathbf{W}_x \mathbf{x}_t + \mathbf{b}_h)
\mathbf{y}_t = g(\mathbf{W}_y \mathbf{h}_t + \mathbf{b}_y)
ここで、
\begin{itemize}
\item $h_t$は時刻$t$の隠れ状態
\item $x_t$は入力
\item $y_t$は出力
\item $W_h, W_x, W_y$はそれぞれの重み行列
\item $b_h, b_y$はバイアス
\item $f$は活性化関数（例: tanh, ReLU）
\item $g$は出力関数（例: softmax）
\end{itemize}
\subsubsection{\textbf{(2) 損失関数}\index{(2) そんしつかんすう@(2) 損失関数}}
一般的な損失関数$\mathcal{L}$は、時系列全体の誤差の合計として定義されます。
\mathcal{L} = \sum_{t=1}^{T} \ell(y_t, \hat{y}_t)
ここで$\ell(y_t, \hat{y}_t)$は時刻$t$における損失。
\subsubsection{\textbf{(3) 誤差逆伝播}\index{(3) ごさぎゃくでんぱ@(3) 誤差逆伝播}}
BPTTでは、勾配を時間方向に逆伝播させます。
\paragraph{\textbf{出力層の勾配}\index{しゅつりょくそうのこうばい@出力層の勾配}}
出力層の重み$W_y$に関する勾配：
\frac{\partial L}{\partial W_y} = \sum_{t=1}^{T} \frac{\partial \ell_t}{\partial y_t} \frac{\partial y_t}{\partial W_y}
\paragraph{\textbf{隠れ層の勾配}\index{かくれそうのこうばい@隠れ層の勾配}}
隠れ層の重み$W_h$に関する勾配は、時間的な依存関係を考慮して連鎖律を適用します。
\delta_t = \frac{\partial L}{\partial h_t} = \frac{\partial \ell_t}{\partial y_t} \frac{\partial y_t}{\partial h_t} + \delta_{t+1} \frac{\partial h_{t+1}}{\partial h_t}
重み$W_h$に関する勾配は以下のようになります。
\frac{\partial L}{\partial W_h} = \sum_{t=1}^{T} \delta_t \frac{\partial h_t}{\partial W_h}
BPTTでは、計算量削減のために「一定の時間範囲（トランケーション）」のみを考慮する \textbf{Truncated BPTT}\index{Truncated BPTT} もよく用いられます。
---
\subsection{\textbf{2. RTRL（Real-Time Recurrent Learning）}\index{2. RTRL（Real-Time Recurrent Learning）}}
RTRLは、RNNの各時刻の重み更新をリアルタイムに行う学習アルゴリズムです。BPTTとは異なり、時間展開をせずに、各時刻での勾配を逐次更新します。
\subsubsection{\textbf{(1) 隠れ状態の再掲}\index{(1) かくれじょうたいのさいけい@(1) 隠れ状態の再掲}}
h_t = f(W_h h_{t-1} + W_x x_t + b_h)
\subsubsection{\textbf{(2) 隠れ状態の勾配伝播}\index{(2) かくれじょうたいのこうばいでんぱ@(2) 隠れ状態の勾配伝播}}
RTRLでは、すべての時刻で重み$W_h$に対する状態の変化率を直接追跡します。
S_t = \frac{\partial h_t}{\partial W_h}
この$S_t$を逐次的に更新する式は以下のようになります。
S_t = f'(W_h h_{t-1} + W_x x_t + b_h) \left( W_h S_{t-1} + \frac{\partial h_t}{\partial W_h} \right)
これを利用して、損失$L$に関する勾配を計算します。
\frac{\partial L}{\partial W_h} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_t} S_t
---
\subsection{\textbf{BPTTとRTRLの比較}\index{BPTTとRTRLのひかく@BPTTとRTRLの比較}}
| 項目 | BPTT | RTRL |
|------|------|------|
| 勾配計算 | 時間展開後に逆伝播 | リアルタイムに勾配更新 |
| 計算コスト | 高い（長期依存の計算が可能） | 非常に高い（$O(T n^4)$の計算量） |
| メモリ使用量 | 長期依存を扱う場合多い | 少ない（即時更新） |
| 応用 | 深層学習で一般的（LSTM, GRU） | ほぼ使われない（計算コストの問題） |
通常、計算コストの問題でRTRLはほとんど使用されず、BPTTやTruncated BPTTが主流です。
##
A Practical Sparse Approximation for
Real Time Recurrent Learning
BPTT
\frac{\partial \mathcal{L}}{\partial \theta}=\sum_{t=1}^T\frac{\partial \mathcal{L}}{\partial \mathbf{h}_t}\frac{\partial \mathbf{h}_t}{\partial \theta_t}=\sum_{t=1}^T\left(\frac{\partial \mathcal{L}}{\partial \mathbf{h}_t}\frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t}+\frac{\partial \mathcal{L}_t}{\partial \mathbf{h}_t}\right)\frac{\partial \mathbf{h}_t}{\partial \theta_t}
\frac{\partial \mathcal{L}}{\partial \mathbf{h}_t}=\frac{\partial \mathcal{L}}{\partial \mathbf{h}_t}\frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t}+\frac{\partial \mathcal{L}_t}{\partial \mathbf{h}_t}
RTRL
\frac{\partial \mathcal{L}}{\partial \theta}=\sum_{t=1}^T\frac{\partial \mathcal{L}_t}{\partial \mathbf{h}_t}\frac{\partial \mathbf{h}_t}{\partial \theta}=\sum_{t=1}^T\frac{\partial \mathcal{L}_t}{\partial \mathbf{h}_t}\left(\frac{\partial \mathbf{h}_t}{\partial \theta_t}+\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}}\frac{\partial \mathbf{h}_{t-1}}{\partial \theta}\right)
\frac{\partial \mathbf{h}_t}{\partial \theta}=\frac{\partial \mathbf{h}_t}{\partial \theta_t}+\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}}\frac{\partial \mathbf{h}_{t-1}}{\partial \theta}
\frac{d\mathcal{L}_t}{d \theta}=\frac{d\mathcal{L}_t}{d h_t}\frac{d h_t}{d \theta}
$h_t=f(x_t, h_{t-1}, \theta)$を全微分すると，
\frac{dh_t}{d\theta}=\frac{\partial h_t}{\partial h_{t-1}}\frac{d h_{t-1}}{d \theta}+\frac{\partial h_t}{\partial x_t}\frac{d x_t}{d \theta}+\frac{\partial h_t}{\partial\theta}=\frac{\partial h_t}{\partial h_{t-1}}\frac{d h_{t-1}}{d \theta}+\frac{\partial h_t}{\partial\theta}
$x_t$は$\theta$に依存しない．全微分と偏微分に注意すると，
RFLOに記載
\Delta \mathbf{W}_{\textrm{out}} = \frac{\eta}{T} \sum_{t=1}^T \epsilon(t) h(t)^\top 
\Delta \mathbf{W}_{\textrm{rec}} = \frac{\eta}{T} \sum_{t=1}^T \mathbf{W}_{\textrm{out}}^\top \epsilon(t) \frac{\partial h(t)}{\partial \mathbf{W}_{\textrm{rec}}}
\section{実時間リカレント学習 (RTRL)}
## 適格度トレースによるRTRLの近似※
