{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "896a4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from base64 import b64decode\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pykakasi\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262e227-1e2c-495a-947f-f90427790f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d95923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIndexing():\n",
    "    def __init__(self,):\n",
    "        self.kks = pykakasi.kakasi()\n",
    "    \n",
    "    def japanese_check(self, s):\n",
    "        # s: string\n",
    "        if re.search(r'[ぁ-ん]+|[ァ-ヴー]+|[一-龠]+', s):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def converter(self, s):\n",
    "        s = s.replace(r\"**\", \"\")\n",
    "        if self.japanese_check(s):\n",
    "            s_hira = \"\".join([word[\"hira\"] for word in self.kks.convert(s)])\n",
    "            return r'\\textbf{'+s+r'}\\index{'+s_hira + r\"@\" + s + r'}'\n",
    "            #return r'\\index{'+s_hira + r\"@\\textbf{\" + s + r'}|textbf}'\n",
    "        else:\n",
    "            return r'\\textbf{'+s+r'}\\index{'+s + r'}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb10e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_indexing = AutoIndexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aed8dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown2latex(s):\n",
    "    # s: string\n",
    "    #s = re.sub(r'\\####\\ (.+?)\\n', r'\\\\paragraph{\\1}\\n', s)  # subsubsection\n",
    "    #s = re.sub(r'\\###\\ (.+?)\\n', r'\\\\subsubsection{\\1}\\n', s)  # subsubsection\n",
    "    #s = re.sub(r'\\##\\ (.+?)\\n', r'\\\\subsection{\\1}\\n', s)  # subsection\n",
    "    #s = re.sub(r'\\#\\ (.+?)\\n', r'\\\\section{\\1}\\n', s)      # section\n",
    "    s = re.sub(r'\\#####\\ (.+?)\\n', r'\\\\paragraph{\\1}\\n', s)  # subsubsection\n",
    "    s = re.sub(r'\\####\\ (.+?)\\n', r'\\\\subsubsection{\\1}\\n', s)  # subsubsection\n",
    "    s = re.sub(r'\\###\\ (.+?)\\n', r'\\\\subsection{\\1}\\n', s)  # subsubsection\n",
    "    s = re.sub(r'\\##\\ (.+?)\\n', r'\\\\section{\\1}\\n', s)  # subsection\n",
    "    #s = re.sub(r'\\#\\ (.+?)\\n', r'\\\\section{\\1}\\n', s)      # section\n",
    "    \n",
    "    #s = re.sub(r'\\*\\*(.+?)\\*\\*', r'\\\\textbf{\\\\index{\\1}}', s)   # bold\n",
    "    s = re.sub(r'\\*\\*(.+?)\\*\\*', lambda m: auto_indexing.converter(m.group()), s)   # bold\n",
    "    #s = re.sub(r'\\*(.+?)\\*', r'\\\\textit{\\1}', s)       # italic\n",
    "    \n",
    "    #s = s.replace(r\"```{note}\", \"\\\\footnote{\") # note to footnote\n",
    "    #s = s.replace(r\"```\", \"}\")\n",
    "    s = s.replace(r\"```julia\", \"\\\\begin{lstlisting}\") # note to footnote\n",
    "    s = s.replace(r\"```\", \"\\\\end{lstlisting}\")\n",
    "    s = re.sub(r'<(.+?)>', r'\\\\url{\\1}', s) # url\n",
    "\n",
    "    s = re.sub(r'{cite:p}`(.+?)`', r'\\\\citep{\\1}', s)     \n",
    "    s = re.sub(r'`(.+?)`', r'\\\\jl{\\1}', s) # inline code with \\newcommand{\\jl}{\\lstinline[language=julia]}\n",
    "\n",
    "    s = re.sub(r'<(.+?)>', r'\\\\url{\\1}', s) # url\n",
    "    s = s.replace(r\":=\", r\"\\coloneqq \")\n",
    "    s = s.replace(r\"=:\", r\"\\eqqcolon \")\n",
    "    #s = s.replace(r\"（\", \" (\") \n",
    "    #s = s.replace(r\"）\", \") \") \n",
    "    s = s.replace(r\"$$\", \"\") \n",
    "    s = s.replace(\"\\r\\n\", \"\\n\") \n",
    "    s = s.replace(\"\\r\", \"\")\n",
    "    #s = s.replace(r\"．\", \"．\\n\") \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "281faa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_itemized(text):\n",
    "    #splited_text = text.split('\\n')\n",
    "    #splited_text = all_remove(splited_text, \"\\n\")\n",
    "    splited_text = list(filter(None, text))\n",
    "    # itemize\n",
    "    item_idx = [line[:2] == \"- \" for line in splited_text]\n",
    "    if np.sum(item_idx) > 0:\n",
    "        item_idx += [False]\n",
    "        item_startend = np.where(np.diff(np.array(item_idx)) == True)[0]\n",
    "        item_startend += np.arange(len(item_startend)) + 1\n",
    "\n",
    "        # replace - to \\item\n",
    "        for i in range(len(splited_text)):\n",
    "            if item_idx[i]:\n",
    "                splited_text[i] = splited_text[i].replace('- ', r'\\item ', 1) \n",
    "\n",
    "        # add begin and end\n",
    "        for j in range(len(item_startend)):\n",
    "            if j % 2 == 0:\n",
    "                splited_text.insert(item_startend[j], r\"\\begin{itemize}\")\n",
    "            else:\n",
    "                splited_text.insert(item_startend[j], r\"\\end{itemize}\")\n",
    "    \n",
    "    # enumerate\n",
    "    enum_idx = [line[:3] == \"1. \" for line in splited_text]\n",
    "    if np.sum(enum_idx) > 0:\n",
    "        enum_idx += [False]\n",
    "        enum_startend = np.where(np.diff(np.array(enum_idx)) == True)[0]\n",
    "        enum_startend += np.arange(len(enum_startend)) + 1\n",
    "\n",
    "        # replace 1. to \\item\n",
    "        for i in range(len(splited_text)):\n",
    "            if enum_idx[i]:\n",
    "                splited_text[i] = splited_text[i].replace('1. ', r'\\item ', 1) \n",
    "\n",
    "        # add begin and end\n",
    "        for j in range(len(enum_startend)):\n",
    "            if j % 2 == 0:\n",
    "                splited_text.insert(enum_startend[j], r\"\\begin{enumerate}\")\n",
    "            else:\n",
    "                splited_text.insert(enum_startend[j], r\"\\end{enumerate}\")\n",
    "\n",
    "    for i in range(len(splited_text)):\n",
    "        if splited_text[i][-1:] != \"\\n\":\n",
    "            splited_text[i] += \"\\n\"\n",
    "    return splited_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c73ef2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_remove(xlist, remove):\n",
    "    return [value for value in xlist if value != remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28bc5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_ipynb2latex(dir_path, filename, save_dir=\"./text/\", code_include=True):\n",
    "    if code_include:\n",
    "        os.makedirs(f\"{save_dir}{filename.split(\"/\")[0]}\", exist_ok=True)\n",
    "    else:\n",
    "        os.makedirs(f\"{save_dir}{filename}\", exist_ok=True)\n",
    "    file_path = dir_path + filename\n",
    "    master_list = []\n",
    "    if os.path.isfile(file_path + \".md\"):\n",
    "        f = codecs.open(file_path + \".md\", 'r', encoding=\"utf8\")\n",
    "        md = f.read()\n",
    "        # convert\n",
    "        text = markdown2latex(md)\n",
    "        text = text.split('\\n')\n",
    "        text = latex_itemized(text) #\n",
    "        if not \":filter: docname in docnames\" in \"\".join(text):\n",
    "            # save\n",
    "            text = all_remove(text, \"\\n\")\n",
    "            text = all_remove(text, '\\r\\n')\n",
    "            master_list += text\n",
    "    elif os.path.isfile(file_path + \".ipynb\"):\n",
    "        f = codecs.open(file_path + \".ipynb\", 'r', encoding=\"utf8\")\n",
    "        source = f.read()\n",
    "        y = json.loads(source)\n",
    "        num_cells = len(y['cells'])\n",
    "        for cell_idx in range(num_cells):\n",
    "            cell = y['cells'][cell_idx]\n",
    "            if cell['cell_type'] == 'markdown':\n",
    "                # convert\n",
    "                text = [markdown2latex(s) for s in cell['source']]\n",
    "                text = latex_itemized(text)\n",
    "                if not \":filter: docname in docnames\" in \"\".join(text):\n",
    "                    # save\n",
    "                    text = all_remove(text, \"\\n\")\n",
    "                    master_list += text\n",
    "                    #parted_file_path = save_dir + \"/{:03d}.tex\".format(cell_idx)\n",
    "                    #with open(parted_file_path, 'w', encoding='UTF-8') as f:\n",
    "                    #    f.writelines(text)\n",
    "                    #master_list.append(r\"\\input{\"+parted_file_path+\"}\\n\")\n",
    "            elif cell['cell_type'] == 'code':\n",
    "                # ToDo:'outputs'\n",
    "                code = cell['source']\n",
    "                if code_include:\n",
    "                    master_list.append(r\"\\begin{lstlisting}[language=julia]\"+\"\\n\")\n",
    "                    master_list += code\n",
    "                    master_list.append(\"\\n\" + r\"\\end{lstlisting}\"+\"\\n\")\n",
    "                else:\n",
    "                    parted_file_path = f\"{save_dir}{filename}/{cell_idx:03d}.jl\"\n",
    "                    with open(parted_file_path, 'w', encoding='UTF-8') as f:\n",
    "                        f.writelines(code)\n",
    "                    master_list.append(r\"\\lstinputlisting[language=julia]{\"+parted_file_path+\"}\\n\")\n",
    "\n",
    "                if cell['outputs']:\n",
    "                    if 'data' in cell['outputs'][0]:\n",
    "                        output = cell['outputs'][0]['data']\n",
    "                        if \"image/png\" in output.keys():\n",
    "                            png_bytes = output['image/png']\n",
    "                            png_bytes = b64decode(png_bytes)\n",
    "                            bytes_io = BytesIO(png_bytes)\n",
    "                            image = Image.open(bytes_io)\n",
    "\n",
    "                            figname = \"cell{:03d}.png\".format(cell_idx)\n",
    "                            figsavepath = \"./fig/\" + filename + \"/\" + figname\n",
    "                            os.makedirs(\"./fig/\" + filename, exist_ok=True)\n",
    "                            image.save(figsavepath, 'png')\n",
    "\n",
    "                            caption = figname\n",
    "                            figlabel = figname #\"ccc\"\n",
    "                            figcode = r\"\\begin{figure}[ht]\"+\"\\n\\t\"+r\"\\centering\"+\"\\n\"\n",
    "                            figcode += \"\\t\" + r\"\\includegraphics[scale=0.8, max width=\\linewidth]{\"+figsavepath+\"}\\n\"\n",
    "                            figcode += \"\\t\" + r\"\\caption{\" + caption + \"}\\n\"\n",
    "                            figcode += \"\\t\" + r\"\\label{\"+figlabel+\"}\\n\"\n",
    "                            figcode += r\"\\end{figure}\" + \"\\n\"\n",
    "                            if code_include:\n",
    "                                master_list.append(figcode)\n",
    "                            else:\n",
    "                                parted_output_path = f\"{save_dir}{filename}/output_{cell_idx:03d}.tex\"\n",
    "                                with open(parted_output_path, 'w', encoding='UTF-8') as f:\n",
    "                                    f.writelines(figcode)\n",
    "                                master_list.append(r\"\\input{\"+parted_output_path+\"}\\n\")\n",
    "                        elif \"text/plain\" in output.keys():\n",
    "                            print(output[\"text/plain\"])\n",
    "\n",
    "    with open(f\"{save_dir}{filename}.tex\", 'w', encoding='UTF-8') as f:\n",
    "        f.writelines(master_list[1:])\n",
    "        #f.writelines(master_list)\n",
    "    return master_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f7aa3d5-c693-4ba3-9be9-e6c55277ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../markdowns/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "715d7824-3edd-4323-b72a-26de6db38130",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = glob.glob(f\"{dir_path}*.md\")\n",
    "filenames = [f.split(\"\\\\\")[-1].split(\".\")[0] for f in filepaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6957eb0-4505-4409-85ca-d42b4b105c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00_overview',\n",
       " '01_introduction',\n",
       " '02_local-learning-rule',\n",
       " '03_energy-based-model',\n",
       " '04_credit-assignment-problem',\n",
       " '05_temporal-credit-assignment-problem',\n",
       " '06_neuron-synapse-model',\n",
       " '07_spiking-neural-networks',\n",
       " '08_reservoir-computing',\n",
       " '09_bayesian-brain',\n",
       " '10_motor-learning',\n",
       " '11_reinforcement-learning',\n",
       " '12_morphology']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6cf2708f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    #filename = \"01_introduction\"\n",
    "    master_list = md_ipynb2latex(dir_path, filename, save_dir=\"./text/\", code_include=True)\n",
    "#master_list = md_ipynb2latex(dir_path, filename)\n",
    "#master_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c694ce8-6f4b-4110-b043-025d8f2b1e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# 第1章：はじめに\\n',\n",
       " '\\\\section{本書の目的と構成}\\n',\n",
       " '\\\\subsection{神経科学における計算論}\\n',\n",
       " '本書では神経科学における数理モデルを主として取り扱う．初めに神経科学におけるモデルの役割について触れておこう．まず，神経科学の目標は端的に言えば「脳神経系を理解する」ことにある．神経科学に限らず，種々の学問分野においては実験と理論の2本柱で，対象とする現象や物質の理解が進められる．ここで実験は調査等も踏まえ実データを取る行為とする．理論の役割は複数あり，実験結果の抽象化および統合，仮説の提供，現象の予測等である \\\\citep{Blohm2020-vc}．\\n',\n",
       " \"「脳神経系を理解する」ということに関して，その定義は研究者により様々である．ここでは脳の計算処理に関する理論的理解を進めるための1つの方法として Marrの3レベル (Marr's Three Levels) を紹介する \\\\citep{Marr1982-wk}．Marrの3レベルは視覚系における計算処理の理解を主としていたが，他でも適用可能である．3レベルとは(1)計算理論 (computational theory), (2) 表現・アルゴリズム (representation and algorithm), (3)実装 (implementation) であり，それぞれの段階での議論や理解を行う．(1)では脳の目的関数とそれを用いた最適化問題の設定を行う．(2)では(1)を実現するための表現およびアルゴリズムを解明する．(3)では(1,2)を神経回路・ハードウェア上で実装する方法を解明することを目標とし，平易には「脳」を作って理解すると言い換えることもできる\\\\footnote{ここでの「作る」は計算機等でシミュレーションするという意味であり，脳オルガノイド (brain organoid) を作成するなどの意味ではない}．本書ではこの(3)を重視し，読者が自らの手で理論を検証し，数値計算による結果を再現できることを目標とした．また，本書は数式をプログラミングのコードに変換する具体例集としての役割も持っている．\\n\",\n",
       " 'モデルの中でも，本書では機械学習に関連する内容が多数登場する．これは神経科学と機械学習は互いに影響を及ぼし合ってきたためである \\\\citep{Hassabis2017-zm}. \\n',\n",
       " '神経科学から機械学習への応用は例えば，ニューラルネットワーク，記憶モデル，注意モデルなどがある．逆に機械学習から神経科学への応用は強化学習，運動制御，ベイズ脳仮説などが挙げられる．\\n',\n",
       " '筆者の立場としては，神経科学は機械学習の発展のためにあるわけではないので，後者の流れ，すなわち機械学習から神経科学への応用を重視して本書を執筆した次第である．\\n',\n",
       " '\\\\subsection{本書の構成}\\n',\n",
       " '第1章では，Julia言語の使用法と用いる数学について簡単に説明する．第2章から第5章までは発火率モデルおよびニューラルネットワークについての説明を行う．第2章では，まず神経細胞の簡単な生理学について説明する．発火率モデルを説明したのち，局所学習則によって訓練されるネットワークの説明を行う．第3章では，同じく局所学習則ではあるが，ネットワーク全体のエネルギーを下げることを目的としたエネルギーベースモデルと呼ばれる枠組みのネットワークについて説明をする．第4章では，誤差逆伝播法に基づいたニューラルネットワークを説明し，貢献度分配問題の生理学的な解決策について説明をする．第5章では，さらに再起型ニューラルネットワークを説明し，経時的貢献度分配問題について説明を行う．\\n',\n",
       " '第6・7章ではスパイキングニューラルネットワークとその学習について取り扱う．第6章ではネットワークレベルの話から再び神経細胞とシナプスに回帰する．次に，シナプスのダイナミクスについて説明を補いながらモデルを構築する．第7章ではネットワークの構築と学習について，第2章から第5章までを踏まえて説明する．\\n',\n",
       " '第8章から12章は上記以外の内容について各論的に説明を行う．第8章のリザバーコンピューティングの章では，リザバーコンピューティングと呼ばれる枠組みのネットワークおよびその学習則について，発火率・スパイキングモデルの双方をまとめて紹介する．第9章ではベイズ推論の章では，神経回路網により，如何にして確率計算を行うかを説明する．第10章では運動学習では，最適制御問題の解決策について説明する．第11章の強化学習では，強化学習の基本的事項の説明と，大脳基底核との関連性について説明する．第12章は補足的な話題であり，ネットワーク・形態学・グリアについて説明を行う．\\n',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0364021387800253\\n',\n",
       " '\\\\section{Julia言語の使用法}\\n',\n",
       " '\\\\subsection{Julia言語の特徴}\\n',\n",
       " 'Julia言語は\\n',\n",
       " '本書を執筆するにあたり，なぜJulia言語を選択したかというのにはいくつか理由がある．\\n',\n",
       " 'JuliaはJIT（Just-In-Time）コンパイルを用いており\\n',\n",
       " 'JITコンパイラ\\n',\n",
       " '実行速度が高速であること．\\n',\n",
       " 'ライセンスフリーであり，無料で使用できること．\\n',\n",
       " '線型代数演算が簡便に書けること．\\n',\n",
       " 'Unicodeを使用できるため，疑似コードに近いコードを書けること．\\n',\n",
       " '他の言語の候補として，MATLAB, Pythonが挙げられた．MATLABは神経科学分野で根強く使用される言語であり，線型代数計算の記述が簡便である．なお，線型代数演算の記法に関してはJuliaはMATLABを参考に構築されたため，ほぼ同様に記述することができる．また，MATLABを使用するには有償ライセンスが必要である．ただし，互換性を持ったフリーソフトウェアであるOctaveが存在することは明記しておく．\\n',\n",
       " 'Pythonは機械学習等の豊富なライブラリと書きやすさから広く利用されている言語である．ただし，numpyを用いないと高速な処理を書けない場合が多く，ナイーブな実装では実行速度が低下してしまう問題がある．線型代数計算も簡便に書くことができず，数式をコードに変換する際の手間が増えるという問題がある．\\n',\n",
       " '多重ディスパッチ（multiple dispatch）があることはJulia言語の大きな特徴である．\\n',\n",
       " '\\\\subsection{Julia言語のインストール方法}\\n',\n",
       " 'Julia (\\\\url{https://julialang.org/}) に\\n',\n",
       " 'juliaup (\\\\url{https://github.com/JuliaLang/juliaup}) でバージョン管理\\n',\n",
       " 'また，2025年3月以降，Google Colab (\\\\url{https://colab.google/}) においてPythonやRに並んでJuliaを選択して使用することが可能となっている．\\n',\n",
       " '\\\\subsection{使用するライブラリ}\\n',\n",
       " 'REPL\\n',\n",
       " 'で\\\\jl{]} を入力することで，パッケージ管理モードに移行する．\\n',\n",
       " '本書で使用するJuliaライブラリは以下の通りである．\\n',\n",
       " '\\\\begin{itemize}\\n',\n",
       " '\\\\item IJulia: 開発環境\\n',\n",
       " '\\\\item PyPlot: 描画用ライブラリ\\n',\n",
       " '\\\\item LinearAlgebra: 高度な線形代数演算\\n',\n",
       " '\\\\item Random: \\n',\n",
       " '\\\\end{itemize}\\n',\n",
       " 'Pythonではnumpyで完結するところをライブラリをいくつも読み込む必要がある点は欠点ではある．\\n',\n",
       " '描画用のライブラリには \\\\jl{PyPlot.jl} を使用した．\\\\jl{PyPlot} はPythonライブラリである \\\\jl{matplotlib} に依存したライブラリである．Juliaで完結させたい場合は \\\\jl{Plot.jl} や \\\\jl{Makie.jl} を使用することが推奨されるが，\\\\jl{PyPlot} (\\\\jl{matplotlib}) の方が高機能であるため，\\n',\n",
       " 'Pythonがない場合は\\n',\n",
       " '\\\\begin{lstlisting}\\n',\n",
       " 'julia> ENV[\"PYTHON\"] = \"\"\\n',\n",
       " 'julia> ]\\n',\n",
       " 'pkg> build PyCall\\n',\n",
       " '\\\\end{lstlisting}\\n',\n",
       " 'Pythonを既にインストールしている場合は，\\n',\n",
       " '\\\\begin{lstlisting}\\n',\n",
       " 'julia> ENV[\"PYTHON\"] = raw\"C:\\\\Users\\\\TakutoYamamoto\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python.exe\"\\n',\n",
       " 'julia> ]\\n',\n",
       " 'pkg> build PyCall\\n',\n",
       " '\\\\end{lstlisting}\\n',\n",
       " 'Windowsの場合\\n',\n",
       " '例としてPythonの実行ファイル (python.exe) への完全なパスを\\n',\n",
       " '\\\\subsection{開発環境}\\n',\n",
       " 'インタプリタ型言語である\\n',\n",
       " 'vscode\\n',\n",
       " '筆者は（Pythonユーザーでもあるため）Jupyter Labを使用している．\\n',\n",
       " 'JuliaのみでJupyter Labを使用するには\\n',\n",
       " '\\\\begin{lstlisting}\\n',\n",
       " 'using IJulia\\n',\n",
       " 'jupyterlab(detached=true)\\n',\n",
       " '\\\\end{lstlisting}\\n',\n",
       " 'とすればよい．ただし，この際にCondaを入れることになるため，別途Pythonをインストールしておく方が推奨される．\\n',\n",
       " 'p.33\\n',\n",
       " '\\\\jl{Pluto.jl} を用いることも可能である\\n',\n",
       " '\\\\subsection{Julia言語の基本構文}\\n',\n",
       " 'https://docs.julialang.org/en/v1/manual/noteworthy-differences/\\n',\n",
       " '\\\\subsection{命名規則}\\n',\n",
       " 'この節では，本書で用いるJuliaの変数名や関数名等に関する基本的な取り決めをまとめる．\\n',\n",
       " '\\\\subsection{変数名}\\n',\n",
       " '\\\\begin{itemize}\\n',\n",
       " '\\\\item \\\\jl{nt}: 時間ステップ数 (number of time steps)\\n',\n",
       " '\\\\item \\\\jl{t}, \\\\jl{tt}: 時間ステップのインデント\\n',\n",
       " '\\\\end{itemize}\\n',\n",
       " '\\\\section{基礎的数学とJuliaでの記法}\\n',\n",
       " '本書で使用する数学的内容を整理する．\\n',\n",
       " '\\\\subsection{表記法}\\n',\n",
       " '本書では次のような記号表記を用いる．\\n',\n",
       " '\\\\begin{itemize}\\n',\n",
       " '\\\\item 実数全体を$\\\\mathbb{R}$, 複素数全体は$\\\\mathbb{C}$と表記する．\\n',\n",
       " '\\\\item スカラーは小文字・斜体で$x$のように表記する．\\n',\n",
       " '\\\\item ベクトルは小文字・立体・太字で$\\\\mathbf{x}$のように表記し，列ベクトル (縦ベクトル) として扱う．\\n',\n",
       " '\\\\item 行列は大文字・立体・太字で$\\\\mathbf{X}$のように表記する．\\n',\n",
       " '\\\\item $n\\\\times 1$の実ベクトルの集合を$\\\\mathbb{R}^n$,$n\\\\times m$の実行列の集合を$\\\\mathbb{R}^{n\\\\times m}$と表記する．\\n',\n",
       " '\\\\item 行列$\\\\mathbf{X}$の置換は$\\\\mathbf{X}^\\\\top$と表記する．ベクトルの要素を表す場合は$\\\\mathbf{x} = (x_1, x_2,\\\\cdots, x_n)^\\\\top$のように表記する．\\n',\n",
       " '\\\\item 単位行列を$\\\\mathbf{I}$と表記する．$n \\\\times n$ 次元の単位行列は $\\\\mathbf{I}_n$ と表記する．\\n',\n",
       " '\\\\item ゼロベクトルは$\\\\mathbf{0}$, 要素が全て1のベクトルは$\\\\mathbf{1}$と表記する．\\n',\n",
       " '\\\\item ベクトル・行列の微分には分子レイアウト記法を使用する．\\n',\n",
       " '\\\\item 基本的に確率変数は大文字 $X$ のように表記し，確率変数の実現値は小文字 $x$ を用いる．ただし，大文字であっても確率変数でない場合や，実現値がベクトルの場合などがあるため，必ずしもこの規則に従うわけではない．\\n',\n",
       " '\\\\item $e$を自然対数の底とし，指数関数を$e^x=\\\\exp(x)$と表記する．また，自然対数を$\\\\ln(x)$と表記する．\\n',\n",
       " '\\\\item 定義を$\\\\coloneqq$を用いて行う．例えば，$f(x)\\\\coloneqq2x$ は $f(x)$ という関数を$2x$として定義するという意味である．定義する対象が右側である場合は，$\\\\eqqcolon$を用いる．\\n',\n",
       " '\\\\item 平均$\\\\mu$, 標準偏差$\\\\sigma$の正規分布を$\\\\mathcal{N}(\\\\mu, \\\\sigma^2)$と表記する．\\n',\n",
       " '\\\\end{itemize}\\n',\n",
       " '\\\\subsection{線形代数と微分}\\n',\n",
       " '\\\\textbf{線形代数 (Linear Algebra)}\\\\index{せんけいだいすう (Linear Algebra)@線形代数 (Linear Algebra)} は、ベクトルや行列といった線形構造を持つ対象の性質を解析する数学の分野であり、現代のあらゆる数学・工学・情報科学の基礎をなしている。線形代数の中心的な対象は、\\\\textbf{ベクトル空間}\\\\index{べくとるくうかん@ベクトル空間}、\\\\textbf{線形写像}\\\\index{せんけいしゃぞう@線形写像}、およびそれらの表現である\\\\textbf{行列}\\\\index{ぎょうれつ@行列}である。\\n',\n",
       " 'まず、\\\\textbf{ベクトル空間 (vector space)}\\\\index{べくとるくうかん (vector space)@ベクトル空間 (vector space)} とは、スカラー体（通常は実数$\\\\mathbb{R}$または複素数$\\\\mathbb{C}$）に対して定義された加法とスカラー倍という2つの演算に関して閉じている集合である。たとえば$\\\\mathbb{R}^n$は、$n$個の実数からなるベクトル全体の集合であり、典型的なベクトル空間の例である。任意のベクトル$\\\\mathbf{v}, \\\\mathbf{w} \\\\in \\\\mathbb{R}^n$とスカラー$\\\\alpha \\\\in \\\\mathbb{R}$に対して、\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\alpha\\\\mathbf{v} + \\\\mathbf{w} \\\\in \\\\mathbb{R}^n\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'が成り立つ。\\n',\n",
       " '\\\\textbf{線形写像 (linear transformation)}\\\\index{せんけいしゃぞう (linear transformation)@線形写像 (linear transformation)} とは、ベクトル空間からベクトル空間への写像$T: V \\\\to W$であり、加法とスカラー倍に対して線形性を持つもの、すなわち\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'T(\\\\alpha \\\\mathbf{v} + \\\\beta \\\\mathbf{w}) = \\\\alpha T(\\\\mathbf{v}) + \\\\beta T(\\\\mathbf{w})\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'が任意の$\\\\mathbf{v}, \\\\mathbf{w} \\\\in V$とスカラー$\\\\alpha, \\\\beta$に対して成り立つ写像である。\\n',\n",
       " 'このような線形写像は、基底を定めることで\\\\textbf{行列 (matrix)}\\\\index{ぎょうれつ (matrix)@行列 (matrix)} によって表現できる。たとえば、$n$次元から$m$次元への線形写像は、$m \\\\times n$の行列$A$を用いて\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbf{y} = A\\\\mathbf{x}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'という形で記述される。ここで$\\\\mathbf{x} \\\\in \\\\mathbb{R}^n$,$\\\\mathbf{y} \\\\in \\\\mathbb{R}^m$はそれぞれ入力および出力ベクトルである。\\n',\n",
       " '\\\\textbf{行列の積}\\\\index{ぎょうれつのせき@行列の積}：$A \\\\in \\\\mathbb{R}^{m \\\\times n}, B \\\\in \\\\mathbb{R}^{n \\\\times p}$に対し、$AB \\\\in \\\\mathbb{R}^{m \\\\times p}$を定義。\\n',\n",
       " '\\\\textbf{転置}\\\\index{てんち@転置}：$A^\\\\top$は行列$A$の行と列を交換したもの。\\n',\n",
       " '\\\\textbf{逆行列}\\\\index{ぎゃくぎょうれつ@逆行列}：$A \\\\in \\\\mathbb{R}^{n \\\\times n}$が正則（可逆）であれば、$A^{-1}$が存在し$AA^{-1} = A^{-1}A = I$を満たす。\\n',\n",
       " '\\\\textbf{行列式}\\\\index{ぎょうれつしき@行列式}（determinant）：$\\\\det A$は正方行列$A$に対するスカラー量で、行列の体積のスケーリング率や可逆性の指標となる。\\n',\n",
       " '\\\\textbf{線形方程式系}\\\\index{せんけいほうていしきけい@線形方程式系}の解法である。$A\\\\mathbf{x} = \\\\mathbf{b}$の形をした方程式において、$A$の逆行列が存在するならば、その解は\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbf{x} = A^{-1}\\\\mathbf{b}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'と求められる。\\n',\n",
       " '\\\\textbf{固有値問題}\\\\index{こゆうちもんだい@固有値問題}\\n',\n",
       " 'ある正方行列$A$に対し、スカラー$\\\\lambda$およびベクトル$\\\\mathbf{v} \\\\ne \\\\mathbf{0}$が\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'A\\\\mathbf{v} = \\\\lambda \\\\mathbf{v}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'を満たすとき、$\\\\lambda$は$A$の\\\\textbf{固有値 (eigenvalue)}\\\\index{こゆうち (eigenvalue)@固有値 (eigenvalue)}、$\\\\mathbf{v}$は\\\\textbf{固有ベクトル (eigenvector)}\\\\index{こゆうべくとる (eigenvector)@固有ベクトル (eigenvector)} と呼ばれる。固有値分解や対角化は、線形変換の構造解析や行列の関数（例：指数関数）を考える際に中心的な役割を果たす。\\n',\n",
       " 'matrix cookbookに詳しいが，\\n',\n",
       " '\\\\subsection{ベクトル・行列の微分}\\n',\n",
       " '本書ではベクトルおよび行列の微分を多用する．これは成分ごとに記載するよりも，ベクトル・行列演算をコードに変換しやすいという実装上の利点があるためである．初めに注意したいこととして，ベクトル・行列の微分の記法には分子レイアウト記法 (numerator-layout notation) と分母レイアウト記法 (denominator-layout notation) の2種類が存在する．これらは，ベクトル関数やスカラー関数に対する微分の定義の仕方に違いがあり，特に勾配ベクトルの形（行ベクトルか列ベクトルか）や連鎖律の表記に影響を及ぼす．いずれが使用されているかは文献ごとにバラバラであり，中には両方の記法を採用している文献も存在する．本書では，本書では\\\\textbf{分子レイアウト記法}\\\\index{ぶんしれいあうときほう@分子レイアウト記法}を統一的に使用する．記法の例を記述するため，スカラー $x, y \\\\in \\\\mathbb{R}$, ベクトル $\\\\mathbf{x}=[x_i] \\\\in \\\\mathbb{R}^n, \\\\mathbf{y}=[y_j] \\\\in \\\\mathbb{R}^m$, 行列 $\\\\mathbf{A}=[a_{ij}] \\\\in \\\\mathbb{R}^{p \\\\times q}$ を使用する．分子（従属変数）と分母（独立変数）の組み合わせから，次の6通りの微分が定義される．\\n',\n",
       " '\\\\begin{align*}\\n',\n",
       " '\\\\begin{array}{c|c|c|c}\\n',\n",
       " '& \\\\text{スカラー} & \\\\text{ベクトル} & \\\\text{行列}\\\\\\\\\\n',\n",
       " '\\\\hline\\n',\n",
       " '\\\\text{スカラー} & \\\\frac{\\\\partial y}{\\\\partial x} & \\\\frac{\\\\partial \\\\mathbf{y}}{\\\\partial x} & \\\\frac{\\\\partial \\\\mathbf{A}}{\\\\partial x} \\\\\\\\\\n',\n",
       " '\\\\hline\\n',\n",
       " '\\\\text{ベクトル} & \\\\frac{\\\\partial y}{\\\\partial \\\\mathbf{x}} & \\\\frac{\\\\partial \\\\mathbf{y}}{\\\\partial \\\\mathbf{x}} &  \\\\\\\\\\n',\n",
       " '\\\\hline\\n',\n",
       " '\\\\text{行列} & \\\\frac{\\\\partial y}{\\\\partial \\\\mathbf{A}} & &  \\\\\\\\\\n',\n",
       " '\\\\hline\\n',\n",
       " '\\\\end{array}\\n',\n",
       " '\\\\end{align*}\\n',\n",
       " '行名が分子の変数の種類，列名が分母の変数の種類を表している．まず，スカラーで偏微分する場合は，\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " '\\\\dfrac{\\\\partial y}{\\\\partial x} \\\\in \\\\mathbb{R}, \\\\quad\\n',\n",
       " '\\\\frac{\\\\partial \\\\mathbf{y}}{\\\\partial x}\\\\coloneqq \\n',\n",
       " '\\\\begin{bmatrix}\\n',\n",
       " '\\\\frac{\\\\partial y_{1}}{\\\\partial x}\\\\\\\\\\n',\n",
       " '\\\\frac{\\\\partial y_{2}}{\\\\partial x}\\\\\\\\\\n',\n",
       " '\\\\vdots \\\\\\\\\\n',\n",
       " '\\\\frac{\\\\partial y_{m}}{\\\\partial x}\\\\\\\\\\n',\n",
       " '\\\\end{bmatrix}\\n',\n",
       " '\\\\in \\\\mathbb{R}^m, \\\\quad\\n',\n",
       " '\\\\frac{\\\\partial \\\\mathbf{A}}{\\\\partial x}\\\\coloneqq \\n',\n",
       " '\\\\begin{bmatrix}\\n',\n",
       " '\\\\frac{\\\\partial a_{11}}{\\\\partial x}&\\\\frac{\\\\partial a_{12}}{\\\\partial x}&\\\\cdots &\\\\frac{\\\\partial a_{1q}}{\\\\partial x}\\\\\\\\\\n',\n",
       " '\\\\frac{\\\\partial a_{21}}{\\\\partial x}&\\\\frac{\\\\partial a_{22}}{\\\\partial x}&\\\\cdots &\\\\frac{\\\\partial a_{2q}}{\\\\partial x}\\\\\\\\\\n',\n",
       " '\\\\vdots &\\\\vdots &\\\\ddots &\\\\vdots \\\\\\\\\\n',\n",
       " '\\\\frac{\\\\partial a_{p1}}{\\\\partial x}&\\\\frac{\\\\partial a_{p2}}{\\\\partial x}&\\\\cdots &\\\\frac{\\\\partial a_{pq}}{\\\\partial x}\\\\\\\\\\n',\n",
       " '\\\\end{bmatrix}\\n',\n",
       " '\\\\in \\\\mathbb{R}^{p \\\\times q}\\n',\n",
       " '\\\\end{align}\\n',\n",
       " 'である．次にベクトルで偏微分する場合，\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " '\\\\frac{\\\\partial y}{\\\\partial \\\\mathbf{x}}\\\\coloneqq \\n',\n",
       " '\\\\begin{bmatrix}\\n',\n",
       " '\\\\frac{\\\\partial y}{\\\\partial x_{1}}&\\\\frac{\\\\partial y}{\\\\partial x_{2}}&\\\\cdots &\\\\frac{\\\\partial y}{\\\\partial x_{n}}\\n',\n",
       " '\\\\end{bmatrix}\\n',\n",
       " '\\\\in \\\\mathbb{R}^{1\\\\times n}, \\\\quad\\n',\n",
       " '\\\\frac{\\\\partial \\\\mathbf{y}}{\\\\partial \\\\mathbf{x}}\\\\coloneqq \\n',\n",
       " '\\\\begin{bmatrix}\\n',\n",
       " '\\\\frac{\\\\partial y_{1}}{\\\\partial x_{1}}&\\\\frac{\\\\partial y_{1}}{\\\\partial x_{2}}&\\\\cdots &\\\\frac{\\\\partial y_{1}}{\\\\partial x_{n}}\\\\\\\\\\n',\n",
       " '\\\\frac{\\\\partial y_{2}}{\\\\partial x_{1}}&\\\\frac{\\\\partial y_{2}}{\\\\partial x_{2}}&\\\\cdots &\\\\frac{\\\\partial y_{2}}{\\\\partial x_{n}}\\\\\\\\\\n',\n",
       " '\\\\vdots &\\\\vdots &\\\\ddots &\\\\vdots\\\\\\\\\\n',\n",
       " '\\\\frac{\\\\partial y_{m}}{\\\\partial x_{1}}&\\\\frac{\\\\partial y_{m}}{\\\\partial x_{2}}&\\\\cdots &\\\\frac{\\\\partial y_{m}}{\\\\partial x_{n}}\\\\\\\\\\n',\n",
       " '\\\\end{bmatrix}\\n',\n",
       " '\\\\in \\\\mathbb{R}^{m \\\\times n}\\n',\n",
       " '\\\\end{align}\\n',\n",
       " 'である．ここで $\\\\nabla_\\\\mathbf{x} y(\\\\mathbf{x})\\\\coloneqq \\\\left(\\\\frac{\\\\partial y}{\\\\partial \\\\mathbf{x}}\\\\right)^\\\\top$ は $y$ に対する $\\\\mathbf{x}$ の勾配 (gradient) と呼ばれる．また，$\\\\frac{\\\\partial \\\\mathbf{y}}{\\\\partial \\\\mathbf{x}}$ は $\\\\mathbf{y}$ に対する $\\\\mathbf{x}$ のJacobian行列と呼ばれる．最後に，行列で偏微分する場合，\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " '\\\\frac{\\\\partial y}{\\\\partial \\\\mathbf{A}}=\\n',\n",
       " '\\\\begin{bmatrix}\\n',\n",
       " '\\\\frac{\\\\partial y}{\\\\partial a_{11}}&\\\\frac{\\\\partial y}{\\\\partial a_{21}}&\\\\cdots &{\\\\frac{\\\\partial y}{\\\\partial a_{p1}}}\\\\\\\\\\n',\n",
       " '\\\\frac{\\\\partial y}{\\\\partial a_{12}}&\\\\frac{\\\\partial y}{\\\\partial a_{22}}&\\\\cdots &{\\\\frac{\\\\partial y}{\\\\partial a_{p2}}}\\\\\\\\\\n',\n",
       " '\\\\vdots &\\\\vdots &\\\\ddots &\\\\vdots \\\\\\\\\\n',\n",
       " '\\\\frac{\\\\partial y}{\\\\partial a_{1q}}&\\\\frac{\\\\partial y}{\\\\partial a_{2q}}&\\\\cdots &{\\\\frac{\\\\partial y}{\\\\partial a_{pq}}}\\\\\\\\\\n',\n",
       " '\\\\end{bmatrix}\\n',\n",
       " '\\\\in \\\\mathbb{R}^{q \\\\times p}\\n',\n",
       " '\\\\end{align}\\n',\n",
       " 'である．これは $\\\\nabla_\\\\mathbf{A} y(\\\\mathbf{A})\\\\coloneqq  (\\\\frac{\\\\partial y}{\\\\partial \\\\mathbf{A}})^\\\\top$ とも表記する．\\n',\n",
       " '\\\\subsection{微分方程式}\\n',\n",
       " '微分方程式はある関数とそれを微分した導関数の関係式であり，関数の特定の変数に対する変化を記述することができる．まず，1階線形微分方程式を例として見てみよう．\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\frac{dx(t)}{dt}=a_c x(t)+b_c u(t)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " '状態変数$x(t)$は，時間$t$に対する関数である．\\n',\n",
       " '添え字の$c$は連続 (continuous) を意味するが，これは後で離散化する際に区別するためである．この方程式においては$b_c=0$の場合を\\\\textbf{同次方程式}\\\\index{どうじほうていしき@同次方程式},$b_c\\\\neq 0$の場合を\\\\textbf{非同次方程式}\\\\index{ひどうじほうていしき@非同次方程式}という．\\n',\n",
       " '\\\\subsection{微分方程式の解}\\n',\n",
       " '微分方程式を解くとは$x(t)$のような関数の具体的な式を求めることである．上式の解は\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'x(t)=e^{a_c t}x(0)+\\\\int_0^t e^{a_c (t-\\\\tau)}b_c u(\\\\tau) d\\\\tau\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'として与えられる．微分方程式を解く手法は様々で，それぞれの方程式について適切な手法を選択する．本書ではLaplace変換を多用するが，細かい説明は付録にて行う．\\n',\n",
       " '\\\\subsection{連立線形微分方程式}\\n',\n",
       " '$n$個の微分方程式\\n',\n",
       " '連立線形微分方程式という．これをベクトル，行列を用いて\\n',\n",
       " '時不変 (time-invariant) の定数行列を$\\\\mathbf{A}_c \\\\in \\\\mathbb{R}^{n\\\\times n}, \\\\mathbf{B}_c \\\\in \\\\mathbb{R}^{n\\\\times m}$, 状態ベクトルを$\\\\mathbf{x}(t)\\\\in\\\\mathbb{R}^n$, 入力ベクトルを$\\\\mathbf{u}(t)\\\\in\\\\mathbb{R}^m$とする．\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\frac{d\\\\mathbf{x}(t)}{dt} = \\\\mathbf{A}_c\\\\mathbf{x}(t) + \\\\mathbf{B}_c\\\\mathbf{u}(t)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " '解は\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbf{x}(t)=e^{t\\\\mathbf{A}_c}\\\\mathbf{x}(0)+\\\\int_0^t e^{(t-\\\\tau)\\\\mathbf{A}_c}\\\\mathbf{B}_c\\\\mathbf{u}(\\\\tau) d\\\\tau\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " '\\\\subsection{Laplace変換}\\n',\n",
       " '\\\\textbf{Laplace変換}\\\\index{Laplaceへんかん@Laplace変換}は、与えられた時間領域の関数$f(t)$を複素数変数$s$の関数$F(s)$に写像する積分変換である。特に、線形微分方程式の解析や制御工学において非常に有効な手法であり、Fourier変換と密接な関係をもつ。\\n',\n",
       " 'Laplace変換は、実時間領域$t \\\\ge 0$上で定義された関数$f(t)$に対して、以下のように定義される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'F(s) \\\\coloneqq \\\\mathscr{L}(f(t)) = \\\\int_0^{\\\\infty} f(t)\\\\, e^{-st} dt\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで$s \\\\in \\\\mathbb{C}$は複素数変数であり、通常は$s = \\\\sigma + i\\\\omega$の形をとる。変換核$e^{-st}$を掛けて積分することにより、関数$f(t)$の無限大での振る舞いを抑制し、積分を収束させる効果を持つ。特に、$f(t)$が指数関数的増加を含む場合でも、$e^{-st}$による減衰によってその成分を抑えることが可能となる。\\n',\n",
       " 'Laplace変換の大きな利点の一つは、\\\\textbf{微分演算を代数演算に変換できる}\\\\index{びぶんえんざんをだいすうえんざんにへんかんできる@微分演算を代数演算に変換できる}という性質にある。すなわち、$f(t)$の微分$\\\\frac{d}{dt}f(t)$に対するLaplace変換は、次のように与えられる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathscr{L}\\\\left(\\\\frac{df}{dt}\\\\right) = sF(s) - f(0)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この性質により、常微分方程式はLaplace変換の下で代数方程式に変換され、解の導出が容易となる。初期値を含んだ微分方程式を直接的に解くことができるため、初期値問題への応用にも適している。\\n',\n",
       " 'さらに、Laplace変換には線形性：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathscr{L}(af(t) + bg(t)) = a\\\\mathscr{L}(f(t)) + b\\\\mathscr{L}(g(t))\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'および畳み込みに関する定理：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathscr{L}(f * g)(t) = F(s)G(s)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'など、多くの有用な性質がある。これにより、時間領域での複雑な演算が周波数領域で簡単な演算として扱えるようになる．\\n',\n",
       " '\\\\subsection{1階線形行列微分方程式の解}\\n',\n",
       " '時不変 (time-invariant) の定数行列を$\\\\mathbf{A} \\\\in \\\\mathbb{R}^{n\\\\times n}, \\\\mathbf{B} \\\\in \\\\mathbb{R}^{n\\\\times m}$, 状態ベクトルを$\\\\mathbf{x}(t)\\\\in\\\\mathbb{R}^n$, 入力ベクトルを$\\\\mathbf{u}(t)\\\\in\\\\mathbb{R}^m$とする．\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\frac{d\\\\mathbf{x}(t)}{dt} = \\\\mathbf{A}\\\\mathbf{x}(t) + \\\\mathbf{B}\\\\mathbf{u}(t)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この線形行列微分方程式をLaplace変換$\\\\mathscr{L}$を用いて解こう．$\\\\boldsymbol{X}(s) \\\\coloneqq \\\\mathscr{L}(\\\\mathbf{x}(t)), \\\\boldsymbol{U}(s) \\\\coloneqq \\\\mathscr{L}(\\\\mathbf{u}(t))$とすると，\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " 's\\\\boldsymbol{X}(s) - \\\\mathbf{x}(0) &= \\\\mathbf{A}\\\\boldsymbol{X}(s)+ \\\\mathbf{B}\\\\boldsymbol{U}(s)\\\\\\\\\\n',\n",
       " '(s\\\\mathbf{I} - \\\\mathbf{A}) \\\\boldsymbol{X}(s) &= \\\\mathbf{x}(0) + \\\\mathbf{B}\\\\boldsymbol{U}(s)\\\\\\\\\\n',\n",
       " '\\\\boldsymbol{X}(s) &= (s\\\\mathbf{I} - \\\\mathbf{A})^{-1}(\\\\mathbf{x}(0) + \\\\mathbf{B}\\\\boldsymbol{U}(s))\\\\\\\\\\n',\n",
       " '\\\\end{align}\\n',\n",
       " '行列指数関数 (matrix exponential)は\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'e^\\\\mathbf{A} = \\\\exp(\\\\mathbf{A}) \\\\coloneqq \\\\sum_{k=0}^\\\\infty \\\\frac{1}{k!}\\\\mathbf{A}^k = \\\\mathbf{I}+\\\\mathbf{A}+\\\\frac{\\\\mathbf{A}^2}{2!}+\\\\cdots\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'として定義される．天下り的だが，\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " '\\\\mathscr{L}(e^{at})&=\\\\frac{1}{s-a}\\\\\\\\\\n',\n",
       " '\\\\mathscr{L}(e^{t\\\\mathbf{A}})&=(s\\\\mathbf{I} - \\\\mathbf{A})^{-1}\\\\\\\\\\n',\n",
       " '\\\\end{align}\\n',\n",
       " 'であるので．よって\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " '\\\\boldsymbol{X}(s) &= (s\\\\mathbf{I} - \\\\mathbf{A})^{-1}(\\\\mathbf{x}(0) + \\\\mathbf{B}\\\\boldsymbol{U}(s))\\\\\\\\\\n',\n",
       " '&= (s\\\\mathbf{I} - \\\\mathbf{A})^{-1}\\\\mathbf{x}(0) + (s\\\\mathbf{I} - \\\\mathbf{A})^{-1}\\\\mathbf{B}\\\\boldsymbol{U}(s)\\\\\\\\\\n',\n",
       " '\\\\mathbf{x}(t)&=e^{t\\\\mathbf{A}}\\\\mathbf{x}(0)+\\\\int_0^t e^{(t-\\\\tau)\\\\mathbf{A}}\\\\mathbf{B}\\\\mathbf{u}(\\\\tau) d\\\\tau\\n',\n",
       " '\\\\end{align}\\n',\n",
       " 'となる．最後の式は両辺を逆Laplace変換した．ここで，$\\\\mathscr{L}^{-1}(F(s)G(s))=\\\\int_0^tf(\\\\tau)g(t-\\\\tau)d\\\\tau$であることを用いた．区間$[t, t+\\\\Delta t]$において入力$\\\\mathbf{u}(t)$が一定であると仮定すると，\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " '\\\\mathbf{x}(t+\\\\Delta t)&=e^{(t+\\\\Delta t)\\\\mathbf{A}}\\\\mathbf{x}(0)+\\\\int_0^{t+\\\\Delta t} e^{(t+\\\\Delta t-\\\\tau)\\\\mathbf{A}}\\\\mathbf{B}\\\\mathbf{u}(\\\\tau) d\\\\tau\\\\\\\\\\n',\n",
       " '&=e^{\\\\Delta t\\\\mathbf{A}}e^{t\\\\mathbf{A}}\\\\mathbf{x}(0)+e^{\\\\Delta t\\\\mathbf{A}}\\\\int_0^{t} e^{(t-\\\\tau)\\\\mathbf{A}}\\\\mathbf{B}\\\\mathbf{u}(\\\\tau) d\\\\tau + \\\\int_t^{t+\\\\Delta t} e^{(t+\\\\Delta t-\\\\tau)\\\\mathbf{A}}\\\\mathbf{B}\\\\mathbf{u}(\\\\tau) d\\\\tau\\\\\\\\\\n',\n",
       " '&\\\\approx \\\\underbrace{e^{\\\\Delta t\\\\mathbf{A}}}_{\\\\eqqcolon  \\\\mathbf{A}_d}\\\\mathbf{x}(t)+\\\\underbrace{\\\\left[\\\\int_t^{t+\\\\Delta t} e^{(t+\\\\Delta t-\\\\tau)\\\\mathbf{A}} d\\\\tau\\\\right] \\\\mathbf{B}}_{\\\\eqqcolon  \\\\mathbf{B}_d}\\\\mathbf{u}(t)\\\\\\\\\\n',\n",
       " '&=\\\\mathbf{A}_d\\\\mathbf{x}(t)+\\\\mathbf{B}_d\\\\mathbf{u}(t)\\\\\\\\\\n',\n",
       " '\\\\end{align}\\n',\n",
       " 'となる．添え字の$d$は離散化(discretization)を意味する．$\\\\mathbf{A}_c$が正則行列の場合，\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " '\\\\mathbf{B}_d &= \\\\left[\\\\int_t^{t+\\\\Delta t} e^{(t+\\\\Delta t-\\\\tau)\\\\mathbf{A}} d\\\\tau\\\\right] \\\\mathbf{B}\\\\\\\\\\n',\n",
       " '&=\\\\mathbf{A}^{-1}\\\\left[e^{\\\\Delta t \\\\mathbf{A}}-\\\\mathbf{I}\\\\right]\\\\mathbf{B}\\n',\n",
       " '\\\\end{align}\\n',\n",
       " 'が成り立つ．\\n',\n",
       " '\\\\subsection{確率論}\\n',\n",
       " '確率論の基本的な対象は\\\\textbf{確率分布 (probability distribution)}\\\\index{かくりつぶんぷ (probability distribution)@確率分布 (probability distribution)} である。確率分布は、ある確率変数がどのような値をどの程度の確率でとるかを定量的に記述するものである。確率変数$x$は、離散的あるいは連続的な値をとる場合があり、それぞれに応じて確率分布の定義も異なる。\\n',\n",
       " '離散的な場合、確率分布は\\\\textbf{確率質量関数}\\\\index{かくりつしつりょうかんすう@確率質量関数} (probability mass function; PMF) により定義され、任意の値$x$に対して$p(x)$はその値が観測される確率を与える。このとき、全ての確率の総和は 1 に等しくなければならない：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\sum_x p(x) = 1\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この代表例として\\\\textbf{ポアソン分布 (Poisson distribution)}\\\\index{ぽあそんぶんぷ (Poisson distribution)@ポアソン分布 (Poisson distribution)} がある。ポアソン分布は、ある固定時間・空間内における稀な離散事象の発生回数をモデル化するものであり、以下のように定義される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'p(x) = \\\\frac{\\\\lambda^x e^{-\\\\lambda}}{x!}, \\\\quad x = 0,1,2,\\\\dots\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで$\\\\lambda > 0$は単位時間（または空間）あたりの平均発生回数を表す。この分布は事象が独立かつ一定の発生率で起きると仮定する場面で用いられる。\\n',\n",
       " '一方、連続的な場合には\\\\textbf{確率密度関数}\\\\index{かくりつみつどかんすう@確率密度関数} (probability density function; PDF) を用いて定義される。確率密度関数$p(x)$は特定の値における確率そのものではなく、ある範囲に入る確率を積分によって与える関数である。たとえば、区間$[a,b]$における確率は次のように表される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbb{P}(a \\\\leq x \\\\leq b) = \\\\int_a^b p(x)\\\\,dx\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " '確率密度関数もまた、定義域全体にわたる積分が 1 でなければならない：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\int p(x)\\\\,dx = 1\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この典型例として\\\\textbf{正規分布 (normal distribution)}\\\\index{せいきぶんぷ (normal distribution)@正規分布 (normal distribution)} が挙げられる。正規分布は、多くの自然現象や測定誤差の分布を記述するのに適しており、平均$\\\\mu$、分散$\\\\sigma^2$をパラメータとして次のように定義される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'p(x) = \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma^2}} \\\\exp\\\\left( -\\\\frac{(x - \\\\mu)^2}{2\\\\sigma^2} \\\\right)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この分布は平均値$\\\\mu$を中心に左右対称であり、確率変数の分布が「中央に集まり、端にいくほど稀になる」という性質を持つ。特に$\\\\mu = 0$,$\\\\sigma^2 = 1$の場合は標準正規分布と呼ばれる．また，正規分布の概念は一変数の場合に限らず、多次元の確率変数にも拡張される。これが\\\\textbf{多変量正規分布 (multivariate normal distribution)}\\\\index{たへんりょうせいきぶんぷ (multivariate normal distribution)@多変量正規分布 (multivariate normal distribution)} であり、ベクトル値の確率変数がとる値の分布を記述する。\\n',\n",
       " '$d$次元の確率変数$\\\\mathbf{x} \\\\in \\\\mathbb{R}^d$が平均ベクトル$\\\\boldsymbol{\\\\mu} \\\\in \\\\mathbb{R}^d$、共分散行列$\\\\boldsymbol{\\\\Sigma} \\\\in \\\\mathbb{R}^{d \\\\times d}$をもつ多変量正規分布に従うとき、その確率密度関数は以下のように定義される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'p(\\\\mathbf{x}) = \\\\frac{1}{(2\\\\pi)^{d/2} \\\\det(\\\\boldsymbol{\\\\Sigma})^{1/2}} \\\\exp\\\\left( -\\\\frac{1}{2} (\\\\mathbf{x} - \\\\boldsymbol{\\\\mu})^\\\\top \\\\boldsymbol{\\\\Sigma}^{-1} (\\\\mathbf{x} - \\\\boldsymbol{\\\\mu}) \\\\right)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで、$\\\\mathbf{x}$は$d$次元の確率変数ベクトル、$\\\\boldsymbol{\\\\mu}$は平均ベクトルであり、各成分が$\\\\mathbf{x}$の平均値，$\\\\boldsymbol{\\\\Sigma}$は対象対称かつ正定値な共分散行列であり、成分$\\\\Sigma_{ij}$は$\\\\mathrm{Cov}(x_i, x_j)$を表す．\\n',\n",
       " 'この分布は、各変数が正規分布に従い、かつそれらの間の線形な関係（共分散）もモデル化できる点で、非常に広範に用いられる。特に共分散行列が対角行列のとき、すなわち変数間が独立な場合には、各変数は独立な一変量正規分布に従う。\\n',\n",
       " '確率論においては、不確実性を定量的に扱うための基本的な概念がいくつか存在する。以下では、期待値、情報量、エントロピー、Kullback-Leibler情報量、そして相互情報量について簡単に説明を行う。\\n',\n",
       " 'まず、\\\\textbf{期待値 (Expectation)}\\\\index{きたいち (Expectation)@期待値 (Expectation)} は、確率変数$x$に関する関数$f(x)$の平均値を、$x$の確率分布$p(x)$に基づいて計算する操作である。連続値の場合、期待値は次のように定義される。\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbb{E}_{x\\\\sim p(x)}\\\\left[f(x)\\\\right] \\\\coloneqq \\\\int f(x)p(x)\\\\,dx\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで$x \\\\sim p(x)$は、$x$が分布$p(x)$に従うことを表す。文脈が明確な場合には、簡略に$\\\\mathbb{E}_{p(x)}[f(x)]$や$\\\\mathbb{E}[f(x)]$と表記する。\\n',\n",
       " '次に、\\\\textbf{情報量 (Information)}\\\\index{じょうほうりょう (Information)@情報量 (Information)} は、ある特定の事象$x$の出現がどれほどの「驚き」や「情報」をもたらすかを定量化するものである。情報理論の創始者であるShannon (1948) によって導入された。出現確率が低い事象ほど、多くの情報を含むと考えられる。情報量は次のように定義される。\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbb{I}(x) \\\\coloneqq \\\\ln\\\\left(\\\\frac{1}{p(x)}\\\\right) = -\\\\ln p(x)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " '\\\\textbf{エントロピー (Entropy)}\\\\index{えんとろぴー (Entropy)@エントロピー (Entropy)} は、確率変数の持つ平均的な不確実性、すなわち平均情報量を表す。離散的な場合には和を、連続的な場合には積分を用いて定義されるが、ここでは連続的な場合を考える。エントロピーは以下のように定義される。\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbb{H}(x) \\\\coloneqq \\\\mathbb{E}[-\\\\ln p(x)] = -\\\\int p(x) \\\\ln p(x)\\\\,dx\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'また、条件付きエントロピー$\\\\mathbb{H}(x|y)$は、$y$が与えられたときの$x$の不確実性を測る指標であり、次のように定義される。\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbb{H}(x \\\\vert y) \\\\coloneqq \\\\mathbb{E}_{x,y}[-\\\\ln p(x \\\\vert y)]\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この期待値は、$p(x,y)$に基づいて計算される。\\n',\n",
       " '次に、\\\\textbf{Kullback-Leibler情報量 (KL divergence)}\\\\index{Kullback-Leiblerじょうほうりょう (KL divergence)@Kullback-Leibler情報量 (KL divergence)} は、ある確率分布$p(x)$と別の分布$q(x)$の間の「距離」あるいは「ずれ」を測る尺度である。対称性は持たないため、厳密には距離ではないが、情報理論や機械学習において極めて重要な概念である。KLダイバージェンスは以下のように定義される。\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " 'D_{\\\\text{KL}}(p(x)\\\\Vert q(x)) &\\\\coloneqq \\\\int p(x) \\\\ln \\\\frac{p(x)}{q(x)} dx \\\\\\\\\\n',\n",
       " '&= \\\\int p(x) \\\\ln p(x)\\\\,dx - \\\\int p(x) \\\\ln q(x)\\\\,dx \\\\\\\\\\n',\n",
       " '&= \\\\mathbb{E}_{x\\\\sim p(x)}[\\\\ln p(x)] - \\\\mathbb{E}_{x\\\\sim p(x)}[\\\\ln q(x)] \\\\\\\\\\n',\n",
       " '&= -\\\\mathbb{H}(x) - \\\\mathbb{E}_{x\\\\sim p(x)}[\\\\ln q(x)]\\n',\n",
       " '\\\\end{align}\\n',\n",
       " '最後に、\\\\textbf{相互情報量 (Mutual Information)}\\\\index{そうごじょうほうりょう (Mutual Information)@相互情報量 (Mutual Information)} は、二つの確率変数$x$と$y$の間にどれほどの情報的関連性があるか、すなわち$y$を知ることによって$x$の不確実性がどれほど減少するかを定量化する。相互情報量は、エントロピーの差として次のように定義される。\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbb{I}(x;y) \\\\coloneqq \\\\mathbb{H}(x) - \\\\mathbb{H}(x\\\\vert y)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'これはまた、対称的な形でも書ける。\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbb{I}(x;y) = \\\\mathbb{H}(x) + \\\\mathbb{H}(y) - \\\\mathbb{H}(x,y)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'あるいは、確率分布の比を使って次のようにも表現される。\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbb{I}(x;y) = \\\\int p(x,y) \\\\ln \\\\frac{p(x,y)}{p(x)p(y)} dxdy\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この表現は、相互情報量が、$p(x,y)$と$p(x)p(y)$のKLダイバージェンスであることを示しており、すなわち独立であれば情報共有はゼロであることを意味する。\\n',\n",
       " '\\\\subsection{確率過程}\\n',\n",
       " '\\\\textbf{確率過程}\\\\index{かくりつかてい@確率過程}（stochastic process）とは、時間とともに変化する確率変数の集まりを指す。日常においても、株価の変動、気温の変化、人の行動など、未来の状態が確実には予測できず、ある程度の不確実性を含む現象は数多く存在する。これらの確率的な時間変化を数学的に扱う枠組みが確率過程である。\\n',\n",
       " '形式的には、確率過程とは、ある時間 $t$ における確率変数$X_t$の集まり$\\\\{X_t\\\\}_{t \\\\in \\\\mathcal{T}}$のことである。ここで、$\\\\mathcal{T}$は時間を表す集，たとえば$\\\\mathcal{T} = \\\\{0,1,2,\\\\dots\\\\}$や $\\\\mathcal{T} = [0,\\\\infty)$ であり，各$X_t$はある量（たとえば位置や価格など）を表す確率変数である。\\n',\n",
       " '\\\\textbf{Markov過程}\\\\index{Markovかてい@Markov過程}（Markov process）は、確率過程の中でも特に重要なクラスに属する。最大の特徴は、「現在の状態が分かれば、未来の状態の予測に過去の情報（履歴）は不要である」という性質、すなわち\\\\textbf{Markov性}\\\\index{Markovせい@Markov性}を持つことである。\\n',\n",
       " 'この性質は次のように定式化される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'P(X_{t+1} = x \\\\mid X_t = x_t, X_{t-1} = x_{t-1}, \\\\dots, X_0 = x_0) = P(X_{t+1} = x \\\\mid X_t = x_t)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'すなわち、未来の状態$X_{t+1}$の確率は、現在の状態$X_t$のみに依存し、それ以前の状態には依存しない。これにより、Markov過程は過去の情報を逐一保持する必要がなく、解析やシミュレーションが容易になるという利点を持つ。\\n',\n",
       " '\\\\textbf{Wiener過程}\\\\index{Wienerかてい@Wiener過程} (Wiener process) は、連続時間Markov過程の代表例であり、ブラウン運動とも呼ばれる。微粒子が液体中で不規則に運動する現象（ブラウン運動）を数学的に記述するために導入されたものであるが、現在では金融や情報理論をはじめとした多くの分野において基本的なモデルとなっている。\\n',\n",
       " 'Wiener過程$\\\\{W_t\\\\}_{t \\\\ge 0}$は、以下の性質を満たす確率過程である：\\n',\n",
       " '\\\\begin{itemize}\\n',\n",
       " '\\\\item 初期値が 0 である：$W_0 = 0$。\\n',\n",
       " '\\\\item 増分の独立性：任意の$0 \\\\le t_1 < t_2 < \\\\dots < t_n$に対し、各増分$W_{t_{i+1}} - W_{t_i}$は互いに独立である。\\n',\n",
       " '\\\\item 増分の分布が正規分布に従う：任意の$s < t$に対して、増分$W_t - W_s$は平均 0、分散$t - s$の正規分布$N(0, t - s)$に従う。\\n',\n",
       " '\\\\item パスが連続である：時間$t$に対する関数$W_t$は、ほとんど確実に連続である。\\n',\n",
       " '\\\\end{itemize}\\n',\n",
       " 'このような性質により、Wiener過程は時間的に連続かつ不規則に変化するランダムな運動を表現することができる。また、Wiener過程はガウス過程でもあり、その共分散関数は$\\\\mathbb{E}[W_s W_t] = \\\\min(s, t)$で与えられる。\\n',\n",
       " 'さらに、Wiener過程はスケーリングの性質も持つ。すなわち、任意の$c > 0$に対して、$\\\\{\\\\sqrt{c}W_{t/c}\\\\}_{t \\\\ge 0}$もまたWiener過程である。このような対称性により、Wiener過程は確率解析における中心的な対象として位置づけられている。\\n',\n",
       " '以上のように、確率過程はランダムな時間変化を記述するための基本概念であり、Markov過程やWiener過程はその中でも特に重要な例である。それぞれの性質を理解することは、確率論や統計物理、数理ファイナンス、機械学習などの応用分野において基礎的かつ不可欠である。\\n',\n",
       " '\\\\subsection{確率微分方程式}\\n',\n",
       " '神経活動を含む生体活動には様々なゆらぎ（ノイズ）が常に存在しており、神経モデルにおいてもこれを考慮する必要がある。神経活動のダイナミクスを連続時間で記述する際には，決定論的な時間変化に加えて確率的なゆらぎ（ノイズ）を含む微分方程式，すなわち確率微分方程式（Stochastic Differential Equation; SDE）を用いることがある。SDEの一般的な形は以下のように与えられる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'dX(t) = f(X(t), t)\\\\,dt + g(X(t), t)\\\\,dW(t)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで、$f(X(t), t)$はドリフト項と呼ばれる決定論的な変化、$g(X(t), t)$は拡散項と呼ばれるノイズの強度を表す関数、$W(t)$は標準ブラウン運動（Wiener過程）である。$W(t)$は連続時間の確率過程であり、その増分$W(t + \\\\Delta t) - W(t)$は平均0、分散$\\\\Delta t$の正規分布$\\\\mathcal{N}(0, \\\\Delta t)$に従う。\\n',\n",
       " '\\\\begin{itemize}\\n',\n",
       " '\\\\item 離散モデルでのノイズ$\\\\mathbf{w}_t$,$\\\\mathbf{v}_t$は各離散時刻ごとにガウス分布から独立にサンプルされるもので、各離散時点に明示的に加えられる雑音です。\\n',\n",
       " '\\\\item 一方、連続モデルのノイズ$d\\\\mathbf{w}(t), d\\\\mathbf{v}(t)$はブラウン運動の微小増分を表しており、連続時間での微小な変動をモデル化しています。\\n',\n",
       " '\\\\item このように連続時間モデルと離散時間モデルは形式上対応しています。離散化する際には通常、\\n',\n",
       " '\\\\end{itemize}\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbf{w}_{t} \\\\approx \\\\int_{t}^{t+\\\\Delta t} d\\\\mathbf{w}(s),\\\\quad\\n',\n",
       " '\\\\mathbf{v}_{t} \\\\approx \\\\int_{t}^{t+\\\\Delta t} d\\\\mathbf{v}(s)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'という対応を用いて導出します。\\n',\n",
       " '神経活動には\\\\textbf{ノイズ}\\\\index{のいず@ノイズ}（neuronal noise）が常に存在しており、神経モデルにおいてもこれを考慮する必要がある。そのため、シナプス入力にノイズを加えることがある。たとえば、Leaky Integrate-and-Fire（LIF）モデルにおける膜電位の力学にノイズを加える場合を考える。ノイズ$\\\\xi(t)$を平均$\\\\tilde{\\\\mu}$、分散$\\\\tilde{\\\\sigma}^2$の正規分布$\\\\mathcal{N}(\\\\tilde{\\\\mu}, \\\\tilde{\\\\sigma}^2)$に従うガウシアンノイズとすると、膜電位$V_m(t)$の時間発展は次式で記述される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\tau_m \\\\frac{dV_m(t)}{dt} = -(V_m(t) - V_\\\\text{rest}) + R_m I(t) + \\\\xi(t)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'このように、線形のドリフト項$-(V_m(t) - V_\\\\text{rest})$とガウシアンノイズ項$\\\\xi(t)$を含む確率微分方程式（stochastic differential equation; SDE）で表される確率過程は、\\\\textbf{Ornstein–Uhlenbeck（OU）過程}\\\\index{Ornstein–Uhlenbeck（OU）かてい@Ornstein–Uhlenbeck（OU）過程} と呼ばれる。ノイズ$\\\\xi(t)$が標準正規分布$\\\\mathcal{N}(0, 1)$に従うホワイトノイズ$\\\\eta(t)$を用いて$\\\\xi(t) = \\\\tilde{\\\\mu} + \\\\tilde{\\\\sigma} \\\\eta(t)$と表すこともできる。\\n',\n",
       " 'さらに、$\\\\xi(t)$が発火率$\\\\lambda$のポアソン過程に従う場合を考える。シナプス前細胞の数を$N_\\\\text{pre}$、$i$番目のシナプスにおけるシナプス強度に比例する定数を$J_i$とすると、ノイズの平均と分散はそれぞれ$\\\\tilde{\\\\mu} = \\\\langle J_i \\\\rangle N_\\\\text{pre} \\\\cdot \\\\lambda$、$\\\\tilde{\\\\sigma}^2 = \\\\langle J_i^2 \\\\rangle N_\\\\text{pre} \\\\cdot \\\\lambda$と書ける。ただし、$\\\\langle \\\\cdot \\\\rangle$は平均を意味する。このような連続的なガウス過程でポアソン入力を近似する手法を\\\\textbf{拡散近似}\\\\index{かくさんきんじ@拡散近似}（diffusion approximation）と呼び、これは\\\\textbf{Campbellの定理}\\\\index{Campbellのていり@Campbellの定理}に基づいて導かれる。\\n',\n",
       " 'このような確率微分方程式を数値的にシミュレーションするためには、時間離散化が必要となるが、その際には注意が必要である。たとえば、ドリフト項を省略し、ノイズ項のみを残した場合、\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\tau_m \\\\frac{dV_m(t)}{dt} = \\\\xi(t)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'となる。この式を時間ステップ$\\\\Delta t$でEuler法により離散化すると、\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'V_m(t + \\\\Delta t) = V_m(t) + \\\\frac{1}{\\\\tau_m} \\\\xi_1(t)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'と書ける。ここで、時間ステップを$\\\\Delta t$から$\\\\Delta t/2$に変更して同様に離散化すると、\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " 'V_m(t + \\\\Delta t) &= V_m(t + \\\\Delta t/2) + \\\\frac{1}{\\\\tau_m} \\\\xi_1(t) \\\\\\\\\\n',\n",
       " '&= V_m(t) + \\\\frac{1}{\\\\tau_m} \\\\left[ \\\\xi_1(t) + \\\\xi_2(t) \\\\right]\\n',\n",
       " '\\\\end{align}\\n',\n",
       " 'となる。ノイズ項$\\\\xi_1(t)$と$\\\\xi_2(t)$は互いに独立と仮定すると、それぞれの標準偏差は$\\\\tilde{\\\\sigma}/\\\\tau_m$であり、その和$\\\\xi_1(t) + \\\\xi_2(t)$の分散は$2\\\\tilde{\\\\sigma}^2$、すなわち標準偏差は$\\\\sqrt{2} \\\\tilde{\\\\sigma}/\\\\tau_m$となる。これは時間ステップの取り方によってノイズ項の大きさが変化することを意味しており、正確なシミュレーションのためには問題となる。したがって、時間ステップに依存しないようノイズ項をスケーリングする必要があり、そのためにはノイズに$\\\\sqrt{\\\\Delta t}$を掛けることで対処できる。すなわち、離散化式は以下のように修正するのが望ましい：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'V_m(t + \\\\Delta t) = V_m(t) + \\\\frac{\\\\sqrt{\\\\Delta t}}{\\\\tau_m} \\\\xi_1(t)\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'このように修正することで、時間ステップに依存しない安定なノイズスケーリングが可能となる。このように確率微分方程式をEuler法で離散化する方法は、\\\\textbf{Euler–Maruyama法}\\\\index{Euler–Maruyamaほう@Euler–Maruyama法}と呼ばれる．他の離散化手法としては、Milstein法なども存在する。\\n',\n",
       " 'このようなSDEを解析的に解くことは一般に困難であるため、数値的な近似解法が必要となる。Euler–Maruyama法は、その最も基本的な手法の一つであり、常微分方程式に対するEuler法の自然な拡張である。時間を刻み幅$\\\\Delta t$で離散化し、$t_n = n \\\\Delta t$、$X_n \\\\approx X(t_n)$とおくと、Euler–Maruyama法は次のように与えられる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'X_{n+1} = X_n + f(X_n, t_n)\\\\, \\\\Delta t + g(X_n, t_n)\\\\, \\\\Delta W_n\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで$\\\\Delta W_n = W(t_{n+1}) - W(t_n)$は、正規分布$\\\\mathcal{N}(0, \\\\Delta t)$に従う確率変数である。実装上は、$\\\\Delta W_n$を標準正規分布$\\\\mathcal{N}(0, 1)$に従う独立な乱数$\\\\eta_n$を用いて$\\\\Delta W_n = \\\\sqrt{\\\\Delta t} \\\\cdot \\\\eta_n$と近似する。この結果、Euler–Maruyama法は以下のように書き換えられる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'X_{n+1} = X_n + f(X_n, t_n)\\\\, \\\\Delta t + g(X_n, t_n)\\\\, \\\\sqrt{\\\\Delta t} \\\\cdot \\\\eta_n\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この手法は簡便でありながら、確率過程の時間発展を模倣するのに有用であり、多くのシミュレーションにおいて第一選択となる。ただし、その強収束次数は$0.5$であり、すなわち刻み幅$\\\\Delta t$を小さくしても誤差は$\\\\mathcal{O}(\\\\sqrt{\\\\Delta t})$でしか減少しない。そのため、より高精度な数値解が必要な場合には、Milstein法などの高次手法の導入が検討される。Euler–Maruyama法は、その単純さと広範な適用性から、確率微分方程式の数値解析における基本的な出発点となる手法である。\\n',\n",
       " '\\\\section{学習に関する基礎的概念}\\n',\n",
       " '本書のテーマの1つとして「学習」が挙げられる．\\n',\n",
       " '神経科学における「学習」と機械学習における「学習」はやや異なるが，ここで両者における学習を定義しておく．\\n',\n",
       " '神経科学の学習は\\n',\n",
       " '共通する点として，過去の経験に基づいて，将来の行動や出力を改善するためにシステムを変化させる，という点で共通している．システムの\\n',\n",
       " 'システムのパラメータが変化する\\n',\n",
       " '神経科学：シナプス強度（重み）が変化する。\\n',\n",
       " '機械学習：ネットワークの重みやバイアスなどのパラメータが更新される。\\n',\n",
       " '異なる点として，\\n',\n",
       " '神経科学のモデルに機械学習\\n',\n",
       " '\\\\subsection{モデルと学習・予測}\\n',\n",
       " '\\\\textbf{機械学習}\\\\index{きかいがくしゅう@機械学習} (machine learning) における\\\\textbf{モデル}\\\\index{もでる@モデル} (model) とは，2つの集合$\\\\mathcal{X}, \\\\mathcal{Y}$を仮定した際に，入力$x\\\\in \\\\mathcal{X}$を出力$y\\\\in \\\\mathcal{Y}$に変換する関数 (写像)$f: x \\\\to y$あるいは条件付き確率分布$p(y|x)$を意味する．モデルは内部に媒介変数あるいはパラメータ (parameter)$\\\\theta$を持ち，$\\\\mathcal{Y}$を設定した後に$y=f(x; \\\\theta)$あるいは$p(y|x; \\\\theta)$を満たすように$\\\\theta$を更新する．この過程を\\\\textbf{学習}\\\\index{がくしゅう@学習} (learning) あるいは\\\\textbf{訓練}\\\\index{くんれん@訓練} (training) と呼ぶ．学習後のパラメータ$\\\\theta^*$を用い，$x$が与えられた際の$y$の推定値$\\\\hat{y}$を$\\\\hat{y}=f(x; \\\\theta^*)$あるいは$p(y|x; \\\\theta^*)$から取得することを\\\\textbf{予測}\\\\index{よそく@予測} (prediction) と呼ぶ．推定値の取得の方法としてはサンプリング$\\\\hat{y}\\\\sim p(y|x; \\\\theta^*)$や$\\\\hat{y}=\\\\textrm{argmax}\\\\ p(y|x; \\\\theta^{*})$などが考えられる．学習の際に用いられるデータを訓練データ (training data) と呼び，学習後のモデルの予測精度の評価に用いるデータを評価データ (test data) と呼ぶ．\\n',\n",
       " '$y$が既知の場合は$D=\\\\{(x,y)\\\\}$は教師付きデータ ($y$がラベルの場合はラベル付きデータ) と呼ばれ，$x$と$y$の対応関係を学習する過程を\\\\textbf{教師あり学習}\\\\index{きょうしありがくしゅう@教師あり学習} (supervised learning) と呼ぶ．$y$が未知の場合，$D=\\\\{x\\\\}$はラベルなしデータと呼ばれ，これのみでモデルを学習する過程を\\\\textbf{教師なし学習}\\\\index{きょうしなしがくしゅう@教師なし学習} (unsupervised learning) と呼ぶ．この2つの学習の派生として，ラベルあり・なしデータを併用する\\\\textbf{半教師あり学習}\\\\index{はんきょうしありがくしゅう@半教師あり学習} (semi-supervised learning) や, 教師なし学習の一種であり，入力データの部分集合から他の部分集合を予測する\\\\textbf{自己教師あり学習}\\\\index{じこきょうしありがくしゅう@自己教師あり学習} (self-supervised learning) などが存在する．この他の学習手法として\\\\textbf{強化学習}\\\\index{きょうかがくしゅう@強化学習} (reinforcement learning) があり，第11章で詳しく説明を行う．強化学習では環境の中で行動するエージェントを仮定し，状態に応じて多くの報酬を得るための行動を学習することが目的である．\\n',\n",
       " '\\\\subsection{回帰と分類}\\n',\n",
       " '\\\\subsection{識別モデル・生成モデル}\\n',\n",
       " 'オンライン・オフライン学習\\n',\n",
       " '\\\\section{線形回帰}\\n',\n",
       " '\\\\textbf{線形回帰モデル}\\\\index{せんけいかいきもでる@線形回帰モデル}（linear regression）は、与えられた説明変数（explanatory variable）$\\\\mathbf{x}$に基づいて、目的変数（objective variable）$y$を線形に予測することを目的とする。\\n',\n",
       " '説明変数の次元が$p$であるとき、線形回帰モデルは次のように表される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'y = w_0 + w_1x_1 + \\\\cdots + w_px_p + \\\\varepsilon = w_0 + \\\\sum_{j=1}^p w_j x_j + \\\\varepsilon\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで$w_0$は切片（バイアス項）、$w_1, \\\\dots, w_p$は各説明変数に対する重み、$\\\\varepsilon$は誤差項を表す。$p = 1$の場合を\\\\textbf{単回帰}\\\\index{たんかいき@単回帰}（*simple regression*）、$p > 1$の場合を\\\\textbf{重回帰}\\\\index{じゅうかいき@重回帰}（*multiple regression*）と呼ぶ。\\n',\n",
       " '\\\\subsection{回帰モデルの行列表現}\\n',\n",
       " '$n$個の観測データからなるデータセット$\\\\mathcal{D} = \\\\{(\\\\mathbf{x}^{(i)}, y^{(i)})\\\\}_{i=1}^n$を考える。ここで$\\\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, \\\\dots, x_p^{(i)}]^\\\\top \\\\in \\\\mathbb{R}^p$は$i$番目の説明変数ベクトル、$y^{(i)} \\\\in \\\\mathbb{R}$は対応する目的変数の値である。なお、添字$(i)$は観測値を表し、添字のない$x_j, w_j$などはモデル内の変数を指すことに注意する。\\n',\n",
       " 'このとき、モデル全体を行列の形で次のように記述できる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbf{y} = \\\\begin{bmatrix} y^{(1)} \\\\\\\\ y^{(2)} \\\\\\\\ \\\\vdots \\\\\\\\ y^{(n)} \\\\end{bmatrix} \\\\in \\\\mathbb{R}^n,\\\\quad\\n',\n",
       " '\\\\mathbf{X} = \\\\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\\\cdots & x_p^{(1)} \\\\\\\\\\n',\n",
       " '1 & x_1^{(2)} & x_2^{(2)} & \\\\cdots & x_p^{(2)} \\\\\\\\\\n',\n",
       " '\\\\vdots & \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\n',\n",
       " '1 & x_1^{(n)} & x_2^{(n)} & \\\\cdots & x_p^{(n)} \\\\end{bmatrix} \\\\in \\\\mathbb{R}^{n \\\\times (p+1)},\\\\quad\\n',\n",
       " '\\\\mathbf{w} = \\\\begin{bmatrix} w_0 \\\\\\\\ w_1 \\\\\\\\ \\\\vdots \\\\\\\\ w_p \\\\end{bmatrix} \\\\in \\\\mathbb{R}^{p+1}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'これにより、回帰モデルは次のように簡潔に表される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbf{y} = \\\\mathbf{X} \\\\mathbf{w} + \\\\boldsymbol{\\\\varepsilon}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで$\\\\mathbf{X}$は\\\\textbf{計画行列}\\\\index{けいかくぎょうれつ@計画行列}（design matrix）、$\\\\boldsymbol{\\\\varepsilon}$は誤差ベクトルである。特に、$\\\\boldsymbol{\\\\varepsilon} \\\\sim \\\\mathcal{N}(0, \\\\sigma^2 \\\\mathbf{I})$、すなわち各誤差成分が独立な平均0・分散$\\\\sigma^2$の正規分布に従うと仮定すれば、$\\\\mathbf{y} \\\\sim \\\\mathcal{N}(\\\\mathbf{X} \\\\mathbf{w}, \\\\sigma^2 \\\\mathbf{I})$という確率モデルが得られる。\\n',\n",
       " '\\\\subsection{最小二乗法}\\n',\n",
       " '\\\\textbf{最小二乗法}\\\\index{さいしょうじじょうほう@最小二乗法}（ordinary least squares, OLS）では、観測値$\\\\mathbf{y}$と予測値$\\\\mathbf{Xw}$との差（残差）を最小にするようにパラメータ$\\\\mathbf{w}$を推定する。残差ベクトル$\\\\boldsymbol{\\\\delta} = \\\\mathbf{y} - \\\\mathbf{Xw}$に対し、目的関数$\\\\mathcal{L}(\\\\mathbf{w})$は次のように定義される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathcal{L}(\\\\mathbf{w}) \\\\coloneqq \\\\|\\\\boldsymbol{\\\\delta}\\\\|^2 = \\\\boldsymbol{\\\\delta}^\\\\top \\\\boldsymbol{\\\\delta}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この$\\\\mathcal{L}(\\\\mathbf{w})$を最小化する$\\\\mathbf{w}$を求めることで、最適な重み$\\\\hat{\\\\mathbf{w}}$を得る。最適解の推定は主に\\\\textbf{正規方程式}\\\\index{せいきほうていしき@正規方程式}（normal equation）あるいは\\\\textbf{勾配法}\\\\index{こうばいほう@勾配法}（gradient descent）によって行うことができる．いずれの手法でも，目的関数$\\\\mathcal{L}(\\\\mathbf{w})$の$\\\\mathbf{w}$について微分、すなわち勾配 (gradient)$\\\\nabla \\\\mathcal{L}(\\\\mathbf{w})$が必要となる．$\\\\nabla \\\\mathcal{L}(\\\\mathbf{w})$は以下のように計算できる：\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " '\\\\nabla \\\\mathcal{L}(\\\\mathbf{w})\\n',\n",
       " '&= \\\\frac{\\\\partial}{\\\\partial \\\\mathbf{w}}\\\\left[(\\\\mathbf{y} - \\\\mathbf{Xw})^\\\\top (\\\\mathbf{y} - \\\\mathbf{Xw}) \\\\right] \\\\\\\\\\n',\n",
       " '&= \\\\frac{\\\\partial}{\\\\partial \\\\mathbf{w}} \\\\left( \\\\mathbf{y}^\\\\top \\\\mathbf{y} - 2 \\\\mathbf{y}^\\\\top \\\\mathbf{Xw} + \\\\mathbf{w}^\\\\top \\\\mathbf{X}^\\\\top \\\\mathbf{Xw} \\\\right) \\\\\\\\\\n',\n",
       " '&= -2 \\\\mathbf{X}^\\\\top \\\\mathbf{y} + 2 \\\\mathbf{X}^\\\\top \\\\mathbf{Xw}\\\\\\\\\\n',\n",
       " '&= -2\\\\mathbf{X}^\\\\top (\\\\mathbf{y} - \\\\mathbf{Xw})\\n',\n",
       " '\\\\end{align}\\n',\n",
       " '\\\\subsection{正規方程式による解析解}\\n',\n",
       " '目的関数の勾配について$\\\\nabla \\\\mathcal{L}(\\\\mathbf{w})=0$となる解を$\\\\hat{\\\\mathbf{w}}$とすると，次の\\\\textbf{正規方程式}\\\\index{せいきほうていしき@正規方程式}（normal equation）が得られる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbf{X}^\\\\top \\\\mathbf{X} \\\\hat{\\\\mathbf{w}} = \\\\mathbf{X}^\\\\top \\\\mathbf{y}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この方程式を解くことで、パラメータの推定値$\\\\hat{\\\\mathbf{w}}$は次のように求まる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\hat{\\\\mathbf{w}} = (\\\\mathbf{X}^\\\\top \\\\mathbf{X})^{-1} \\\\mathbf{X}^\\\\top \\\\mathbf{y}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'なお、$A^+ \\\\coloneqq (\\\\mathbf{X}^\\\\top \\\\mathbf{X})^{-1} \\\\mathbf{X}^\\\\top$は$\\\\mathbf{X}$の\\\\textbf{Moore–Penrose 擬似逆行列}\\\\index{Moore–Penrose ぎじぎゃくぎょうれつ@Moore–Penrose 擬似逆行列}（pseudoinverse）と呼ばれ、この表現を用いると$\\\\hat{\\\\mathbf{w}} = A^+ \\\\mathbf{y}$と簡潔に記述できる。\\n',\n",
       " '\\\\subsection{勾配法による数値的推定}\\n',\n",
       " '最小二乗法に基づくパラメータ推定は、数値的には\\\\textbf{勾配法}\\\\index{こうばいほう@勾配法}（gradient descent）によっても実現できる。 目的関数の勾配$\\\\nabla \\\\mathcal{L}(\\\\mathbf{w})$を用いると、更新式は次のように与えられる：\\n',\n",
       " '\\\\begin{align}\\n',\n",
       " '\\\\Delta \\\\mathbf{w} \\\\propto - \\\\nabla \\\\mathcal{L}(\\\\mathbf{w})= 2\\\\mathbf{X}^\\\\top (\\\\mathbf{y} - \\\\mathbf{Xw})\\\\\\\\\\n',\n",
       " '\\\\mathbf{w} \\\\leftarrow \\\\mathbf{w} + \\\\alpha \\\\cdot \\\\frac{1}{n} \\\\mathbf{X}^\\\\top (\\\\mathbf{y} - \\\\mathbf{Xw})\\n',\n",
       " '\\\\end{align}\\n',\n",
       " 'ここで$\\\\alpha$は\\\\textbf{学習率}\\\\index{がくしゅうりつ@学習率}（learning rate）と呼ばれるハイパーパラメータである。\\n',\n",
       " '\\\\subsection{リッジ回帰}\\n',\n",
       " '線形回帰においては、説明変数が高次元である場合や、多重共線性（説明変数間の相関）が存在する場合などに、最小二乗法による推定が不安定になることがある。これに対処する手法として、\\\\textbf{L2 正則化}\\\\index{L2 せいそくか@L2 正則化}を加えた\\\\textbf{リッジ回帰}\\\\index{りっじかいき@リッジ回帰}（ridge regression）が用いられる。\\n',\n",
       " 'リッジ回帰では、目的関数にパラメータの二乗ノルムを加えた正則化項を導入することにより、モデルの複雑さを抑制し、過学習の防止や推定の安定化を図る。具体的には、次のような正則化付き目的関数$\\\\mathcal{L}_\\\\lambda(\\\\mathbf{w})$を最小化する：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathcal{L}_\\\\lambda(\\\\mathbf{w}) = \\\\|\\\\mathbf{y} - \\\\mathbf{Xw}\\\\|^2 + \\\\lambda \\\\|\\\\mathbf{w}\\\\|^2 = (\\\\mathbf{y} - \\\\mathbf{Xw})^\\\\top (\\\\mathbf{y} - \\\\mathbf{Xw}) + \\\\lambda \\\\mathbf{w}^\\\\top \\\\mathbf{w},\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで$\\\\lambda \\\\geq 0$は\\\\textbf{正則化係数}\\\\index{せいそくかけいすう@正則化係数}（regularization parameter）であり、モデルのあてはまりと複雑さのトレードオフを制御する。なお通常、$w_0$（切片）には正則化を加えないことが多いため、必要に応じて$\\\\mathbf{w}$の対象を$[w_1, \\\\dots, w_p]^\\\\top$に限定する処理を行う。\\n',\n",
       " '\\\\subsection{正規方程式による解}\\n',\n",
       " 'L2 正則化付きの目的関数を$\\\\mathbf{w}$で微分して0に等しいとおくと、次のような修正された正規方程式が得られる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '(\\\\mathbf{X}^\\\\top \\\\mathbf{X} + \\\\lambda \\\\mathbf{I}) \\\\hat{\\\\mathbf{w}} = \\\\mathbf{X}^\\\\top \\\\mathbf{y},\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで$\\\\mathbf{I} \\\\in \\\\mathbb{R}^{(p+1)\\\\times(p+1)}$は単位行列である．ただし、$w_0$を正則化対象から除く場合、$\\\\lambda \\\\mathbf{I}$の最初の対角成分をゼロにすることで対処する。この式を解くと、リッジ回帰におけるパラメータの推定値は次のように求まる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\hat{\\\\mathbf{w}} = (\\\\mathbf{X}^\\\\top \\\\mathbf{X} + \\\\lambda \\\\mathbf{I})^{-1} \\\\mathbf{X}^\\\\top \\\\mathbf{y}.\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'この推定式は、$\\\\mathbf{X}^\\\\top \\\\mathbf{X}$が特異（非正則）である場合でも、$\\\\lambda > 0$により逆行列の存在が保証される点で、最小二乗法に比べて数値的に安定であるという利点がある。\\n',\n",
       " '\\\\subsection{勾配法による推定}\\n',\n",
       " 'リッジ回帰に対しても勾配法を適用できる。目的関数$\\\\mathcal{L}_\\\\lambda(\\\\mathbf{w})$の勾配は次のように求まる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\nabla \\\\mathcal{L}_\\\\lambda(\\\\mathbf{w}) = -2\\\\mathbf{X}^\\\\top(\\\\mathbf{y} - \\\\mathbf{Xw}) + 2\\\\lambda \\\\mathbf{w},\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'これに基づいて、更新式は以下のように与えられる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\mathbf{w} \\\\leftarrow \\\\mathbf{w} + \\\\alpha \\\\cdot \\\\left( \\\\frac{1}{n} \\\\mathbf{X}^\\\\top (\\\\mathbf{y} - \\\\mathbf{Xw}) - \\\\lambda \\\\mathbf{w} \\\\right),\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'または$\\\\alpha$を調整することで、$\\\\lambda$を勾配更新の一部として組み込む方法もある。いずれにしても、正則化項によって重みの更新が抑制されることで、過学習を防ぐ効果が得られる。\\n',\n",
       " '\\\\subsection{ロジスティック回帰}\\n',\n",
       " '本節では、非線形回帰の一種である\\\\textbf{ロジスティック回帰}\\\\index{ろじすてぃっくかいき@ロジスティック回帰} (logistic regression) について取り扱う。\\n',\n",
       " 'ロジスティック回帰は、入力$\\\\mathbf{x} \\\\in \\\\mathbb{R}^p$に対して出力$y \\\\in \\\\{0, 1\\\\}$を予測する\\\\textbf{確率的な分類モデル}\\\\index{かくりつてきなぶんるいもでる@確率的な分類モデル}である。出力は事後確率$\\\\Pr(y=1 \\\\mid \\\\mathbf{x})$を表し、その予測にはシグモイド関数（ロジスティック関数）を用いる。\\n',\n",
       " '\\\\subsection{モデルの定義}\\n',\n",
       " 'ロジスティック回帰では、まず説明変数の線形結合を求める：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " \"z = w_0 + \\\\sum_{j=1}^p w_j x_j = \\\\mathbf{w}^\\\\top \\\\mathbf{x}'\\n\",\n",
       " '\\\\end{equation}\\n',\n",
       " \"ここで$\\\\mathbf{x}' \\\\coloneqq [1, x_1, x_2, \\\\dots, x_p]^\\\\top \\\\in \\\\mathbb{R}^{p+1}$はバイアス項を含んだ拡張入力ベクトル、$\\\\mathbf{w} \\\\in \\\\mathbb{R}^{p+1}$はパラメータベクトルである。\\n\",\n",
       " 'この線形出力$z$に対して、シグモイド関数$\\\\sigma(z)$を適用することで、出力の確率的解釈が得られる：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\Pr(y = 1 \\\\mid \\\\mathbf{x}) = \\\\sigma(z) = \\\\frac{1}{1 + \\\\exp(-z)}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'したがって、クラスラベル$y \\\\in \\\\{0, 1\\\\}$の\\\\textbf{確率モデル}\\\\index{かくりつもでる@確率モデル}は次のように表される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " 'p(y \\\\mid \\\\mathbf{x}; \\\\mathbf{w}) = \\\\sigma(z)^y (1 - \\\\sigma(z))^{1 - y}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " '\\\\subsection{パラメータの推定：最尤推定}\\n',\n",
       " 'ロジスティック回帰のパラメータは\\\\textbf{最尤推定}\\\\index{さいゆうすいてい@最尤推定}により求める。データ集合$\\\\mathcal{D} = \\\\{(\\\\mathbf{x}^{(i)}, y^{(i)})\\\\}_{i=1}^n$に対して、対数尤度関数は以下のように定義される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\ell(\\\\mathbf{w}) = \\\\sum_{i=1}^n \\\\left[ y^{(i)} \\\\log \\\\sigma(z^{(i)}) + (1 - y^{(i)}) \\\\log (1 - \\\\sigma(z^{(i)})) \\\\right]\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'ここで$z^{(i)} = \\\\mathbf{w}^\\\\top \\\\mathbf{x}^{(i)}$である。\\n',\n",
       " 'この尤度を最大化することで$\\\\mathbf{w}$を学習する。一般には閉形式解を持たないため、\\\\textbf{勾配降下法}\\\\index{こうばいこうかほう@勾配降下法}などの最適化手法を用いて数値的に解く。\\n',\n",
       " '勾配は以下のように計算される：\\n',\n",
       " '\\\\begin{equation}\\n',\n",
       " '\\\\nabla \\\\ell(\\\\mathbf{w}) = \\\\sum_{i=1}^n (y^{(i)} - \\\\sigma(z^{(i)})) \\\\mathbf{x}^{(i)}\\n',\n",
       " '\\\\end{equation}\\n',\n",
       " 'Cox, D. R. (1958). \"The regression analysis of binary sequences.\" Journal of the Royal Statistical Society: Series B (Methodological), 20(2), 215–242.\\n']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64faa163-0a3c-4a0b-a972-90774666d0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filename = \"synapse-model/expo-synapse\"\n",
    "#master_list = md_ipynb2latex(dir_path, filename)\n",
    "#master_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c53f82-b800-4837-8fe4-696f57cc6df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb6f31-9193-45da-b4e6-f1268247c50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9b7bc-e541-409d-bded-f50327da7422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c6c32-5b76-46fc-adc4-62cb5b8c4892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf941ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4d53452252475bb632fb9155af48ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preface\n",
      "introduction/intro\n",
      "introduction/computational-neuroscience\n",
      "introduction/notation\n",
      "introduction/usage-julia-lang\n",
      "['2']\n",
      "['10']\n",
      "['right! (generic function with 1 method)']\n",
      "['foo (generic function with 1 method)']\n",
      "introduction/linear-algebra\n",
      "['3-element Vector{Int64}:\\n', ' 1\\n', ' 2\\n', ' 3']\n",
      "['Any[]']\n",
      "['3×3 Matrix{Float64}:\\n', ' 0.346295  0.323508  0.962472\\n', ' 0.406592  0.679734  0.552339\\n', ' 0.927452  0.788856  0.981346']\n",
      "['3×3 Matrix{Float64}:\\n', ' 0.206086  0.180519  0.385582\\n', ' 0.24197   0.379295  0.221276\\n', ' 0.551943  0.440186  0.393143']\n",
      "['2×2 Matrix{Int64}:\\n', ' 1  2\\n', ' 3  4']\n",
      "['2×3 Matrix{Int64}:\\n', ' 4  5  6\\n', ' 7  8  9']\n",
      "['2×5 Matrix{Int64}:\\n', ' 1  2  4  5  6\\n', ' 3  4  7  8  9']\n",
      "['2×5 Matrix{Int64}:\\n', ' 1  2  4  5  6\\n', ' 3  4  7  8  9']\n",
      "['2-element Vector{Matrix{Int64}}:\\n', ' [1 2; 3 4]\\n', ' [4 5 6; 7 8 9]']\n",
      "['5×2 Matrix{Int64}:\\n', ' 1  2\\n', ' 3  4\\n', ' 4  7\\n', ' 5  8\\n', ' 6  9']\n",
      "['5×2 Matrix{Int64}:\\n', ' 1  2\\n', ' 3  4\\n', ' 4  7\\n', ' 5  8\\n', ' 6  9']\n",
      "['5×4 Matrix{Int64}:\\n', ' 1  2  1  2\\n', ' 3  4  3  4\\n', ' 4  7  4  7\\n', ' 5  8  5  8\\n', ' 6  9  6  9']\n",
      "['3-element Vector{Float64}:\\n', ' 0.9225597515419179\\n', ' 0.658120481093438\\n', ' 0.41401801671066496']\n",
      "['1×3 Matrix{Float64}:\\n', ' 0.92256  0.65812  0.414018']\n",
      "['UniformScaling{Bool}\\n', 'true*I']\n",
      "['3×3 Diagonal{Bool, Vector{Bool}}:\\n', ' 1  ⋅  ⋅\\n', ' ⋅  1  ⋅\\n', ' ⋅  ⋅  1']\n",
      "['2-element Vector{Float64}:\\n', ' 0.5002132597166149\\n', ' 0.03509562556923285']\n",
      "['2-element Vector{Float64}:\\n', ' -0.1805788303672428\\n', '  0.8082531148175677']\n",
      "['2-element Vector{Float64}:\\n', ' -0.1805788303672428\\n', '  0.8082531148175676']\n",
      "['4×4 Matrix{Float64}:\\n', ' 1.0  5.0   9.0  13.0\\n', ' 2.0  6.0  10.0  14.0\\n', ' 3.0  7.0  11.0  15.0\\n', ' 4.0  8.0  12.0  16.0']\n",
      "['4×4 Matrix{Float64}:\\n', '  25.5193   -29.5701     5.09719  -18.3365\\n', ' -61.7388    71.768    -12.3934    44.6203\\n', '   5.53255   -6.36443    1.09258   -3.92298\\n', '   6.70725   -7.05331    1.14599   -4.0074']\n",
      "['4×4 Matrix{Float64}:\\n', ' 1.0  5.0   9.0  13.0\\n', ' 2.0  6.0  10.0  14.0\\n', ' 3.0  7.0  11.0  15.0\\n', ' 4.0  8.0  12.0  16.0']\n",
      "['2×2×2 Array{Float64, 3}:\\n', '[:, :, 1] =\\n', ' 0.915692  0.18149\\n', ' 0.132838  0.0376235\\n', '\\n', '[:, :, 2] =\\n', ' 0.807789  0.297739\\n', ' 0.569867  0.705417']\n",
      "['8-element Vector{Float64}:\\n', ' 0.9156924090979948\\n', ' 0.13283838898511735\\n', ' 0.18149003405285813\\n', ' 0.03762345094967079\\n', ' 0.8077889408656388\\n', ' 0.5698673388911334\\n', ' 0.297739498855886\\n', ' 0.7054169935372759']\n",
      "['8-element Vector{Float64}:\\n', ' 0.9156924090979948\\n', ' 0.13283838898511735\\n', ' 0.18149003405285813\\n', ' 0.03762345094967079\\n', ' 0.8077889408656388\\n', ' 0.5698673388911334\\n', ' 0.297739498855886\\n', ' 0.7054169935372759']\n",
      "['6×5 Matrix{Float64}:\\n', ' 0.286014   0.923302  0.174386   0.10705    0.744143\\n', ' 0.457232   0.234084  0.149783   0.684196   0.21295\\n', ' 0.308209   0.712286  0.903116   0.487475   6.84179e-6\\n', ' 0.0566201  0.526367  0.235321   0.0381212  0.336639\\n', ' 0.0933628  0.980388  0.0258844  0.0371784  0.849971\\n', ' 0.384149   0.628456  0.077691   0.970843   0.678124']\n",
      "['3×5 Matrix{Float64}:\\n', ' 0.399752  1.0085    0.36412    0.66242   1.06852\\n', ' 0.250858  0.946007  1.07921    0.627488  0.179271\\n', ' 0.75941   0.899138  0.0960996  3.52572   0.717514']\n",
      "['3×4×5×6 Array{Float64, 4}:\\n', '[:, :, 1, 1] =\\n', ' 0.88996    0.418819  0.317106  0.809881\\n', ' 0.0656426  0.403243  0.748969  0.656944\\n', ' 0.0165343  0.21815   0.20768   0.271664\\n', '\\n', '[:, :, 2, 1] =\\n', ' 0.924109  0.874625  0.548064   0.0665163\\n', ' 0.438622  0.808703  0.881086   0.777017\\n', ' 0.740098  0.48068   0.0283703  0.667576\\n', '\\n', '[:, :, 3, 1] =\\n', ' 0.316527   0.26011   0.453848  0.33607\\n', ' 0.610262   0.285777  0.888585  0.645278\\n', ' 0.0159621  0.470272  0.430346  0.686911\\n', '\\n', '[:, :, 4, 1] =\\n', ' 0.306327  0.521515   0.125245  0.28525\\n', ' 0.436886  0.397245   0.876463  0.822274\\n', ' 0.391734  0.0093844  0.210796  0.080409\\n', '\\n', '[:, :, 5, 1] =\\n', ' 0.720239  0.597256   0.00176386  0.287453\\n', ' 0.59386   0.0274512  0.321321    0.0833419\\n', ' 0.655899  0.303632   0.787094    0.543664\\n', '\\n', '[:, :, 1, 2] =\\n', ' 1.52439   0.524795  0.0910437  1.94272\\n', ' 0.955208  0.638171  1.8238     0.30505\\n', ' 0.634682  1.17551   0.770143   1.01951\\n', '\\n', '[:, :, 2, 2] =\\n', ' 1.55138   1.10778   0.825705  1.71204\\n', ' 0.778812  0.167395  1.05283   0.496133\\n', ' 1.49202   1.04271   0.768803  0.888104\\n', '\\n', '[:, :, 3, 2] =\\n', ' 1.57256   0.362477  0.987483  0.768631\\n', ' 1.94081   1.344     0.137258  1.23047\\n', ' 0.188979  0.793865  0.76025   0.558448\\n', '\\n', '[:, :, 4, 2] =\\n', ' 1.56428    0.349541  0.907228  0.551257\\n', ' 0.0598789  0.608847  0.178587  1.83449\\n', ' 1.39614    0.92944   0.609819  0.369434\\n', '\\n', '[:, :, 5, 2] =\\n', ' 1.46451  0.698757  0.257167  1.13523\\n', ' 1.90936  0.587094  1.79273   0.535054\\n', ' 0.63959  1.75057   1.10514   1.80024\\n', '\\n', '[:, :, 1, 3] =\\n', ' 2.39584  0.324164  1.74676   1.35733\\n', ' 1.7207   0.144594  0.343904  2.37319\\n', ' 2.62666  0.168394  2.88495   1.9154\\n', '\\n', '[:, :, 2, 3] =\\n', ' 1.9677    2.87961   1.96704  0.867237\\n', ' 1.89466   2.17317   1.50039  0.387591\\n', ' 0.155398  0.376322  2.01483  1.58617\\n', '\\n', '[:, :, 3, 3] =\\n', ' 0.167492  2.95293   1.86561  2.43932\\n', ' 1.82734   0.985729  2.29529  2.66207\\n', ' 0.528923  1.58292   2.37377  2.40504\\n', '\\n', '[:, :, 4, 3] =\\n', ' 2.74134  0.0518728  0.345255  1.26673\\n', ' 2.35339  0.0249801  0.858517  2.88582\\n', ' 2.12268  0.96       0.723985  1.52997\\n', '\\n', '[:, :, 5, 3] =\\n', ' 2.83398   0.694443  0.598329  0.729414\\n', ' 0.624493  2.07602   1.31222   1.2702\\n', ' 0.796103  0.572787  0.797189  2.51299\\n', '\\n', '[:, :, 1, 4] =\\n', ' 1.46751  1.47563  3.04088  3.80627\\n', ' 3.07408  2.99604  3.69522  2.13379\\n', ' 2.11139  3.23237  1.98099  2.09582\\n', '\\n', '[:, :, 2, 4] =\\n', ' 0.412196  3.86739    1.4051    2.75114\\n', ' 2.94004   3.20301    0.296766  0.222383\\n', ' 3.21486   0.0395058  0.552863  2.58788\\n', '\\n', '[:, :, 3, 4] =\\n', ' 2.70044  2.21041  3.52705   0.727296\\n', ' 0.91456  3.2226   3.82373   1.4074\\n', ' 3.13616  3.13658  0.330075  0.0344954\\n', '\\n', '[:, :, 4, 4] =\\n', ' 3.30181  3.35825  3.28717   1.25415\\n', ' 1.6306   3.27834  0.260342  1.02253\\n', ' 3.57028  2.18016  0.854409  3.9699\\n', '\\n', '[:, :, 5, 4] =\\n', ' 3.73104  3.11372  0.948956  0.718564\\n', ' 1.32246  3.47632  1.89129   3.02821\\n', ' 1.1886   3.6657   0.703906  3.63052\\n', '\\n', '[:, :, 1, 5] =\\n', ' 4.54075  4.11371  0.865309  3.61641\\n', ' 1.17843  2.32359  3.40058   4.19308\\n', ' 2.82815  4.94534  2.44516   2.08957\\n', '\\n', '[:, :, 2, 5] =\\n', ' 1.93973  1.06067  1.64007   3.57131\\n', ' 4.34119  4.36608  0.241168  0.0655764\\n', ' 2.85547  2.73876  4.01211   3.50433\\n', '\\n', '[:, :, 3, 5] =\\n', ' 0.545141  0.120245  1.2887   4.80195\\n', ' 3.53376   4.83037   1.79706  3.48993\\n', ' 3.03275   4.28879   1.7707   1.91026\\n', '\\n', '[:, :, 4, 5] =\\n', ' 4.70091  3.23968  1.20852  4.23932\\n', ' 3.54115  3.19075  1.91554  1.02991\\n', ' 1.86595  3.22341  3.05072  3.05978\\n', '\\n', '[:, :, 5, 5] =\\n', ' 1.84798   1.28311   4.81296  2.04469\\n', ' 0.373411  1.06052   4.2899   0.954725\\n', ' 4.59676   0.160808  1.58396  3.56051\\n', '\\n', '[:, :, 1, 6] =\\n', ' 0.981247    5.74285  4.33641  0.397288\\n', ' 0.00737349  4.93684  2.41447  0.440869\\n', ' 0.862491    4.52457  5.12091  4.53781\\n', '\\n', '[:, :, 2, 6] =\\n', ' 0.460174  5.97078   3.92445  5.71072\\n', ' 5.50241   0.575818  3.3802   4.74823\\n', ' 3.44502   3.3458    3.1195   0.563531\\n', '\\n', '[:, :, 3, 6] =\\n', ' 3.19706   4.09554   0.895392  5.1414\\n', ' 5.84708   5.43967   2.54962   5.0533\\n', ' 0.352629  0.200952  0.496654  5.7511\\n', '\\n', '[:, :, 4, 6] =\\n', ' 0.392204  1.69312   5.48683  0.874287\\n', ' 4.72632   4.70637   1.20864  3.17975\\n', ' 1.42709   0.670438  5.88614  4.88048\\n', '\\n', '[:, :, 5, 6] =\\n', ' 4.81313  3.62939  3.10386  2.59265\\n', ' 2.68798  5.64427  2.99339  3.83071\\n', ' 1.96035  3.09336  3.55476  0.824868']\n",
      "introduction/differential-equation\n",
      "introduction/linear-regression\n",
      "introduction/probability-information-theory\n",
      "introduction/stochastic-process-differential-equation\n",
      "neuron-model/intro\n",
      "neuron-model/neuron-physiol\n",
      "neuron-model/hodgkin-huxley\n",
      "['update! (generic function with 2 methods)']\n",
      "['fi_curve (generic function with 1 method)']\n",
      "neuron-model/fhn\n",
      "neuron-model/lif\n",
      "['update! (generic function with 1 method)']\n",
      "neuron-model/izhikevich\n",
      "neuron-model/isi\n",
      "['rasterplot (generic function with 1 method)']\n",
      "['gamma_spike (generic function with 1 method)']\n",
      "['gamma_isi_plot (generic function with 2 methods)']\n",
      "neuron-model/neurite-growth-model\n",
      "synapse-model/intro\n",
      "synapse-model/synapse-physiol\n",
      "synapse-model/current-conductance-synapse\n",
      "synapse-model/expo-synapse\n",
      "synapse-model/kinetic-synapse\n",
      "synapse-model/synaptic-weighted\n",
      "synapse-model/dynamical-synapses\n",
      "neuronal-computation/intro\n",
      "neuronal-computation/neuronal-arithmetic\n",
      "['update! (generic function with 1 method)']\n",
      "['GammaSpike (generic function with 1 method)']\n",
      "['FIcurve (generic function with 3 methods)']\n",
      "['HHIAFIcurve_multi (generic function with 1 method)']\n",
      "local-learning-rule/intro\n",
      "local-learning-rule/pca-hebbian-learning\n",
      "['SVD{Float64, Float64, Matrix{Float64}, Vector{Float64}}\\n', 'U factor:\\n', '2×2 Matrix{Float64}:\\n', ' -0.722509  -0.691362\\n', ' -0.691362   0.722509\\n', 'singular values:\\n', '2-element Vector{Float64}:\\n', ' 418.9073852600819\\n', ' 138.22321877667497\\n', 'Vt factor:\\n', '2×2 Matrix{Float64}:\\n', ' -0.722509  -0.691362\\n', ' -0.691362   0.722509']\n",
      "['DoG (generic function with 6 methods)']\n",
      "['relu (generic function with 1 method)']\n",
      "local-learning-rule/mds-anti-hebbian-learning\n",
      "local-learning-rule/slow-feature-analysis\n",
      "['whiten (generic function with 1 method)']\n",
      "['linsfa (generic function with 1 method)']\n",
      "local-learning-rule/stdp-learning\n",
      "local-learning-rule/logistic-regression-perceptron\n",
      "['(2, 1)']\n",
      "['step (generic function with 1 method)']\n",
      "local-learning-rule/self-organizing-map\n",
      "['make_blobs (generic function with 1 method)']\n",
      "['u_matrix2d (generic function with 1 method)']\n",
      "['find_bmu (generic function with 1 method)']\n",
      "energy-based-model/intro\n",
      "energy-based-model/energy-based-model\n",
      "energy-based-model/hopfield-model\n",
      "['corrupted (generic function with 2 methods)']\n",
      "energy-based-model/boltzmann-machine\n",
      "['(28, 28, 60000)']\n",
      "['20']\n",
      "['4']\n",
      "['energy (generic function with 1 method)']\n",
      "energy-based-model/sparse-coding\n",
      "['soft_nonneg_thres (generic function with 1 method)']\n",
      "['updateOF! (generic function with 1 method)']\n",
      "['normalize_rows (generic function with 1 method)']\n",
      "['calculate_total_error (generic function with 1 method)']\n",
      "['run_simulation (generic function with 1 method)']\n",
      "energy-based-model/predictive-coding\n",
      "['update! (generic function with 1 method)']\n",
      "['gaussian_2d (generic function with 4 methods)']\n",
      "['run_simulation (generic function with 1 method)']\n",
      "solve-credit-assignment-problem/intro\n",
      "solve-credit-assignment-problem/backpropagation\n",
      "['∇tanh (generic function with 1 method)']\n",
      "['softmax (generic function with 1 method)']\n",
      "['forward! (generic function with 1 method)']\n",
      "['backward! (generic function with 1 method)']\n",
      "['squared_error! (generic function with 1 method)']\n",
      "['optimizer_update! (generic function with 1 method)']\n",
      "['optimizer_update! (generic function with 2 methods)']\n",
      "['optim_step! (generic function with 1 method)']\n",
      "['train_step! (generic function with 2 methods)']\n",
      "['Gaussian2d (generic function with 4 methods)']\n",
      "['product (generic function with 1 method)']\n",
      "['\"binary_crossentropy\"']\n",
      "['binary_crossentropy! (generic function with 1 method)']\n",
      "['(66, 16, 64)']\n",
      "solve-credit-assignment-problem/linear-network-learning-dynamics\n",
      "solve-credit-assignment-problem/bptt\n",
      "['update! (generic function with 1 method)']\n",
      "solve-credit-assignment-problem/surrogate-gradient-snn\n",
      "solve-credit-assignment-problem/reservoir-computing\n",
      "motor-learning/intro\n",
      "motor-learning/minimum-jerk\n",
      "['solveEqualityConstrainedQuadProg (generic function with 1 method)']\n",
      "['6']\n",
      "motor-learning/minimum-variance\n",
      "['minimum_variance_model (generic function with 1 method)']\n",
      "motor-learning/optimal-feedback-control\n",
      "['Reaching1DModelCostParameter']\n",
      "['LQG (generic function with 1 method)']\n",
      "['gLQG (generic function with 3 methods)']\n",
      "['simulation (generic function with 1 method)']\n",
      "['simulation_all (generic function with 1 method)']\n",
      "motor-learning/infinite-horizon-ofc\n",
      "['SaccadeModelParameter']\n",
      "['infinite_horizon_ofc (generic function with 3 methods)']\n",
      "['simulation (generic function with 4 methods)']\n",
      "['1.0']\n",
      "['target_jump_simulation (generic function with 6 methods)']\n",
      "motor-learning/local-learning-ofc\n",
      "['Reaching1DModelCostParameter']\n",
      "['LQG (generic function with 1 method)']\n",
      "['simulation (generic function with 1 method)']\n",
      "['simulation_all (generic function with 1 method)']\n",
      "motor-learning/rat-trajectory\n",
      "reinforcement-learning/intro\n",
      "reinforcement-learning/td-learning\n",
      "bayesian-brain/intro\n",
      "bayesian-brain/neural-uncertainty-representation\n",
      "bayesian-brain/bayesian-linear-regression\n",
      "bayesian-brain/mcmc\n",
      "['MixtureModel{IsoNormal}(K = 2)\\n', 'components[1] (prior = 0.5000): IsoNormal(\\n', 'dim: 2\\n', 'μ: [0.0, 0.0]\\n', 'Σ: [1.0 0.0; 0.0 1.0]\\n', ')\\n', '\\n', 'components[2] (prior = 0.5000): IsoNormal(\\n', 'dim: 2\\n', 'μ: [3.0, 3.0]\\n', 'Σ: [1.0 0.0; 0.0 1.0]\\n', ')\\n', '\\n']\n",
      "['grad (generic function with 1 method)']\n",
      "['([1.0 1.0 … 0.4331052638968813 0.11221124838455171; 0.5 0.5 … 2.002625214513086 1.979212663395787], 1183)']\n",
      "['(2, 2000)']\n",
      "['0.1']\n",
      "['(0.005, 5.0)']\n",
      "['ulp (generic function with 1 method)']\n",
      "['4-element Vector{Float64}:\\n', '  2.3512978351397225\\n', '  4.110944423223151\\n', ' 12.557040811868173\\n', ' -2.261645476785874']\n",
      "bayesian-brain/neural-sampling\n",
      "['membrane_potential (generic function with 4 methods)']\n",
      "['∇ᵤlogP (generic function with 1 method)']\n",
      "bayesian-brain/probabilistic-population-coding\n",
      "bayesian-brain/quantile-expectile-regression\n",
      "['normal_equation (generic function with 1 method)']\n",
      "['kde (generic function with 3 methods)']\n",
      "appendix/intro\n",
      "appendix/grid-cells-decoding\n",
      "['MAT.MAT_v5.Matlabv5File(IOStream(<file ../_static/datasets/grid_cells_data/10704-07070407_T2C3.mat>), false, #undef)']\n",
      "['nearest_pos (generic function with 1 method)']\n",
      "appendix/graph-theory-network-model\n",
      "[\"PyObject <module 'networkx' from 'C:\\\\\\\\Users\\\\\\\\yamta\\\\\\\\miniconda3\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\networkx\\\\\\\\__init__.py'>\"]\n",
      "appendix/useful-links\n",
      "appendix/usage-jupyter-book\n"
     ]
    }
   ],
   "source": [
    "main_list = []\n",
    "for i, section in tqdm(enumerate(toc_yaml['sections'])):\n",
    "    print(section['file']) # intro\n",
    "    if i > 0:\n",
    "        for subsection in section['sections']:\n",
    "            filename = subsection['file']\n",
    "            print(filename)\n",
    "            md_ipynb2latex(dir_path, filename)\n",
    "            main_list.append(r\"\\input{./text/\"+filename+\".tex}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8e3c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"contents_list.tex\", 'w', encoding='UTF-8') as f:\n",
    "    f.writelines(main_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
